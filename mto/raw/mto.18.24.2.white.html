 
 

<!-------------------------------- HEADER -------------------------------------------->

      
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> 
<head>

<title> MTO 24.2: White, Feedback and Feedforward Models of Musical Key</title>

<link rel="SHORTCUT ICON" href="https://www.mtosmt.org/gifs/favicon.ico">
<link rel="stylesheet" href="https://www.mtosmt.org/scripts/colorbox.css">
<link rel=StyleSheet href="https://www.mtosmt.org/scripts/mto-tufte.css" type="text/css" media=all>
<link rel="stylesheet" href="//code.jquery.com/ui/1.11.4/themes/smoothness/jquery-ui.css">

<script src="https://www.google-analytics.com/urchin.js" type="text/javascript"></script>
<script type="text/javascript">_uacct = "UA-968147-1"; urchinTracker();</script>

<script type="text/javascript" src="https://www.mtosmt.org/scripts/expandingMenu.js"></script>
<script type="text/javascript" src="https://www.mtosmt.org/scripts/dropdownMenu.js"></script>
<!--<script language="JavaScript" type="text/javascript" src="https://www.mtosmt.org/scripts/AC_QuickTime.js"></script>-->
<!--<script type="text/javascript" src="https://www.mtosmt.org/scripts/examples.js"></script>-->
<script type="text/javascript" src="https://www.mtosmt.org/scripts/hover.js"></script>  
<script src="https://code.jquery.com/jquery-1.10.2.js"></script>
<script src="https://code.jquery.com/ui/1.11.4/jquery-ui.js"></script>
<script src="https://www.mtosmt.org/scripts/colorbox-master/jquery.colorbox.js"></script>
<script type="text/javascript" src="https://www.mtosmt.org/scripts/jQueryRotate.2.2.js"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
<script>
MathJax.Hub.Config({
    TeX: { noErrors: { disabled: true } }
});
</script>

  <script>
   $(function () {
      $(document).tooltip({
        position: { my: "center bottom-10", at: "center top", },
    content: function () {
              return $(this).prop('title');
          }
      });
  });
  </script>

  <style>
    .ui-tooltip {
      color: #3a3a3a;
      font: 300 14px/20px "Lato", "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
      max-width: 600px;
      box-shadow: 0 0 7px gray;
    }
    ol.mto-alpha {
        list-style: lower-alpha none outside;
    }
   ol.mto-alpha li {
       margin-bottom: 0.75em;
       margin-left: 2em;
       padding-left: 0.5em;
    }
  </style>

    <script language="Javascript">
        $(document).ready(function() {
            $(".mp3").colorbox({iframe:true, internalWidth:360, width:400, internalHeight:100, rel:'mp3', height:150, opacity:0.1, onComplete: function(e) {
                $('#colorbox').on({
                    mousedown: function(e){
                        if (~$.inArray(e.target, $('input, textarea, button, a, .no_drag', $('#colorbox')))) return;
                        var os = $('#colorbox').offset(),
                            dx = e.pageX-os.left, dy = e.pageY-os.top;
                        $(document).on('mousemove.drag', function(e){
                            $('#colorbox').offset({ top: e.pageY-dy, left: e.pageX-dx } );
                        });
                    },
                    mouseup: function(){ $(document).unbind('mousemove.drag'); }
                });
            }
        });
            $(".youtube").colorbox({iframe:true, innerWidth:640, innerHeight:390, opacity:0.1, rel:'youtube', onComplete: function(e) {
                $('#colorbox').on({
                    mousedown: function(e){
                        if (~$.inArray(e.target, $('input, textarea, button, a, .no_drag', $('#colorbox')))) return;
                        var os = $('#colorbox').offset(),
                            dx = e.pageX-os.left, dy = e.pageY-os.top;
                        $(document).on('mousemove.drag', function(e){
                            $('#colorbox').offset({ top: e.pageY-dy, left: e.pageX-dx } );
                        });
                    },
                    mouseup: function(){ $(document).unbind('mousemove.drag'); }
                });
            }
        });

      $("a[id^=footnote]").each(function(){
        var fnnum = $(this).attr('id').substring(8);
	var foot_me = '#fndiv'+fnnum;
        $("#footnote" + fnnum).attr('title', $(foot_me).html());

        });


        $("a[id^=citation]").each(function(){
         var separatorPos = $(this).attr('id').lastIndexOf('_');
         var linkid = $(this).attr('id');
         var citeref = $(this).attr('id').substring(8,separatorPos);
         var cite_me = '#citediv'+citeref;
         $("#" + linkid).attr('title', $(cite_me).html());

        });
    });

    </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-968147-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-968147-1');
</script>


<meta http-equiv="Content-Language" content="en-us">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
 

<meta name="citation_title" content="Feedback and Feedforward Models of Musical Key">

    <meta name="citation_author" content="White, Christopher Wm.">
      
<meta name="citation_publication_date" content="2018/06/01">
<meta name="citation_journal_title" content="Music Theory Online">
<meta name="citation_volume" content="24">
<meta name="citation_issue" content="2">

</head>

<body>
<div class="bannertop">
	<a id="smt-link" alt="Society for Music Theory" href="https://www.societymusictheory.org">&nbsp;</a>
</div>
		
		<div style = "height:160px; width:900px; background-image: url('../../gifs/banner_blue_grey_900px.png'); background-repeat: no-repeat; background-position: 0px 0px"></div>
		
<!-------------------------------- MENU -------------------------------------------->

    
<div class="dropdown_menu">

<ul class="fullwidth" id="ddm">
    <li><a href="https://www.mtosmt.org/index.php">MTO Home</a>
    </li>
    <li><a href="https://www.mtosmt.org/issues/mto.24.30.4/toc.30.4.html">Current Issue</a>    </li>
    <li><a href="https://www.mtosmt.org/issues/issues.php"
    	onmouseover="mopen('m3')" 
        onmouseout="mclosetime()">Previous Issues</a>
        <div id="m3" 
            onmouseover="mcancelclosetime()" 
            onmouseout="mclosetime()">
	        <a href="https://www.mtosmt.org/docs/index-author.php">By Author</a>
	        <a href="https://www.mtosmt.org/issues/issues.php">By Volume&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
	</li>
	
    <li><a href="https://www.mtosmt.org/docs/authors.html.php"
    	onmouseover="mopen('m4')" 
        onmouseout="mclosetime()">For Authors</a>
        <div id="m4" 
            onmouseover="mcancelclosetime()" 
            onmouseout="mclosetime()">
	        <a href="https://www.mtosmt.org/docs/mto-editorial-policy.html">MTO Editorial Policy</a>
	      <a href="https://www.mtosmt.org/docs/mto-style-guidelines.html">MTO Style Guidelines</a>
	      <a href="https://www.mtosmt.org/docs/how-to-submit-an-article-to-mto.html">How to Submit an Article</a>
	      <a href="https://www.mtosmt.org/ojs">Submit Article Online</a>
	      <a href="https://www.mtosmt.org/docs/reviewers.html">Book Review Guidelines</a>
        </div>
	</li>

 <!--   <li><a href="https://www.mtosmt.org/docs/authors.html">Submit</a>
	</li> -->
	
    <li><a href="https://www.mtosmt.org/mto-jobs.php"
    	onmouseover="mopen('m6')" 
        onmouseout="mclosetime()">Jobs</a>
        <div id="m6" 
            onmouseover="mcancelclosetime()" 
            onmouseout="mclosetime()">
	        <a href="https://www.mtosmt.org/mto-jobs.php">Current Job Listings</a>
	        <a href="https://www.mtosmt.org/mto-job-post.php">Submit Job Listing</a>
        </div>
	</li>
    <li><a href="https://www.mtosmt.org/docs/diss-index.php"
    	onmouseover="mopen('m7')" 
        onmouseout="mclosetime()">Dissertations</a>
        <div id="m7" 
            onmouseover="mcancelclosetime()" 
            onmouseout="mclosetime()">
	        <a href="https://www.mtosmt.org/docs/diss-index.php">All Dissertations</a>
	        <a href="https://www.mtosmt.org/docs/diss-index.php?new=true">New Dissertations</a>
	        <a href="https://www.mtosmt.org/mto-diss-post.php">List Your Dissertation</a>
        </div>
	</li>
    <li><a href="https://www.mtosmt.org/about.html">About</a>
	</li>
<!--    <li><a href="https://www.mtosmt.org/mto_links.html">Journals</a>  
	</li> -->
    <li><a href="https://societymusictheory.org">SMT</a>
	</li>
   <!-- <li><a href="https://societymusictheory.org/announcement/contest-new-mto-logo-2024-02"><span style="color:yellow">Logo Design Contest</span></a>
	</li>-->

</ul>

</div>


<!-------------------------------- TITLE -------------------------------------------->

    <article>

<div id="content">
<a name="Beginning"></a>
			
	<h1 style="width:900px; margin-top:1em">Feedback and Feedforward Models of Musical Key<sup><a name="FN0REF" href="#FN0" id="footnote0">*</a></sup> </h1>
	<div style="width:900px">
				</div>

	
				<h2><span style="font-weight: 400"><font size="5"><a style="color:black" href="#AUTHORNOTE1">Christopher Wm. White</a></font></span></h2><br><br><p><font size='4'>KEYWORDS: Corpus analysis, tonality, key, harmony, modeling, scale degrees</font></p><p><font size='4'>ABSTRACT: This study begins by drawing a distinction between two ways of framing the concept of musical key. <i>Feedforward</i> models understand key as arising from immediately apparent surface characteristics like the distribution of pitch classes or a melody&#8217;s intervallic content. <i>Feedback</i> models, on the other hand, understand key as being determined in tandem with other domains. Here, key arises from the surface being organized into other more complicated musical groupings or schemata&#8212;harmonic progressions, cadences, prolongations, meter, etc.&#8212;that themselves are informed by the music&#8217;s tonal center. While much music theory and theory pedagogy have acknowledged that feedback occurs in various approaches to tonality, formal modeling in the fields of music cognition and computation has focused primarily on feedforward systems. This article attempts to right this imbalance by presenting a corpus-based feedback computational model that can be tested against human behavior. My model will identify a passage&#8217;s key by organizing a surface into its constituent harmonies. Here, harmonic organization and key will be integrated into a feedback system with the ideal key being that which produces the ideal harmonic analysis, and vice versa. To validate the resulting model, its behavior is compared to that of other published tonal models, to the behaviors of undergraduate music students, and to the intuitions of professional music theorists.</font></p>			
	<div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'><a href="http://mtosmt.org/issues/mto.18.24.2/mto.18.24.2.white.pdf">PDF text </a> | <a href="http://mtosmt.org/issues/mto.18.24.2/white_examples.pdf">PDF examples </a></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div>
			<div style="float:left; font-size:1.1rem;"><i>Received August 2016</i></div>
		<div style="width:850px">
	<div style="text-align:center; font-size: 1.1rem; margin-bottom:2em;margin-top:4em;margin-right:auto;margin-left:auto;width:870px">
		Volume 24, Number 2, June 2018 <br> Copyright &#0169; 2018 Society for Music Theory	</div>
	</div>

<hr style="width:850px"><br>
<section>
<!------------------------------- ARTICLE BODY (begin) -------------------------------------->

<h2>Part I: Feedback and Feedforward Understandings of Musical Key</h2>

<p>[1.1] David Temperley begins his <a href="#temperley_1999" id="citation_temperley_1999_67dc99bb18a33">1999</a> article &#8220;What&#8217;s Key for Key?&#8221; by asking a crucial question underpinning listeners&#8217; experience of tonal music: &#8220;By what method do people determine the key of a piece or changes of key within a piece?&#8221; (66) When we glance at a musical score or listen to a passage of music, how do we identify a tonal center and convert pitches into scale degrees? How do we know music&#8217;s &#8220;key&#8221;?</p>

<p>[1.2] Of course the topic of key finding and the establishment of tonal hierarchies has been theorized for centuries: any tonal theory formalizes how scale degrees or harmonies are expressed through pitches and rhythms. From Rameau to Schenker, Riemann to Krumhansl, any theory of tonal music converts musical surfaces into successions of scale degrees in some way.</p>

<p>[1.3] This article uses corpus analysis and computational modeling to identify an often-overlooked aspect of musical key, namely its interrelationship with other equally complex musical parameters. (Here, &#8220;tonal orientation&#8221; and &#8220;key&#8221; will simply mean the identification of some key center, although I will engage in more nuanced definitions later in this essay.) I argue that analyzing a tonal center in tandem with some other musical parameter (like harmony or meter) represents a fundamentally different strategy than treating key as a property that arises independently from other organizing musical structures. I call the former <i>feedback</i> models of musical key, and the latter <i>feedforward</i> models. In a feedback model, a passage&#8217;s key arises from parsing a musical surface into, say, harmonies while at the same time this harmonic analysis relies on the passage&#8217;s tonal orientation: here, harmony and key are intertwined in a feedback loop that produces an analysis or interpretation of both domains. In contrast, feedforward approaches find some immediate piece of evidence&#8212;say, the first lowest note or the repetition of a particular pitch&#8212;and uses that to determine the passage&#8217;s key. Here, key is determined separately from and prior to other musical domains: there is no feedback loop.<sup><a name="FN1REF" href="#FN1" id="footnote1">(1)</a></sup> To put a fine point on the distinction: if knowing a passage&#8217;s key helps you know <i>x</i>, and knowing <i>x</i> helps you identify a passage&#8217;s key, your logic uses feedback; if you are simply using <i>x</i> to determine a passage&#8217;s key, your logic is feedforward.</p>

<p>[1.4] Having established this dichotomy, I argue that&#8212;while both approaches are represented in music theory, pedagogy, computational modeling, and psychology&#8212;feedback methods have received overall less attention, especially regarding formal models, empirical testing, and behavioral studies. I then present a proof-of-concept computational system that integrates key-finding with harmonic analysis into a feedback system, and show how such a model can be subjected to behavioral testing. My approaches will rely on insights from each of these domains&#8212;from computational, psychological, pedagogical and music theoretic research&#8212;but I will show that reaching across these discourses and disciplines can add to, complicate, and hone our understanding of musical key, and of tonality in general. This essay ends by discussing how such formalizations can add not only to our understanding of musical key and tonal orientation, but to how we present this concept in the classroom. Furthermore, while I frame this discussion formally/computationally throughout, I argue that this kind of thinking can contribute to how musicians and researchers think about and engage with the concept of musical key.</p>


<h2>Part II: Defining Feedforward and Feedback Approaches</h2>

<p>[2.1] <b>Example 1a</b> schematizes feedforward key finding. This kind of logic first identifies some property, schema, or template apparent on the musical surface and uses that parameter to identify the music&#8217;s key. There is a one-way mapping between the music&#8217;s &#8220;surface&#8221; and its &#8220;organizing structure&#8221;&#8212;namely <i>key</i>. This surface/structure dichotomy is drawn from Temperley (<a href="#temperley_2007" id="citation_temperley_2007_67dc99bb18a4b">2007</a>), but also tracks the musical perception/cognition dichotomy of Huron (<a href="#huron_2006" id="citation_huron_2006_67dc99bb18a4d">2006</a>): the former categories correspond to uninterpreted musical phenomena (think: pitches and rhythms), while the latter category involves interpreting those phenomena in some way (think: scale degrees and meters).<sup><a name="FN2REF" href="#FN2" id="footnote2">(2)</a></sup> Consider <b>Example 2</b>: if a key-finding process tallied the pitch classes used in the first phrase (a surface phenomenon) and concluded that they corresponded to the key of D major (an organizing structure), that process would be using a feedforward approach. Similarly, if we identified the excerpt&#8217;s modulation by noting that the diminished fifth between <nobr><span style= 'letter-spacing:-0.8px'>G<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr> and D points toward an A key center, we would be using a similar logic. When using information like a key signature, the passage&#8217;s intervals, or a collection to identify key, we gain a tonal orientation (the structural interpretation) by observing some immediately apparent property of the musical surface.<sup><a name="FN3REF" href="#FN3" id="footnote3">(3)</a></sup> Importantly, identifying the key is not conceptually dependent on items like the music&#8217;s phrase structure, harmonic outline, or cadences: the logic moves in a &#8220;feedforward&#8221; direction. Here, analyzing the surface yields a key, after which other organizing structures might subsequently be identified.</p>


<table width=800><tr><td valign='top' width=50%><p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 1a and b</b>. Feedforward and Feedback ways of schematizing how key interacts with other musical parameters</p><p class='fullwidth' style="text-align: center;"><a class='youtube'  target="blank" href="white_examples.php?id=0&nonav=true"><img border="1" alt="Example 1a and b thumbnail" src="white_ex01a_small.png"></a></p><p class='fullwidth' style="text-align: center"><font size="2">(click to enlarge and see the rest)</font></p></td><td width=50></td><td valign='top' width=50%><p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 2</b>. Mozart, Piano Sonata, K. 284, iii, mm. 1&ndash;8 (slurring from Breitkopf edition)</p><p class='fullwidth' style="text-align: center;"><a class='youtube'  target="blank" href="white_examples.php?id=1&nonav=true"><img border="1" alt="Example 2 thumbnail" src="white_ex02_small.png"></a></p><p class='fullwidth' style="text-align: center"><font size="2">(click to enlarge)</font></p></td></tr></table>
<p>[2.2] Alternately, we might identify the key of a passage using a feedback<i></i> process, as outlined in <b>Example 1b</b>. This kind of process views a passage&#8217;s key as dependent upon larger organizational structures like chord progressions, metric hierarchies, or harmonic/melodic groupings. The organizing structure of &#8220;key&#8221; arises concomitantly with&#8212;not prior to&#8212;other musical interpretations: key is determined as part of a feedback loop with other musical organizations. Rather than being an immediate property of the musical surface, key emerges as a byproduct of organizing the surface into more complex structures, which themselves are dependent on the passage&#8217;s key.<sup><a name="FN4REF" href="#FN4" id="footnote4">(4)</a></sup></p>

<fig>
<p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 3</b>. Grieg, &ldquo;The Mountain Maid,&rdquo; op. 67, no. 2, mm. 4&ndash;7, along with tonal analyses provided by two computational models</p><p class='fullwidth' style="text-align: center; margin-bottom:0em"><a class='youtube'  target="blank" href="white_examples.php?id=2&nonav=true"><img border="1" alt="Example 3 thumbnail" src="white_ex03_small.png"></a></p><p class='fullwidth' style="text-align: center; margin-top:0em"><font size="2">(click to enlarge)</font></p></fig>

<p>[2.3] <b>Example 3</b> shows a tonally ambiguous excerpt that highlights the differences between feedforward and feedback methods, measures 5&#8211;7 of Grieg&#8217;s &#8220;The Mountain Maid&#8221; op. 67 no. 2. Below the example, I show how two computational systems parse this excerpt, one feedforward and one feedback. The first roughly follows the methods of Krumhansl (<a href="#krumhansl_1990" id="citation_krumhansl_1990_67dc99bb18b5f">1990</a>), determining musical key by the pitch classes present in some passage (a &#8220;key-profile analysis&#8221;),<sup><a name="FN5REF" href="#FN5" id="footnote5">(5)</a></sup> while the second uses a chord-based approach drawn from my previous work in this area (<a href="#white_2015" id="citation_white_2015_67dc99bb18b63">White 2015</a>). (I will discuss the specifics of these different algorithms below.) Both models return more than one possible key at each point, shown by the different rows allocated to each model.<sup><a name="FN6REF" href="#FN6" id="footnote6">(6)</a></sup></p>

<p>[2.4] The two models provide overlapping but different answers, with the collection-based method finding <nobr><span style= 'letter-spacing:-1px'>A<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr> major as a possible key along with F minor and <nobr><span style= 'letter-spacing:-1px'>D<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr> major. The chord-progression method only returns the latter two keys, but ignores and modifies several chords so that they fit into these keys: note, for instance, in the <nobr><span style= 'letter-spacing:-1px'>D<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr> analysis, the {C, <nobr><span style= 'letter-spacing:-1px'>B<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>, E, <nobr><span style= 'letter-spacing:-1px'>A<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>} chord is ignored and the {<nobr><span style= 'letter-spacing:-1px'>E<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>, <nobr><span style= 'letter-spacing:-1px'>D<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>, G, C} chord is read as an <nobr><span style= 'letter-spacing:-1px'>E<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr> minor-minor seventh chord (<nobr><span style= 'letter-spacing:-1px'>D<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>: ii7). This makes sense given the differing approaches of these methods. From a collectional perspective, the pitch classes of <nobr><span style= 'letter-spacing:-1px'>A<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr> major play a large role in measures 5&#8211;6. But, from a harmonic perspective, the keys of F and <nobr><span style= 'letter-spacing:-1px'>D<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr> are supported by dominant-to-tonic chord progressions, with the remaining chords potentially being modified versions of expected chords (i.e. the analyzed <nobr><span style= 'letter-spacing:-1px'>D<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>: ii7 chord shares two pcs with the observed surface structure).<sup><a name="FN7REF" href="#FN7" id="footnote7">(7)</a></sup></p>

<p>[2.5] But most notably, being a feedback system, the harmonic analysis of the chord-based model changes within its different tonal interpretations: the different keys use and ignore different surface pitches to create the chordal analyses. The same is not true of the feedforward logic of the collectional approach: the key (or ambiguity of key) is simply a product of the notes as they exist in the score. Consider how both methods interpret the chord on the downbeat of measure 6. In the feedforward method, it is not obvious how to interpret the chord, given the tonal ambiguity. However, in the feedback method, the key is part and parcel of how the passage is parsed into chords. The key is ambiguous because the chords are as well, and the chords are ambiguous because so too is the key.</p>


<h2>Part III: Feedforward and Feedback Logics in Music Research</h2>

<p>[3.1] For our purposes, the discourse surrounding key can be usefully divided into four overlapping dimensions/components: theory, pedagogy, computation, and behavioral testing. Together, these approaches constitute a larger multifaceted conceptual symbiosis: while theories creatively engender a concept&#8217;s speculative formal structure, a computer rigorously implements and parameterizes all aspects of this formal structure. Behavioral experiments can then test if and how a formalization describes the cognition or experience of that concept.<sup><a name="FN8REF" href="#FN8" id="footnote8">(8)</a></sup> Finally, pedagogy informs how we present and teach these ideas. Missing any corner of this disciplinary reciprocity means that the discourse surrounding a concept misses some important facet or approach. In this section, I argue that while scholarship in music perception and cognition has shown that feedforward methods are mutually supported by each of these domains, feedback systems have not received comparable attention.</p>

<p>[3.2] Feedforward models of key and tonal centers are manifested in all four of these domains. For instance, a theoretical approach might include Harrison&#8217;s (<a href="#harrison_2016" id="citation_harrison_2016_67dc99bb18b7a">2016</a>) &#8220;dronality&#8221; model, in which a key center is expressed by a constantly sounded pitch, while feedforward pedagogical approaches might include Clendinning and Marvin&#8217;s (<a href="#clendinning_and_marvin_2011" id="citation_clendinning_and_marvin_2011_67dc99bb18b7d">2011</a>) advice to identify modulations by focusing on a phrase&#8217;s opening notes and final chords. Feedforward computational systems would encompass implementations like Longuet-Higgins and Steedman (<a href="#longuet-higgins_and_steedman_1971" id="citation_longuet-higgins_and_steedman_1971_67dc99bb18b7f">1971</a>), which identify a passage&#8217;s key by matching its pitch classes to a diatonic set, as well as Quinn (<a href="#quinn_2010" id="citation_quinn_2010_67dc99bb18b81">2010</a>) and White (<a href="#white_2014" id="citation_white_2014_67dc99bb18b83">2014</a>), which match the interval structure of chord progressions to a scale-degree interpretation.<sup><a name="FN9REF" href="#FN9" id="footnote9">(9)</a></sup> Similarly, key-profile analyses identify a passage&#8217;s tonal orientation by matching its pitch-class distribution to the ideal distribution of some key: this sort of modeling has been shown to output analyses that conform to human assessments (<a href="#temperley_2007" id="citation_temperley_2007_67dc99bb18b85">Temperley 2007</a>, <a href="#temperley_and_marvin_2008" id="citation_temperley_and_marvin_2008_67dc99bb18b87">Temperley and Marvin 2008</a>, <a href="#albrecht_and_shanahan_2013" id="citation_albrecht_and_shanahan_2013_67dc99bb18b89">Albrecht and Shanahan 2013</a>) and has also been shown to predict human behavior in lab settings (<a href="#aarden_2003" id="citation_aarden_2003_67dc99bb18b8b">Aarden 2003</a>, <a href="#albrecht_and_huron_2014" id="citation_albrecht_and_huron_2014_67dc99bb18b8d">Albrecht and Huron 2014</a>).<sup><a name="FN10REF" href="#FN10" id="footnote10">(10)</a></sup> Music-cognition work has also investigated the tonal implications of a passage&#8217;s interval content, focusing on how certain intervals (like the diminished fifth) draw listeners to particular key centers (<a href="#brown_and_butler_1981" id="citation_brown_and_butler_1981_67dc99bb18b92">Brown and Butler 1981</a>, <a href="#brown_butler_and_jones_1994" id="citation_brown_butler_and_jones_1994_67dc99bb18b94">Brown, Butler, and Jones 1994</a>, Matsunaga and Abe <a href="#matsunaga_and_abe_2005" id="citation_matsunaga_and_abe_2005_67dc99bb18b96">2005</a> and <a href="#matsunaga_and_abe_2012" id="citation_matsunaga_and_abe_2012_67dc99bb18b98">2012</a>).<sup><a name="FN11REF" href="#FN11" id="footnote11">(11)</a></sup></p>

<p>[3.3] On the other hand, feedback systems are also represented in music research. Consider Lerdahl and Jackendoff&#8217;s (<a href="#lerdahl_and_jackendoff_1983" id="citation_lerdahl_and_jackendoff_1983_67dc99bb18b9b">1983</a>) intertwined metric and prolongational preference rules or Long&#8217;s (<a href="#long_2018" id="citation_long_2018_67dc99bb18b9d">2018</a>) argument that early tonal practices arose as a braid of interlocking metrical, poetic, and cadential expectations: these authors describe a feedback loop between metric emphases and tonal expectations, each domain informing and reinforcing the other.<sup><a name="FN12REF" href="#FN12" id="footnote12">(12)</a></sup> Similarly, when Castile-Blaze (<a href="#castil-blaze_1810" id="citation_castil-blaze_1810_67dc99bb18ba0">1810</a>) or Agmon (<a href="#agmon_1995" id="citation_agmon_1995_67dc99bb18ba2">1995</a>) theorize tonal music as being traceable to the I, IV, and V chords, they are claiming that the presence of those chord prototypes express the corresponding key while concurrently claiming that a tonal orientation produces those chords. When Schoenberg (<a href="#schoenberg_1978" id="citation_schoenberg_1978_67dc99bb18ba4">1978</a>) locates a passage&#8217;s key using his relationships between chord roots,<sup><a name="FN13REF" href="#FN13" id="footnote13">(13)</a></sup> when Riemann (<a href="#riemann_1893" id="citation_riemann_1893_67dc99bb18ba6">1893</a>), Louis and Thuille (<a href="#louis_and_thuille_1907" id="citation_louis_and_thuille_1907_67dc99bb18ba8">1907</a>), Straus (<a href="#straus_1987" id="citation_straus_1987_67dc99bb18baa">1987</a>), Dahlhaus (<a href="#dahlhaus_1990" id="citation_dahlhaus_1990_67dc99bb18bac">1990</a>), or Harrison (<a href="#harrison_1994" id="citation_harrison_1994_67dc99bb18bae">1994</a>) describe key as something expressed through the progression of harmonic functions, or when Gjerdingen (<a href="#gjerdingen_2007" id="citation_gjerdingen_2007_67dc99bb18baf">2007</a>) or Byros (<a href="#byros_2009" id="citation_byros_2009_67dc99bb18bb1">2009</a>, <a href="#byros_2012" id="citation_byros_2012_67dc99bb18bb3">2012</a>) identify a passage&#8217;s key using a voice-leading schema, they are each engaging in harmonic-progression feedback systems.<sup><a name="FN14REF" href="#FN14" id="footnote14">(14)</a></sup> In each of these theories, tonal orientation arises from a surface being organized into some series of structures with the key also informing how a passage organizes into chords and harmonic functions or which notes are chosen to participate in a voice-leading schema.<sup><a name="FN15REF" href="#FN15" id="footnote15">(15)</a></sup> This kind of logic also enters into music pedagogy: for instance, students are often counseled to identify modulations using chord progressions (e.g., <a href="#piston_1941" id="citation_piston_1941_67dc99bb18bb8">Piston 1941</a>, <a href="#laitz_2008" id="citation_laitz_2008_67dc99bb18bba">Laitz 2008</a>) or cadences (<a href="#clendinning_and_marvin_2011" id="citation_clendinning_and_marvin_2011_67dc99bb18bbc">Clendinning and Marvin 2011</a>).<sup><a name="FN16REF" href="#FN16" id="footnote16">(16)</a></sup></p>

<p>[3.4] Research in music informatics has relied on feedback mechanisms to determine key for some years. From a computational perspective, feedback systems would include models like Winograd (<a href="#winograd_1968" id="citation_winograd_1968_67dc99bb18bcc">1968</a>), Pardo and Birmingham (<a href="#pardo_and_birmingham_1999" id="citation_pardo_and_birmingham_1999_67dc99bb18bce">1999</a>), Pachet (<a href="#pachet_2000" id="citation_pachet_2000_67dc99bb18bd0">2000</a>), Barthelemy and Bonardi (<a href="#barthelemy_and_bonardi_2001" id="citation_barthelemy_and_bonardi_2001_67dc99bb18bd2">2001</a>), Rohrmeier (<a href="#rohrmeier_2007" id="citation_rohrmeier_2007_67dc99bb18bd4">2007</a>), Illescas, Rizo, and I&ntilde;esta (<a href="#illescas_rizo_and_i&ntilde;esta_2007" id="citation_illescas_rizo_and_i&ntilde;esta_2007_67dc99bb18bd7">2007</a>), Quick (<a href="#quick_2014" id="citation_quick_2014_67dc99bb18bd8">2014</a>), and White (<a href="#white_2015" id="citation_white_2015_67dc99bb18bda">2015</a>). Each of these authors relies on various organizational techniques&#8212;including metric emphasis, chord grammars, or chord progressions&#8212;to determine a passage&#8217;s key.<sup><a name="FN17REF" href="#FN17" id="footnote17">(17)</a></sup> Of particular interest are Raphael and Stoddard (<a href="#raphael_and_stoddard_2004" id="citation_raphael_and_stoddard_2004_67dc99bb18bdf">2004</a>), who use harmonic functions to assist in key finding, and Stoddard, Raphael, and Utgoff (<a href="#stoddard_raphael_and_utgoff_2004" id="citation_stoddard_raphael_and_utgoff_2004_67dc99bb18be1">2004</a>), as they convert MIDI integer notation to letter/accidental notation using key finding and modulation metrics: in both these instances, an organizing feature helps determine the key, while the key helps determine that organizing feature.<sup><a name="FN18REF" href="#FN18" id="footnote18">(18)</a></sup> Similarly, Temperley (<a href="#temperley_1997" id="citation_temperley_1997_67dc99bb18be4">1997</a>) ascertains a passage&#8217;s key using the chord-root patterns of a passage: he almost explicitly states the feedback nature of the task when he writes that his &#8220;analysis can be broken down into two problems: root finding and key finding,&#8221; (34) with both informing the other.<sup><a name="FN19REF" href="#FN19" id="footnote19">(19)</a></sup> (Craig Sapp has summarized many of these key-finding topics in his 2011 dissertation, detailing various available approaches to key finding and how they interact with music theoretic concepts of modulation, circle-of-fifths distance, and harmonic analysis.)<sup><a name="FN20REF" href="#FN20" id="footnote20">(20)</a></sup></p>

<p>[3.5] These theoretical, pedagogical, and computational systems have been much less consistently connected to experimental testing in music cognition than have feedforward systems. A handful of studies address the interconnection of meter and key (<a href="#prince_thompson_and_schmuckler_2009" id="citation_prince_thompson_and_schmuckler_2009_67dc99bb18beb">Prince, Thompson, and Schmuckler 2009</a>; <a href="#prince_and_schmuckler_2014" id="citation_prince_and_schmuckler_2014_67dc99bb18bec">Prince and Schmuckler 2014</a>), and many studies address the role that harmony plays in tonal orientation (<a href="#thompson_and_cuddy_1989" id="citation_thompson_and_cuddy_1989_67dc99bb18bee">Thompson and Cuddy 1989</a>, <a href="#thompson_and_cuddy_1992" id="citation_thompson_and_cuddy_1992_67dc99bb18bf0">Thompson and Cuddy 1992</a>, <a href="#trainor_and_trehub_1994" id="citation_trainor_and_trehub_1994_67dc99bb18bf2">Trainor and Trehub 1994</a>, <a href="#povel_and_jansen_2002" id="citation_povel_and_jansen_2002_67dc99bb18bf3">Povel and Jansen 2002</a>) as well as the role that tonal orientation plays in the stability of or distance between harmonies (<a href="#krumhansl_bharucha_and_castellano_1982" id="citation_krumhansl_bharucha_and_castellano_1982_67dc99bb18bf5">Krumhansl, Bharucha, and Castellano 1982</a>; <a href="#tillmann_et_al_2003" id="citation_tillmann_et_al_2003_67dc99bb18bf7">Tillmann et al. 2003</a>).<sup><a name="FN21REF" href="#FN21" id="footnote21">(21)</a></sup> Some neural network modeling has even been done to account for these interlocking behavioral data (<a href="#tillmann_bharucha_and_bigand_2001" id="citation_tillmann_bharucha_and_bigand_2001_67dc99bb18bfa">Tillmann, Bharucha, and Bigand 2001</a>).</p>

<p>[3.6] These studies, however, generally focus on the role tonal orientation plays in how listeners interpret an already-assumed tertian harmonic structure, or how that harmonic structure affects the perception of key. What remains untested is how the <i>actual organization of pitches into chords</i>&#8212;the act of dividing the eighth notes of Example 1 or parsing the dissonances and consonances of Example 2&#8212;influences and is influenced by a passage&#8217;s tonal orientation.<sup><a name="FN22REF" href="#FN22" id="footnote22">(22)</a></sup> In other words, while theoretical, computational, and pedagogical research has relied on feedback logics to relate harmonic organization and musical key, this relationship has been addressed implicitly, intermittently, and piecemeal in behavioral research.<sup><a name="FN23REF" href="#FN23" id="footnote23">(23)</a></sup></p>

<p>[3.7] In what follows, I implement a key-finding model based on some of my previous work&#8212;the method used to analyze Example 2 (<a href="#white_2015" id="citation_white_2015_67dc99bb18c05">White 2015</a>)&#8212;that incorporates the key and harmony analysis tasks, adding a feedback system between those two domains. Having created this feedback system, I will show that such a model can be validated against human behavior by testing whether the model&#8217;s output conforms to theorists&#8217; intuitions about tonal harmony and by comparing the model&#8217;s output both to human behaviors and to the outputs of representative feedforward tonal models.<sup><a name="FN24REF" href="#FN24" id="footnote24">(24)</a></sup></p>



<h2>Part IV: A Model of Tonal Orientation Using Feedback Between Key and Chord Progressions</h2>

<fig>
<p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 4</b>. A feedback loop between key and chord grouping</p><p class='fullwidth' style="text-align: center; margin-bottom:0em"><a class='youtube'  target="blank" href="white_examples.php?id=3&nonav=true"><img border="1" alt="Example 4 thumbnail" src="white_ex04_small.png"></a></p><p class='fullwidth' style="text-align: center; margin-top:0em"><font size="2">(click to enlarge)</font></p></fig>

<p>[4.1] <b>Example 4</b> schematizes a feedback loop between key finding and chord identification in terms of Example 1b, framing the chord identification task as grouping surface pcs into sets, and the key-finding task as identifying the optimal tonal orientation of those sets.</p>

<p>[4.2] Let us first assume &#8220;key&#8221; to be a particular transposition operation that maps some group of pitch classes onto what we will call mod-12 &#8220;scale degrees.&#8221; Let us assume a scale-degree (sd) set to be formed comparably to pc sets, but with 0 designating the tonic scale degree and with the integers mod-12 representing the chromatic distance from the tonic (such that 2 would be the supertonic degree, 7 would be the dominant degree, 11 the leading tone, and so on). <sup><a name="FN25REF" href="#FN25" id="footnote25">(25)</a></sup> The key operation then transposes the tonic pc to zero, and transposes the remaining pcs by that same distance. Consider a D-major triad followed by a B-minor triad, or the normal-form pc sets <2,&nbsp;6,&nbsp;9> and <11,&nbsp;2,&nbsp;6>; interpreting the progression in D major (as a I&ndash;vi progression) would orient pc 2 with the tonic (mod-12) scale degree, or sd 0. Mapping the pc set <2,&nbsp;6,&nbsp;9> onto sd set <0,&nbsp;4,&nbsp;7> would involve subtracting the tonic degree from the pc set, a T<sup>&minus;2</sup> (or T<sup>10</sup>) operation. Equation 1 generalizes this assumption: observed pcs <i>o</i> at timepoint <i>j</i> in a succession of length <i>n</i> are transposed by some key <i>k</i> in modulo 12 space.</p>

<p>Equation 1:
\[
    \hat{k}_j = \text{argmax}_k(P ([o_{j-n} - k ]_{\bmod 12}, \ldots [o_j -k]_{\bmod 12}) ) \\
    k \in |0 \ldots 11|
\]
</p>

<p>[4.3] The equation also states that the ideal key maximizes (argmax) the probability <i>P</i> of some series of solutions (again, see <a href="#temperley_2007" id="citation_temperley_2007_67dc99bb18c6a">Temperley 2007</a> for more on musical probabilities). While these probabilities could in principle be defined in any number of ways, I use probabilistic Markov chains (also used in <a href="#pearce_and_wiggins_2004" id="citation_pearce_and_wiggins_2004_67dc99bb18c6c">Pearce and Wiggins 2004</a>, <a href="#quinn_2010" id="citation_quinn_2010_67dc99bb18c6d">Quinn 2010</a>, and <a href="#white_2014" id="citation_white_2014_67dc99bb18c6f">White 2014</a>; these are also called <i>n</i>-grams since an event&#8217;s probability is contingent on <i>n</i> preceding events). Equation 2 formalizes this, stating that the probability of a chord <i>s</i> at timepoint <i>j</i> is contingent upon <i>n</i> previous chords.</p>

<p>Equation 2:
\[
    P(s_j) = P(s_j) | P\big(s\,^{j-n}_{j-1}\big)
\]
</p>

<p>[4.4] We might intuitively connect Equation 1 to this Markov chain by treating a tonally oriented observation (\(o_{j-k}\)) as a member \(s_j\) in the chain, with the best key <i>k</i> at timepoint <i>j</i> being that which transposes Equation 1&#8217;s series of observations (\(o_{j-n} \ldots o_j\)) to the most probable series of chords (\(s_{j-n} \ldots s_j\)). (NB: the relationship between <i>s</i> and <i>o</i> will be more thoroughly formalized below). <b>Example 5</b> shows a chordal syntax based on Kostka and Payne (<a href="#kostka_and_payne_2012" id="citation_kostka_and_payne_2012_67dc99bb18c71">2012</a>) that we can treat as a toy 2-chord (or, <i>2-gram</i>) probabilistic Markov chain. In the example&#8217;s toy system, we can imagine that any chord progression is possible, but the diagram&#8217;s arrows show only the most expected&#8212;the most probable&#8212;chord progressions.<sup><a name="FN26REF" href="#FN26" id="footnote26">(26)</a></sup> <b>Example</b> <b>6</b> reproduces the music of Example 1 with pitch-class sets now grouped into beats (i.e., half-note durations) and using letter names for ease of reading.<sup><a name="FN27REF" href="#FN27" id="footnote27">(27)</a></sup> We can apply Example 5&#8217;s toy syntax to Equations 1 and 2 to analyze the music of Example 3. The different scale-degree interpretations of the constituent sets would return varying levels of probability, and several such interpretations are shown below the example (using Roman numerals instead of scale-degree sets). While the first passage could be analyzed as I&ndash;vi&ndash;ii&ndash;V7, V&ndash;iii&ndash;vi&ndash;V7/V, or even IV&ndash;ii&ndash;v&ndash;I7 or <nobr><span style= 'letter-spacing:-1px'><span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>II&ndash;<nobr><span style= 'letter-spacing:-1px'><span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>vii&ndash;<nobr><span style= 'letter-spacing:-1px'><span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>iii&ndash;<nobr><span style= 'letter-spacing:-1px'><span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>VI7, the first progression coheres more readily with our assumed model.</p>

<table width=800><tr><td valign='top' width=50%><p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 5</b>. A hypothetical toy chord-progression model, with arrows representing the most probable between-chord successions</p><p class='fullwidth' style="text-align: center;"><a class='youtube'  target="blank" href="white_examples.php?id=4&nonav=true"><img border="1" alt="Example 5 thumbnail" src="white_ex05_small.png"></a></p><p class='fullwidth' style="text-align: center"><font size="2">(click to enlarge)</font></p></td><td width=50></td><td valign='top' width=50%><p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 6</b>. A probabilistic analysis of Mozart K. 284, iii, mm. 1&ndash;8, using the syntax of Example 5</p><p class='fullwidth' style="text-align: center;"><a class='youtube'  target="blank" href="white_examples.php?id=5&nonav=true"><img border="1" alt="Example 6 thumbnail" src="white_ex06_small.png"></a></p><p class='fullwidth' style="text-align: center"><font size="2">(click to enlarge)</font></p></td></tr></table>
<p>[4.5] Note, however, that several of the &#8220;chords&#8221; identified ignore some subset of the observed pitches. Both dotted boxes include the pcs {<nobr><span style= 'letter-spacing:-0.8px'>C<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr>, D, E, <nobr><span style= 'letter-spacing:-0.8px'>F<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr>, B}: they are notably not triads, and yet the analysis treats them as such, &#8220;translating&#8221; the surface into a tertian chord. The translation process hinges upon what we mean by &#8220;chords.&#8221; This question is anything but trivial: over the centuries, the vocabularies of chords theorized as underpinning tonal music have taken the form of diatonic chord roots (<a href="#rameau_1726" id="citation_rameau_1726_67dc99bb18d07">Rameau 1726</a>, <a href="#d&#8217;alembert_1752" id="citation_d&#8217;alembert_1752_67dc99bb18d09">d&#8217;Alembert 1752</a>), the seven triads built upon these chord roots (<a href="#vogler_1776" id="citation_vogler_1776_67dc99bb18d0b">Vogler 1776</a>, <a href="#weber_1817" id="citation_weber_1817_67dc99bb18d0c">Weber 1817</a>) with sevenths possibly included (<a href="#reicha_1818" id="citation_reicha_1818_67dc99bb18d0e">Reicha 1818</a>, <a href="#stumpf_1883" id="citation_stumpf_1883_67dc99bb18d0f">Stumpf 1883</a>), or the alterations and deformations of underlying tertian harmonies (<a href="#sechter_1853" id="citation_sechter_1853_67dc99bb18d11">Sechter 1853</a>, <a href="#kurth_1920" id="citation_kurth_1920_67dc99bb18d12">Kurth 1920</a>). In some theories, the universe of &#8220;true&#8221; chords might even be limited to a group of harmonic functions (<a href="#riemann_1893" id="citation_riemann_1893_67dc99bb18d14">Riemann 1893</a>, <a href="#harrison_1994" id="citation_harrison_1994_67dc99bb18d15">Harrison 1994</a>, <a href="#agmon_1995" id="citation_agmon_1995_67dc99bb18d17">Agmon 1995</a>, and <a href="#kopp_2002" id="citation_kopp_2002_67dc99bb18d19">Kopp 2002</a>), while in others it might even be expanded to include all structures that composers use in predictable and syntactic ways (<a href="#quinn_2010" id="citation_quinn_2010_67dc99bb18d1a">Quinn 2010</a>, <a href="#quinn_and_mavromatis_2011" id="citation_quinn_and_mavromatis_2011_67dc99bb18d1c">Quinn and Mavromatis 2011</a>, <a href="#white_and_quinn_forthcoming" id="citation_white_and_quinn_forthcoming_67dc99bb18d1e">White and Quinn Forthcoming</a>).</p>

<p>[4.6] What unites these theories is that there exists some set of harmonic prototypes into which surface pitches organize themselves, promoting those pitches that conform to the prototypes, excluding those pitches that do not conform (e.g., the act of excluding surfaces dissonances), and even changing or projecting notes not actually present on the surface (e.g., recognizing a chord without a fifth as representing a triad, or reading a ii chord as substituting for a IV chord). I adopt some of my previous work in <a href="#white_2013b" id="citation_white_2013b_67dc99bb18d1f">White 2013b</a> to formalize this relationship between surface and prototype, a relationship shown in Equation 3. The algorithm acts upon a series of scale-degree sets <i>D</i> with time points 1 to <i>n</i> such that \(D = (d_1, d_2, \ldots d_n)\), reducing them to other scale-degree sets <i>s</i> such that \(S = (s_1, s_2, \ldots s_n)\) and \(|s_i \cap o_i| > 1\) where the cardinality of the intersection between each <i>d</i> and its corresponding <i>s</i> is at least 1 (i.e., they share at least one scale-degree). The equation then produces a &#8220;translated&#8221; chord \(\hat{s}\) given the context \(\gamma\) and the proximity of the two sets \(\pi\). Here, \(P(s_i|\gamma(d_i))\) is the probability that a given <i>s</i> would occur in the context \(\gamma\) in which we observe the corresponding <i>d.</i>  The maximized argument now optimally groups the observed scale degrees of series <i>D</i>. At each point<i> j</i> the set <i>d</i> includes all previous observed sets: the equation therefore determines how large a window results in the optimal vocabulary item.</p>

<p>Equation 3: Grouping a surface
\[
    \hat{s} = \text{argmax}_j\ P (s_j|\gamma(d_j))\ \pi (s_j, d_j ) \\
    d_j = d_{j-1} \cup d_j \\
    j \in |0 \ldots n |
\]
</p>

<p>[4.7] So defined, connecting key and chord into a Markov chain entails a feedback system. Knowing the ideal parsing of the musical surface (Equation 3) is contingent on knowing the ideal key (Equation 1); but, the reverse is also true: knowing the key is contingent upon knowing the ideal parsing of the musical surface. I show this interconnection in Equation 4, with the scale degree sets \(d_j\) now replaced with the tonally-oriented sets \([o_j-k]\), and the probabilistic relationship \(\gamma\) is now replaced with Equation 2&#8217;s Markov-chain probabilities (again, my implementation uses 2-grams, or <i>n</i>=1). Additionally, \(\pi\) will correspond to the amount of overlap (or intersection cardinality) between the two sets. Here, the best scale-degree set not only maximizes the key <i>k</i> but also maximizes the grouping at timepoint <i>j</i>. For the vocabulary of chords <i>S</i>, I adopt the machine-learned tertian (probabilistic) syntax developed in previous work (<a href="#white_2013b" id="citation_white_2013b_67dc99bb18d30">White 2013b</a>): the constituent chords are I, i, V, V7, IV, iv, ii, ii7, ii&deg;, ii<sup>&oslash;</sup>7, vi, vi7, iii, <nobr><span style= 'letter-spacing:-1px'><span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>VI, <nobr><span style= 'letter-spacing:-1px'><span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr>VII, V7/IV, V7/V. <sup><a name="FN28REF" href="#FN28" id="footnote28">(28)</a></sup> This analysis process therefore reduces both dotted boxes of Example 3 into the chord {B, D, <nobr><span style= 'letter-spacing:-0.8px'>F<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr>} by recognizing that a) this triad is a subset of the larger surface set, and b) that triad can maximizes the probabilities of Example 5&#8217;s syntax.<sup><a name="FN29REF" href="#FN29" id="footnote29">(29)</a></sup></p>

<p>Equation 4: An integration of two parameters
\[
    \hat{s} = \text{argmax}_{k,j}\ P \Big( P(s_j) | P \big( s^{j-n}_{j-n} \big) \Big) \ \pi(s_j, [o_j - k]) \\
    o_j = o_{j-1} \cup o_j \\
    j \in |0 \ldots n|     \\
    k \in |0 \ldots 11|    \\
    \pi( \hat{s},[o_j -k] )\ \propto\ |\hat{s} \cap [o_j - k ]|

\]
</p>

<p>[4.8] To maximize multiple parameters in tandem, I implement a feedback loop using the <i>Maximum Product Algorithm</i> (or Viterbi Algorithm). Considering all possible chord interpretations and key orientations at each timepoint would be intractable, as the possibilities exponentially increase at each timepoint. This integration therefore uses a process borrowed from language processing, the Viterbi algorithm (<a href="#viterbi_1967" id="citation_viterbi_1967_67dc99bb18d38">Viterbi 1967</a>, <a href="#jurafsky_and_martin_2000" id="citation_jurafsky_and_martin_2000_67dc99bb18d3a">Jurafsky and Martin 2000</a>). In my adaptation of the algorithm, at each point the possible chord parsings and key solutions are arrayed within a table, and the algorithm navigates through the possible options to produce the ideal solution that maximizes all parameters. As formalized in Pseudocode 1, the algorithm divides the surface into scale-degree sets and returns at each timepoint <i>t</i> and for each key <i>k</i> the best vocabulary item&#8212;or &#8220;chord&#8221; <i>s</i>&#8212;to underlay that moment. The overall probability is calculated by combining the probability \(\pi\) of the vocabulary item \(s_t\) given the surface pitch classes \(o_t\), and the probability of the previous item \(s_{t-1}\) transitions into the current item \(s_t\) according to the transition matrix <i>A</i>. Instead of calculating the probability of each possible path through the interpretations, the algorithm only uses the probability of only the <i>most probable pathway so far for each key</i> at each increasing timepoint such that there are only <i>K</i> number of previous pathways at any point in time.<sup><a name="FN30REF" href="#FN30" id="footnote30">(30)</a></sup> The analyses of Example 3, then, are the result of navigating the interrelationships of Equations 1&#8211;4 using the product maximization feedback system.</p>

<p>Pseudocode 1: The product maximization process</p>

<ul>
    <li>Input
        <ul>
            <li>A series of observed pc sets \(O = (o_1, o_2, \ldots o_n) \).</li>
            <li>A vocabulary of scale-degree sets \(S = \{s_1, s_2, \ldots s_L\}\).</li>
            <li>A transition matrix \(A\) of size \(L \cdot L\) defining the transition probabilities between each \(s\) in \(S\).
                (\(A_{s_{t-1}s_t}\) therefore defines the probability of transition between two scale-degree sets at adjacent timepoints.)</li>
            <li>A series of probabilistic relationships \(\Pi\) defining mappings between each \(o\) in \(O\) and each \(s\) in \(S\).
                (\(\Pi_{o_ts_t}\) therefore defines a probabilistic mapping between a pc set<i></i> and a scale-degree set at timepoint \(t\).)</li>
            <li>A number of keys \(K\).</li>
        </ul>
    </li>
    <li>Process
        <ul>
            <li>Construct two 2-dimensional tables \(T_1\) and \(T_2\) of size \(T \times K\)
                with coordinates of timepoints \(t\) and keys \(k\), such that:
                <ul>
                    <li>\(T_1[t,k]\) stores the probability of the most likely path so far at timepoint \(t\) in key \(k\)</li>
                    <li>\(T_2[t,k]\) stores the scale-degree set \(x_{t-1}\) resulting from the most likely path
                        so far at timepoint \(t\) in key \(k\)</li>
                </ul>
            </li>
            <li>Table entries are filled in increasing order at each \(t\) in \(T\) such that:
                <ul>
                    <li>\(T_1[t,k] = \text{max}_s\ T_1[t,k] \times A_{x_{t-1}x_t} \times \Pi_{o_tx_t}\)</li>
                    <li>\(T_2[t,k] = \text{argmax}_s\ T_1[t-1, k] \times A_{x_{t-1}s_t} \times \Pi_{o_ts_t}\)</li>
                </ul>
            </li>
        </ul>
    </li>
    <li>Output
        <ul>
            <li>A path \(X = (x_1, x_2, \ldots x_T)\) which is a sequence of scale-degree sets such that \(x_n \in S\)
                that traces the cells of \(T_2\) that correspond to the maximization sequence of \(T_1\).</li>
        </ul>
    </li>
</ul>


<h2>Part V: Validating the Feedback Model</h2>

<p>[5.1] Having computationally defined a feedback system of musical key, I now present studies that test its connections to established music theories, to the behavior and intuition of musicians, and even to the music theory classroom. These tests will begin to offer a richer picture of how such a feedback system behaves, how it compares to feedforward systems, and how the viewpoints of the various domains of music scholarship (theory, computation, cognition, and pedagogy) might interact with this model. In other words, I address the deficiency in the research surrounding feedback models of musical key by testing a &#8220;proof of concept&#8221; model, a model that shows that feedback logics can represent musical behavior, imitate pedagogical practice, and conform to the published theories of tonal practice.</p>

<p>[5.2] The next several sections therefore divide the feedback model&#8217;s processes into its constituent components: key finding, chord grouping, and its resulting analysis. <i>Test 1</i> first observes whether this model assigns key to musical passages with accuracy comparable to other key-finding models, testing its computational validity. <i>Test 2</i> then observes whether the model groups chords in ways comparable to existing models of tonal harmony, testing its relationship to music theory. <i>Test 3</i> and <i>Test 4</i> then determine whether the model analyzes music similarly to undergraduates, testing its ability to mimic and predict human behavior. Overall, these tests will give a multifaceted view on the characteristics, strengths, and weaknesses of this feedback model of key finding.<sup><a name="FN31REF" href="#FN31" id="footnote31">(31)</a></sup> (All computation was implemented in the Python language using the music21 software package, as described in <a href="#cuthbert_and_ariza_2011" id="citation_cuthbert_and_ariza_2011_67dc99bb18d4c">Cuthbert and Ariza 2011</a>).</p>

<p>[5.3] <i>Test 1: Comparing the performance of different key-finding models.</i> In order to compare how well different key-finding models performed, I used the MIDI files of the Kostka-Payne corpus (<a href="#temperley_2009a" id="citation_temperley_2009a_67dc99bb18d4e">Temperley 2009a</a>), analyzed their openings with three different feedforward (key-profile) models as well as the current study&#8217;s feedback model, and compared their results with the keys identified in the textbook&#8217;s analyses. Forty-one of the corpus&#8217;s MIDI files were used, spanning the Baroque to the Romantic eras.<sup><a name="FN32REF" href="#FN32" id="footnote32">(32)</a></sup></p>

<p>[5.4] Three key profiles were used as the feedforward models, and were drawn from music21&#8217;s library of key-finding functions: the Krumhansl-Schmuckler (<a href="#krumhansl_1990" id="citation_krumhansl_1990_67dc99bb18d51">Krumhansl 1990</a>), the Temperley-Kostka-Payne (<a href="#temperley_2007" id="citation_temperley_2007_67dc99bb18d52">Temperley 2007</a>), and Bellman-Budge (<a href="#bellman_2005" id="citation_bellman_2005_67dc99bb18d54">Bellman 2005</a>) weightings. Various window lengths were attempted for these analyses, and it was found that using the first 6 offsets of each piece produced the highest consistent results. (A similar test of only key-profile methods was run in Albrecht and Shanahan (<a href="#albrecht_and_shanahan_2013" id="citation_albrecht_and_shanahan_2013_67dc99bb18d56">2013</a>); the test produced different rates of success than those reported here. While our different implementations do result in different values, the models&#8217; relative performances basically track one another; however, it should be noted these authors&#8217; reported higher success rates than those reported below and comparable to those values associated with my feedback approach.)</p>

<p>[5.5] For analysis using the feedback system, the first 20 salami slices (verticalities at which at least one pc is added or subtracted from the texture) of each file were analyzed, and the tonic associated with the most probable scale-degree interpretation was recorded. <sup><a name="FN33REF" href="#FN33" id="footnote33">(33)</a></sup></p>

<fig>
<p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 7</b>. The percentage of key assessments on the Kostka-Payne corpus that agreed with the instructor&rsquo;s edition, divided by model</p><p class='fullwidth' style="text-align: center; margin-bottom:0em"><a class='youtube'  target="blank" href="white_examples.php?id=6&nonav=true"><img border="1" alt="Example 7 thumbnail" src="white_ex07_small.png"></a></p><p class='fullwidth' style="text-align: center; margin-top:0em"><font size="2">(click to enlarge)</font></p></fig>

<p>[5.6] As shown in <b>Example 7</b>, of the 41 pieces analyzed, the chord-progression model assigned the same tonic triad as the textbook 87.8% of the time. Of the 5 that did not overlap, twice the model judged the key to be the passage&#8217;s relative major, twice the keys differed by fifth, and once the passage was too scalar for the model to recognize the correct underlying chords. Example 7 also shows how often each key-profile model produced correct answers. The profiles of Krumhansl-Schmuckler, Temperley-Kostka-Payne, and Bellman-Budge produced rates of 78.0%, 85.4%, and 73.2% correct, respectively.</p>

<p>[5.7] As implemented here (and with the caveats regarding <a href="#albrecht_and_shanahan_2013" id="citation_albrecht_and_shanahan_2013_67dc99bb18da1">Albrecht and Shanahan 2013</a>), the feedback model performs similarly to, if not better than each of the other computational models. These results suggest that a feedback approach can find a passage&#8217;s key with accuracy comparable to established feedforward analyses. (And, given that the model is being tested against a textbook, this test begins to connect the computational side feedback systems to both the pedagogical and theoretical dimensions.) However, it remains to be seen whether the way in which the model understands chords and chord progressions resembles that of contemporary music-theory discourse and pedagogy. The following test, then, compares the feedback model to other published models of tonal harmony, essentially testing whether this feedback system uses a harmonic syntax comparable to those used by professional music theorists.</p>

<p>[5.8] <i>Test 2: Comparing the model to the Kostka-Payne corpus (and to other models).</i> Test 2 asks whether the feedback model&#8217;s syntax overlaps with published models of harmonic syntax, thereby testing this feedback system&#8217;s relationship to models proposed within and used by music theorists. In particular, I investigate this by a) implementing five models based on other authors&#8217; work that capture some aspects of harmonic syntax, b) implementing the above-described feedback model, and then c) using these six models to assess the probability of a corpus of human-made chord-progression analyses drawn from the Kostka-Payne textbook. I will quantify and compare these probabilities using <i>cross entropy,</i> a measurement of how well a model overlaps with the series of chords it is observing,  a procedure I will describe below.</p>

<p>[5.9] <i>Test 2: The Models &#8211;</i> The five models include three Western European common-practice models and two models drawn from a decidedly different repertoire, American 20th-century popular music. Each model uses 2-chord probabilistic Markov chains. I name each model after the author(s) and source behind its dataset: the three common-practice models are the Tymoczko-Bach, the Quinn-YCAC, and the Temperley-Kostka-Payne; the popular music models are the deClercq-Temperley and the McGill-Billboard. The Tymoczko-Bach corpus comprises the datasets reported in Tymoczko (<a href="#tymoczko_2011" id="citation_tymoczko_2011_67dc99bb18da4">2011</a>): this source uses hand-analyzed Bach chorales and tallies how frequently each diatonic Roman numeral moves to each other. This model&#8217;s vocabulary includes triads on the standard major scale degrees with the addition of those on the minor third and minor sixth scale degrees as well, producing a total of nine chords.<sup><a name="FN34REF" href="#FN34" id="footnote34">(34)</a></sup> (I use the word <i>vocabulary</i> here to mean the universe of possible chords within a system.) The 2-gram (i.e., Markov chains of 2-chord progressions) transitions of the Temperley-Kostka-Payne corpus are those reported in Temperley (<a href="#temperley_2009a" id="citation_temperley_2009a_67dc99bb18da6">2009a</a>) and are drawn from the harmonic analyses within the instructor&#8217;s edition to the Kostka-Payne harmony textbook. This corpus involves only root information and the mode of the excerpt, totaling 919 annotations. (Here, V7, V, and v would all be represented as ^5, conflating chords that are distinguished in several of the other models.<sup><a name="FN35REF" href="#FN35" id="footnote35">(35)</a></sup>) The Quinn-YCAC, the largest corpus used here, is comprised of transitions between each chord in the YCAC (<a href="#white_and_quinn_2016b" id="citation_white_and_quinn_2016b_67dc99bb18db6">White and Quinn 2016b</a>). (The &#8220;chords&#8221; here consist of this corpus&#8217;s salami slices, those verticalities arising each time a pitch is added or subtracted from the texture.)<sup><a name="FN36REF" href="#FN36" id="footnote36">(36)</a></sup> The deClercq-Temperley model uses the same annotations as the Kostka-Payne corpus (chord roots without modal or figure designations) but is drawn from a corpus of popular music (<a href="#declercq_and_temperley_2011" id="citation_declercq_and_temperley_2011_67dc99bb18db8">deClercq and Temperley 2011</a>). Finally, the McGill-Billboard corpus uses key-centered leadsheet notations, thereby introducing seventh chords, incomplete chords, and dissonances into its vocabulary (<a href="#burgoyne_wild_and_fujinaga_2011" id="citation_burgoyne_wild_and_fujinaga_2011_67dc99bb18dba">Burgoyne, Wild, and Fujinaga 2011</a>). Its vocabulary includes 638 distinct chords.<sup><a name="FN37REF" href="#FN37" id="footnote37">(37)</a></sup> The same model was used as in the previous test: I will refer to it here as the &#8220;Feedback YCAC&#8221; model.</p>

<p>[5.10] <i>Test 2: Cross entropy &#8211;</i> As in the previous test, the Kostka-Payne harmony textbook was used as a ground truth; the Temperley-Kostka-Payne corpus was therefore used as a baseline from which to compare other models. To make these comparisons, I used several measurements involving the <i>cross entropy</i> resulting from each model assessing this corpus, a measurement that shows how different two models are (<a href="#temperley_2007" id="citation_temperley_2007_67dc99bb18dbd">Temperley 2007</a>): the higher the cross entropy, the more &#8220;surprised&#8221; a model is by what it is observing. In other words, models that returned lower cross entropies would better conform to this (literally) textbook model of harmony.<sup><a name="FN38REF" href="#FN38" id="footnote38">(38)</a></sup></p>

<p>[5.11] Equation 6 shows the formula for calculating average cross entropy. With a probabilistic model <i>m</i> assessing some set of events <i>O</i>, cross entropy <i>H</i> represents how well <i>m</i> predicts <i>O,</i> with <i>m</i> assessing the probability of each <i>o</i> in <i>O</i>, or \(m(o)\). In our tests, <i>O</i> is the series of chord progressions being analyzed, and <i>m</i> is one of the 2-gram chord-progression models. Conforming to the norms of information theory, the value&#8217;s base-2 logarithm is used.<sup><a name="FN39REF" href="#FN39" id="footnote39">(39)</a></sup> These logarithms are averaged over the series of length <i>n</i>, and the negative sign transforms the negative logarithm into a positive value (<a href="#temperley_2010" id="citation_temperley_2010_67dc99bb18dc0">Temperley 2010</a>).</p>

<p>Equation 6: Cross Entropy
\[
    H_m(O) = -\frac{1}{n}\, \log_2\big(m (o_1, o_2, \ldots o_n ) \big)
\]
</p>

<p>[5.12] However, the varying sizes and components of each model&#8217;s harmonic vocabulary make a single cross entropy measurement insufficient&#8212;trying to compare apples to oranges requires multifaceted descriptors. Therefore, two differently-executed cross entropy measurements were taken, along with two additional supplemental values. These two approaches represent different ways of dealing with zero-probability events, or progressions that are in the Temperley-Kostka-Payne corpus (the progressions being observed, <i>O</i>) but not present in one of the models (or, <i>m</i>).<sup><a name="FN40REF" href="#FN40" id="footnote40">(40)</a></sup> The first solution uses what informatics researchers call &#8220;smoothing&#8221;: the <i>with-smoothing</i> cross entropy measurement ascribes a very low probability (but not zero!) to all such zero-probability events.<sup><a name="FN41REF" href="#FN41" id="footnote41">(41)</a></sup> These smoothed non-zero probabilities will penalize the model if it has less overlap with its observed chord progressions, yielding a higher cross entropy (or, more &#8220;surprise&#8221;). The second <i>without smoothing</i> cross entropy measurement ignores those 2-grams that the model has never seen before, passing over them as its probability assessments are made. This approach captures how well each model performs when its vocabulary and 2-grams overlap with those of the Kostka-Payne observations; however, if a model overlaps with the observed chord progressions only during high probability events, the cross entropy will be relatively low (i.e., low &#8220;surprise&#8221;) even though the process ignores most of the observed sequence. Therefore, to observe the amount of overlap between corpora, the percentage of the Kostka-Payne 2-grams excluded in the without-smoothing measurement was calculated as the <i>exclusion rate</i>. Finally, the size of each model&#8217;s chord vocabulary was also represented.<sup><a name="FN42REF" href="#FN42" id="footnote42">(42)</a></sup></p>

<p>[5.13] As a baseline, the Kostka-Payne model assessed itself as well. Using a corpus&#8217;s statistics to assess its own data is not strictly a cross-entropy measurement, but rather a measurement that captures the corpus&#8217;s overall complexity&#8212;a more complex system will have more trouble predicting itself than will a simple system. This self-assessment will then let us ask how much better or worse other models predict the Kostka-Payne corpus versus the corpus&#8217;s own predictions of itself.</p>

<fig>
<p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 8</b>. Cross entropy results for each corpus-based model</p><p class='fullwidth' style="text-align: center; margin-bottom:0em"><a class='youtube'  target="blank" href="white_examples.php?id=7&nonav=true"><img border="1" alt="Example 8 thumbnail" src="white_ex08_small.png"></a></p><p class='fullwidth' style="text-align: center; margin-top:0em"><font size="2">(click to enlarge)</font></p></fig>

<p>[5.14]<i> Test 2: Comparing the Various Chord-Progressions Models.</i> <b>Example 8</b> shows cross-entropy quantities for the six models used to assess the Kostka-Payne 2-grams, allowing for one to see whether, in fact, the feedback model creates chord progressions that have a relationship to the Kostka-Payne textbook comparable to other tonal models. The first group of bars shows the Kostka-Payne corpus predicting itself, returning both cross entropies of 2.4. Since the model and the observations are identical, the observations would never present the model with &#8220;impossible&#8221; 2-grams: therefore, 100% of the corpus is used in the first cross-entropy value (yielding a 0% exclusion rate), and since no smoothing is necessary, the second value is identical.</p>

<p>[5.15] The Quinn-YCAC model provides the lowest percentage of exclusion: only 4% of the Kostka-Payne 2-grams are not present in its model. This low exclusion is not surprising, given that the model has such a large vocabulary of salami slices. The model&#8217;s size also can account for its relatively high unsmoothed and smoothed cross entropies, 3.4 and 4.0, respectively: with its probability mass divided among its large vocabulary of slices, the probabilities it assesses will be relatively low. In contrast, the Tymoczko-Bach model produces a high exclusion rate and considerably different smoothed and unsmoothed cross entropies due to the size of the model. Since it uses only diatonic triads, the model assesses diatonic progressions with high probabilities and excludes or smooths all chromatic progressions, yielding the low unsmoothed cross entropy (1.9) and high smoothed cross entropy (3.9).</p>

<p>[5.16] While using the same method of annotation as the observed corpus, the differences in pop/rock&#8217;s musical syntax caused the deClercq-Temperley model to perform somewhat poorly. Even though its unsmoothed cross entropy is relatively low, its smoothed cross entropy is relatively high due to the presence of root progressions in the Kostka-Payne corpus that never occur in the popular music corpus. For instance, chords with a root of <nobr><span style= 'letter-spacing:-0.8px'><span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr><span style='font-size:0.8em'><math><mover><mn>4</mn><mo>&#710;</mo></mover></math></span> or <span style='font-size:0.8em'><math><mover><mn>7</mn><mo>&#710;</mo></mover></math></span> never progress to <span style='font-size:0.8em'><math><mover><mn>5</mn><mo>&#710;</mo></mover></math></span> in the popular music corpus, yet both happen with some frequency in the common practice. The McGill-Billboard corpus&#8217;s unsmoothed and smoothed values (2.49 and 3.45, respectively, with an 8% exclusion rate) can be attributed to the relative size of its chord vocabulary (as in the YCAC salami-slice model) combined with its contrasting syntax (as in the deClercq-Temperley model). Finally, the Feedback-YCAC model returns an unsmoothed cross entropy of 2.7, an exclusion rate of 10%, and a smoothed cross entropy of 3.9.</p>

<p>[5.17] These results show that cross-entropy values do benefit from using the same chord vocabulary (as in the deClercq-Temperley model) and from a small vocabulary (as in the Tymoczko-Bach model), and that exclusion rates benefit from a large vocabulary (as in the YCAC salami-slice and McGill-Billboard models). The Feedback-YCAC model produces results that somewhat balance these factors: even though it uses the same underlying dataset as does the salami-slice model, its cross-entropy rates approach the more-constrained Tymoczko model while sacrificing less exclusion.</p>

<p>[5.18] In sum, the feedback model seems to provide a sufficient method to organize a musical surface into chords that approximate the Kostka-Payne textbook with similar precision to other published models. Importantly these results do not argue that the feedback model <i>ideally</i> represents textbook models or even human intuition. However, there exists variation between how different theorists would define an <i>ideal</i> model (the difference between the Kostka-Payne and Tymoczko-Bach models evidence the potential variance between different human intuitions when creating hand-annotated corpora) and the feedback model seems to exist within this window and does so better than a simple surface model or models using a foreign syntax.</p>

<p>[5.19] The model therefore seems to use a harmonic system somewhat comparable to those used within this sample of music theory discourse, forging a stronger connection between the computational and the theoretical dimensions. To extend this theoretical connection and begin connecting the model to behavior, I now turn my attention to how well the model identifies chords, prolongations, and points of modulation. The following test therefore compares the output of the analytical model to Roman numeral analyses of the same excerpts produced by undergraduate music majors. In other words, I now test the model&#8217;s behavior against that of trained musicians in a pedagogical context.</p>

<p>[5.20] <i>Test 3: Comparing against the performance of music majors</i>. Of the Kostka-Payne excerpts whose key was successfully analyzed by the Feedback YCAC model in Test 1, 32 were chosen to be analyzed in depth by both humans and the feedback model. The examples were selected to control for the passages&#8217; different characteristics, dividing into four groups with 8 examples in each group. Groups were labeled &#8220;Simple,&#8221; &#8220;Modulating,&#8221; &#8220;Chromatic,&#8221; and &#8220;Chromatic Modulating.&#8221; (I chose these categories in relation to their placement in the Kostka-Payne textbook and the type of analytical knowledge they seemed designed to teach or reinforce.)</p>

<p>[5.21] The model&#8217;s parameters were set as in Test 1, with several additions to allow for whole excerpts to be analyzed.<sup><a name="FN43REF" href="#FN43" id="footnote43">(43)</a></sup> For the automated analysis, the algorithm was run using a <i>moving window</i> designed to provide the model with a consistent number of non-repeating chords. The window began with four chords, but if the feedback process reduced that span to fewer than 4 chords, the window was extended until the process produced a four-chord analysis.<sup><a name="FN44REF" href="#FN44" id="footnote44">(44)</a></sup> The window then moved forward, progressing through the piece. I transcribed the results of this windowed analyses using a &#8220;voting&#8221; process. If three or more windows agreed on a scale-degree set at any given timepoint, the Roman numeral annotation of that set was placed in the score at the appropriate timepoint. Using this voting process allowed the model to read modulations: for instance, if three earlier timepoints read a C-major triad as I, and three later timepoints read it as IV, then both those Roman numerals were placed under that moment in the score. If there was no agreement, a question mark was placed in the score. Since the final windows in the piece would only produce one or two annotations, the automated annotations ended before the example&#8217;s final measure in several examples. If a series of chords was reduced to a single annotation, a line was used to indicate the prolongation of that single chord.</p>

<p>[5.22] The same 32 examples were analyzed by 32 undergraduates in their fourth and final semester of a music-theory sequence at the University of North Carolina at Greensboro&#8217;s School of Music, Theater and Dance. (Six subjects were drawn from my theory section, 26 were not.) Students were given 10 minutes to analyze their example and were asked to use Roman numerals without noting inversion. As in the algorithmic analyses, if the student could not make sense of a chord, they were instructed to place a question mark under that chord. In situations where the model&#8217;s moving window did not allow for the annotations to extend to the end of the excerpt, the unannotated music was covered in a grey box: just as in the automated analysis, the students could take the greyed music into consideration when making their analyses, but they were asked to not annotate this music. (The full instruction page, as well as all examples, can be found at <a href="http://chriswmwhite.com/mto-supplement">http://chriswmwhite.com/mto-supplement</a>.)</p>

<p>[5.23] Eight music-theory faculty (each from different institutions, all currently teaching a music theory or fundamentals class) were then asked to grade a randomly selected group of 8 analyses, not knowing which of the set was algorithmically or human generated. The theorists were asked to provide a grade from 0 to 10 for each example, indicating the level of expertise in Roman numeral analysis the annotations seem to convey (i.e., 0 = a student with no music theory experience, 10 = the expertise of a professional).</p>

<fig>
<p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 9</b>. Comparing graders&rsquo;s assessments of both groups of analyses</p><p class='fullwidth' style="text-align: center; margin-bottom:0em"><a class='youtube'  target="blank" href="white_examples.php?id=8&nonav=true"><img border="1" alt="Example 9 thumbnail" src="white_ex09_small.png"></a></p><p class='fullwidth' style="text-align: center; margin-top:0em"><font size="2">(click to enlarge)</font></p></fig>

<p>[5.24] <b>Example 9</b> shows the average grades given to the computer model and to the human analyses.<sup><a name="FN45REF" href="#FN45" id="footnote45">(45)</a></sup> The example includes error bars that indicate the window of statistical significance surrounding each result.<sup><a name="FN46REF" href="#FN46" id="footnote46">(46)</a></sup> Note that these bars slightly overlap, indicating that while the averages are different, they are not quite <i>significantly</i> different<sup><a name="FN47REF" href="#FN47" id="footnote47">(47)</a></sup>&#8212;while the computer was given consistently higher grades, the variation renders these grades statistically indistinguishable from those given to the human analyses.<sup><a name="FN48REF" href="#FN48" id="footnote48">(48)</a></sup></p>

<p>[5.25] These results indicate that the grades assigned by professional music theorists to the human and automated feedback-generated analyses were statistically similar, and that there was even a trend toward the computer performing slightly higher. Even though neither computer nor human perform with anywhere near an &#8220;expert&#8221; proficiency (in an ideal world, one would hope that both the computer and our undergraduates would perform somewhat better than these grades!), these results do suggest comparable behavior between humans and computer, as they perform within the same range on this task. (In terms of my earlier formulation, this test now connects the computational model not only to the theory and pedagogy of key, but also to behavioral evidence.) But this test does not quantify specific differences between the two groups of analyses: are the humans and computer making the same types of errors and achieving similar successes? To answer this, I ran a final test in which expert theorists were asked to distinguish between two analyses of the same example, one produced algorithmically and one produced by a human.</p>

<p>[5.26] <i>Test 4: A musical Turing Test.</i> In 1950, the &#8220;father of the computer&#8221; Alan Turing proposed a simple way to judge whether a machine&#8217;s behavior approximates that of a human. In what has become known as a &#8220;Turing Test,&#8221; humans engage in typed conversations in front of a screen, not knowing whether the other conversant was another human or a computer algorithm. If a critical mass of participants could not distinguish between the human and computer-generated conversations more than a certain percentage of the time, the computer program could be seen as approximating human behavior.<sup><a name="FN49REF" href="#FN49" id="footnote49">(49)</a></sup></p>

<p>[5.27] Musical Turing Tests have been a part of algorithmic musical creation for some time. Since much algorithmic composition is designed to &#8220;pass&#8221; for convincing musical utterances in the concert hall, work like Cope (<a href="#cope_1987" id="citation_cope_1987_67dc99bb18e76">1987</a>, <a href="#cope_2005" id="citation_cope_2005_67dc99bb18e78">2005</a>) and Quick (<a href="#quick_2014" id="citation_quick_2014_67dc99bb18e79">2014</a>) undertake implicit Turing Tests. However, explicit Turing Tests are frequently used by researchers and engineers whose aim is to create pleasing and human-like music, be it chord progressions (e.g., <a href="#burnett_et_al_2012" id="citation_burnett_et_al_2012_67dc99bb18e7b">Burnett et al. 2012</a>), folksong composition (<a href="#dahlig_and_schaffrath_1998" id="citation_dahlig_and_schaffrath_1998_67dc99bb18e7d">Dahlig and Schaffrath 1998</a>), vocal production (<a href="#georgaki_and_kosteletos_2012" id="citation_georgaki_and_kosteletos_2012_67dc99bb18e7f">Georgaki and Kosteletos 2012</a>) or even expressivity (<a href="#hiraga_et_al_2004" id="citation_hiraga_et_al_2004_67dc99bb18e81">Hiraga et al. 2004</a>).<sup><a name="FN50REF" href="#FN50" id="footnote50">(50)</a></sup> (For a thorough overview, see <a href="#ariza_2009" id="citation_ariza_2009_67dc99bb18e84">Ariza 2009</a>.)</p>

<p>[5.28] In order to test how well this feedback model conformed to musical intuitions, I adapted this paradigm to involve the human and automated analyses of Test 3, now reordered into pairs. Each pair included the same excerpt analyzed twice, once by a human and once by the algorithm, with sequential and pairwise ordering randomized. Eight music theory faculty (again, each from different institutions, all currently teaching a music theory or fundamentals class, with two having participated in the earlier grading task) were presented with eight pairs of analyses. Each excerpt was assessed twice by different graders. The theorist&#8217;s task was to report which of the two they believed to be created by the computer and to provide a short, written explanation of their choice. If the sorts of analytical choices made by humans were different than the algorithm&#8217;s behaviors, the theorists would perform better than chance in their choices. (I have included the full packet of analysis pairs in the online supplement.)</p>

<fig>
<p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 10</b>. Percent of participants who correctly distinguish the human/computer analysis</p><p class='fullwidth' style="text-align: center; margin-bottom:0em"><a class='youtube'  target="blank" href="white_examples.php?id=9&nonav=true"><img border="1" alt="Example 10 thumbnail" src="white_ex10_small.png"></a></p><p class='fullwidth' style="text-align: center; margin-top:0em"><font size="2">(click to enlarge)</font></p></fig>

<p>[5.29] <b>Example 10</b> shows the number of times the theorists correctly distinguished the feedback model&#8217;s analysis from the human&#8217;s analysis, first shown as an overall percentage (in white) and then grouped by the excerpt&#8217;s characteristic (in grey). The significance window now relies on a <i>P</i>(.5) <i>binomial distribution</i>, a method that tests whether the theorists performed better than a coin toss.<sup><a name="FN51REF" href="#FN51" id="footnote51">(51)</a></sup> Overall, the theorists did not: of the 64 choices made by the graders, only 35 were correct. However, excerpts tagged with the &#8220;chromatic&#8221; characteristic were significantly distinguishable: of the 16 choices made by theorists, 75% (12) were correct, a sufficiently lopsided result to indicate the theorists were performing better than chance.<sup><a name="FN52REF" href="#FN52" id="footnote52">(52)</a></sup></p>

<fig>
<p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 11</b>. Human (left) and computer (right) analyses of mm. 29&ndash;37 of Brahms&rsquo; &ldquo;Und gehst du &uuml;ber den Kirchhof&rdquo; op. 44, no. 10</p><p class='fullwidth' style="text-align: center; margin-bottom:0em"><a class='youtube'  target="blank" href="white_examples.php?id=10&nonav=true"><img border="1" alt="Example 11 thumbnail" src="white_ex11_small.png"></a></p><p class='fullwidth' style="text-align: center; margin-top:0em"><font size="2">(click to enlarge)</font></p></fig>

<p>[5.30] Recurrent observations within the attached comments suggest that the theorists used two strategies when producing a correct answer within the &#8220;chromatic&#8221; category: 1) noticing that the humans made &#8220;typically human&#8221; mistakes, and 2) noticing the computer model behaving in a distinctly mechanized way. <b>Example 11</b> shows both behaviors in mm. 29&#8211;37  of Brahms&#8217;s &#8220;Und gehst du &uuml;ber den Kirchhof&#8221; op. 44 no. 10. The human analysis is on the left, and the computer model&#8217;s appears on the right. The theorist justifies their (correct) assessment by writing, &#8220;The (hypothesized) computer is a bit more systematic about which chords it chooses to analyze and which it chooses to omit. Also, the faulty analysis of the second half of m. 6 (implicitly identifying it with the first half of m. 7) seems like a more computer-style choice.&#8221; Another grader similarly writes about the human analyses, &#8220;the V7-of-vi is correct but it shows the vi in the wrong place, which looks like human error. In the [computer] one, the cadential \(\substack{6\\4}\) is misidentified as a iii chord, which looks like a mechanical error.&#8221; In these instances, the computer has misidentified the salient pitches (for instance, m. 6.2 shows a iii chord that extends into m. 7), while the human output contains more inconsistencies than one would expect in a mechanical algorithmic analysis.</p>

<p>[5.31] Overall, this test further refines the model&#8217;s connection to human behavior: while the algorithm often produced annotations that seemed very human-like, the feedback system&#8217;s mistakes involve identifying something a human would not while the humans misidentify something a computer would not. This is not surprising, given that &#8220;chromatic&#8221; exercises are designed to test a student&#8217;s ability to parse complicated music replete with applied chords and chromatic embellishments. This music, then, seems to provide more opportunities for these cases to happen: this music creates more occasions for humans to make &#8220;human-like&#8221; inconsistencies while presenting more possible chord choices for the computer, increasing its chances to choose an unintuitive pitch set from the texture.</p>


<h2>Part VI: An Argument for a Feedback Approach to Tonal Modeling</h2>

<p>[6.1] These tests indicate that a model that integrates chord formation and key finding into a feedback system seems, in many ways, to conform to music theories, pedagogical systems, and to human behavior and expectations and performs comparably to feedforward methods. This work formalizes a proof-of-concept model of key finding that integrates the tasks of harmonic and key analysis into a feedback loop. In sum, it potentially legitimizes the feedback systems used in music theory, computation, and pedagogy as cognitive and behavioral models.</p>

<p>[6.2] There are four important outcomes of this work: first, it shows that a feedback system <i>can</i> analyze music in ways that conform to human behavior; second, it adds a formal specificity to connections between harmony and key, theorizing the gears and sprockets that possibly underlie some fundamental aspects of feedback-based notions of key. Third, it proposes some pedagogical strategies to teaching key and harmony in the music theory classroom. Finally, it suggests some broader ideas about tonality and what it means to be &#8220;in a key.&#8221;</p>

<p>[6.3] To the first: these results constitute a proof of concept that organizing a surface into harmonies can be integrated into a feedback loop with key finding, and this feedback model can produce analyses that seem to align with human behavior and with other established tonal models. This is important, given that&#8212;to my knowledge&#8212;no such model has yet been tested against human behavior.</p>

<p>[6.4] To the second point: this work formalizes exactly how harmony and key might interact, and how a tonal center can arise from chordal analysis. Through its engineering, the model specifies 1)&nbsp;the structure of a chordal vocabulary and syntax that might underpin key finding, 2)&nbsp;the process by which a surface might conform to this vocabulary and syntax by editing the observed sets into subsets and supersets, and 3)&nbsp;the way these pc sets group together and transpose into the scale-degree sets that maximizes the conformance to the vocabulary and syntax.<sup><a name="FN53REF" href="#FN53" id="footnote53">(53)</a></sup></p>

<p>[6.5] Third, this feedback model suggests several specifics aspects of music pedagogy that might be informed by this work. While many of these insights are not unique to this work&#8212;indeed, many have been suggested in one way or another by several probabilistic/computational approaches to key finding (<a href="#temperley_2007" id="citation_temperley_2007_67dc99bb18f19">Temperley 2007</a> or <a href="#quinn_2010" id="citation_quinn_2010_67dc99bb18f1b">Quinn 2010</a>, for example), it is nevertheless worth making explicit the pedagogical payoffs of this type of work. I imagine these as four suggestions or tactics that might be helpful for students as they learn to identify key centers.</p>

<ol>
    <li><i>There are potentially different ways keys can arise</i>. This article has argued for a distinction between feedforward and feedback model, showing both to be potentially effective. Given the potential applicability of different models, a student may be well-served being open to more than one key-finding strategy.</li>
    <li><i>If key is not obvious, start analyzing some other parameter.</i> In feedback systems, key arises alongside other organizational tactics. Therefore, analyzing various aspects of a passage&#8212;from the metric structure to the chord structure, from the melodic contour to the cadential articulations&#8212;can help clarify a passage&#8217;s tonal orientation</li>
    <li><i>Even if your first impression suggests one key, be open to revising this impression</i>. As the model&#8217;s windowed analysis and &#8220;voting method&#8221; shows, various pieces of evidence can point to conflicting key centers in a passage. Just because there exists one piece of evidence for a key, does not mean further inspection might not reveal evidence for another key.</li>
    <li><i>Think in terms of probability, not rules</i>. Regardless of the strategy used, determining a passage&#8217;s key is a matter of maximum probability, not of certainty. Much music can potentially be analyzed in more than one key, conforming in one domain or another to a suit of incompatible keys. The process of key finding does not determine which key is absolutely right to the complete exclusion of others, but which key describes events better than the others.</li>
</ol>

<p>[6.6] Finally, the feedback process suggests a certain conception of what it means to be &#8220;in a key.&#8221; In these terms, &#8220;key&#8221; is then <i>a relationship between scale-degree orientations and the ways in which we organize a musical surface</i>&#8212;with harmony used here as the primary organization. &#8220;Key&#8221; is not a first principle, but rather a characteristic of the way a passage is organized. From this perspective, my model&#8217;s relationship to other tonal parameters is dramatically different from a feedforward one: a key-profile model, for instance, would require some additional post-hoc procedure to accomplish the harmony-identification tasks of Experiments 3 and 4 whereas my model integrates harmony and key as equal partners in a holistic process of tonal analysis. This idea is especially evident in the different ways feedforward and feedback methods approach tonal ambiguity. When a feedforward method finds contradictory or insufficient information to yield a single key with some degree of confidence, that has a unidirectional effect on other musical parameters: not knowing the key means that you might not know the passage&#8217;s harmonies, cadence points, etc. Feedback methods make ambiguity a more dynamic process. Consider again the Grieg excerpt of Example 2. In a feedforward analysis, you might not know which notes are dissonances and which are consonances <i>because</i> you don&#8217;t know what the tonal center is. In contrast, in a feedback analysis, the tonal ambiguity also results in ambiguous consonance/dissonance relationships, but not knowing which notes are consonant and which are dissonant is itself the <i>cause</i> of the tonal ambiguity. Allowing feedback systems into our understanding of key allows for this dynamism in tonally ambiguous passages.</p>

<p>[6.7] And while I initially professed to not be interested in notions of Tonality writ large, we might momentarily relax that caveat to reflect on how feedback tonal logics interact with such broader ideas. In particular, in a feedback model, a key&#8217;s transposition of pitch classes into scale degrees is part of the larger process of cognizing and interpreting a variety of musical domains. This type of feedback system, then, allows key to be an active participant in the complexities of a theory of tonality, be it a theory of harmonic function, tonal prolongation, voice-leading prototypes, etc. Unlike feedforward models, key is not a backdrop to support other musical processes, but rather is an actor integrated into a larger tonal system. Feedback modeling, then, allows for more complex notions of tonal organization&#8212;and broader definitions of Tonality&#8212;to incorporate key-finding into their logic.<sup><a name="FN54REF" href="#FN54" id="footnote54">(54)</a></sup></p>

<p>[6.8] Importantly, I am not advocating for the overthrow of feedforward modeling: different definitions of, and approaches to, key and tonality call for different models of those concepts.<sup><a name="FN55REF" href="#FN55" id="footnote55">(55)</a></sup> Clearly, there are different ways of knowing a passage&#8217;s key, and these approaches become even more divergent when considering the chasm between visual analysis/aural experiences (and as an extension, immediate/reflective hearings of tonal centers). That is, a situation in which &#8220;key&#8221; means looking at key signature would be poorly described by a feedback model, while the tonal implications of hearing harmonic functions would be underserved by a feedforward system. Rather than taking an exclusive stance on definitions of key, I would advocate for an ecumenism, holding multiple definitions simultaneously with different models thriving and faltering in different situations. For instance, a chord-based feedback model loses its power in scalar passages and in monophonic music, while sparser polyphonic passages would be better explained by a feedback system that considers a passage&#8217;s harmonic syntax. Indeed, feedback models represent the same musical information as feedforward methods, but with the latter being a simpler representation of the former phenomenon; and, by simplifying its representation, feedforward models can be more flexible. While this study&#8217;s feedback model seems to identify the key of chordal passages with a relatively high accuracy, a feedforward method like that of key-profile analysis could judge the key of any texture, be it scalar, chordal, monophonic, or polyphonic. A richer cognitive model of key finding could therefore potentially modulate between different modes of analysis, given the situation, using more complicated models when possible and using simpler and more generalizable models when needed.</p>

<p>[6.9] Computationally, psychologically, and intuitively, the feedback method presented here has much room for improvement: taking into consideration texture, bass pitch classes, inversion, and phrase position would all likely add accuracy to such a model. Indeed, given the fact that integrating harmony potentially adds musical validity (and even perhaps accuracy) to a tonal model, it would stand to reason that integrating other domains would improve the model even more. Furthermore, modal distinctions have been ignored within this work: key-profile analyses&#8212;my  frequent proxy for feedforward modeling&#8212;distinguish between mode, while my feedback model does not. There are larger issues behind this difference.  Are there indeed two distinct modes that contain different vocabularies and syntaxes, or is there one &#8220;tonal syntax&#8221; that unites the two modes?  These questions lie outside the bounds of this essay.</p>

<p>[6.10] However, even with these caveats and shortfalls, the method is surprisingly successful, especially given its departure from the fundamental mechanisms of other previously tested computational systems. Empirically based speculative models such as the one presented here suggest new hypotheses, plausible explanations, and directions for future experimental research.</p>

<p>[6.11] These observations are also preliminary and speculative. This model of tonal cognition seems to adhere to many behavioral aspects of the key-finding task, but the cognitive validity of much of its engineering remains to be tested. After all, the value of computational work to music research is not only to model what is already known about some cognitive or theoretical process, but to model what <i>might be true</i> given what we know about that process.</p>





<!-------------------------------- END Article Body -------------------------------------------->

       
	<div style="height:24px;width:150px;background-color:#4c7381;float:left;text-align: center;vertical-align: middle;line-height: 24px;">
		&nbsp;&nbsp;&nbsp;
		<a style="color:white;" onmouseover="this.style.color='#0000ff';text-decoration:none" 
		onmouseout="this.style.color='white';" href="#Beginning">Return to beginning</a>
		&nbsp;&nbsp;&nbsp;
	</div><br><br>

	
<!-------------------------------- Author Info -------------------------------------------->

    
<hr>

	<p><a name="AUTHORNOTE1"></a>
	
	Christopher Wm. White<br>
	The University of Massachusetts, Amherst<br>Department of Music and Dance<br>273 Fine Arts Center East<br>151 Presidents Dr., Ofc. 1<br>Amherst, MA 01003-9330<br><a href="mailto:cwmwhite@umass.edu">cwmwhite@umass.edu</a><br>	
</p>

       
	<div style="height:24px;width:150px;background-color:#4c7381;float:left;text-align: center;vertical-align: middle;line-height: 24px;">
		&nbsp;&nbsp;&nbsp;
		<a style="color:white;" onmouseover="this.style.color='#0000ff';text-decoration:none" 
		onmouseout="this.style.color='white';" href="#Beginning">Return to beginning</a>
		&nbsp;&nbsp;&nbsp;
	</div><br><br>

	
<!-------------------------------- Works Cited List -------------------------------------------->

    
	<hr>
	
	<h3><a name="WorksCited">Works Cited</a></h3>
	
	<div id="citediv_aarden_2003" class="flyoverdiv">Aarden, Bret. J. 2003. &#8220;Dynamic Melodic Expectancy.&#8221; Ph.D. diss., Ohio State University. <a href='http://etd.ohiolink.edu/'>http://etd.ohiolink.edu/</a>.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="aarden_2003"></a>Aarden, Bret. J. 2003. &#8220;Dynamic Melodic Expectancy.&#8221; Ph.D. diss., Ohio State University. <a href='http://etd.ohiolink.edu/'>http://etd.ohiolink.edu/</a>.</p><div id="citediv_agmon_1995" class="flyoverdiv">Agmon, Eytan. 1995. &#8220;Functional Harmony Revisited: A Prototype-Theoretic Approach.&#8221; <i>Music Theory Spectrum </i>17 (2): 196&#8211;214.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="agmon_1995"></a>Agmon, Eytan. 1995. &#8220;Functional Harmony Revisited: A Prototype-Theoretic Approach.&#8221; <i>Music Theory Spectrum </i>17 (2): 196&#8211;214.</p><div id="citediv_albrecht_and_huron_2014" class="flyoverdiv">Albrecht, Joshua, and David Huron. 2014. &#8220;A Statistical Approach to Tracing the Historical Development of Major and Minor Pitch Distributions, 1400-1750.&#8221; <i>Music Perception</i> 31 (3): 223-243.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="albrecht_and_huron_2014"></a>Albrecht, Joshua, and David Huron. 2014. &#8220;A Statistical Approach to Tracing the Historical Development of Major and Minor Pitch Distributions, 1400-1750.&#8221; <i>Music Perception</i> 31 (3): 223-243.</p><div id="citediv_albrecht_and_shanahan_2013" class="flyoverdiv">Albrecht, Joshua, and Daniel Shanahan. 2013. &#8220;The Use of Large Corpora to Train a New Type of Key-Finding Algorithm: An Improved Treatment of the Minor Mode.&#8221; <i>Music Perception </i>31 (1): 59-67.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="albrecht_and_shanahan_2013"></a>Albrecht, Joshua, and Daniel Shanahan. 2013. &#8220;The Use of Large Corpora to Train a New Type of Key-Finding Algorithm: An Improved Treatment of the Minor Mode.&#8221; <i>Music Perception </i>31 (1): 59-67.</p><div id="citediv_d&#8217;alembert_1752" class="flyoverdiv">d&#8217;Alembert, Jean le Rond. 1752. <i>El&eacute;mens de musique th&eacute;orique et pratique suivant les principes de M. Rameau</i>. Durand.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="d&#8217;alembert_1752"></a>d&#8217;Alembert, Jean le Rond. 1752. <i>El&eacute;mens de musique th&eacute;orique et pratique suivant les principes de M. Rameau</i>. Durand.</p><div id="citediv_alphonce_1980" class="flyoverdiv">Alphonce, Bo H. 1980. &#8220;Music Analysis by Computer: A Field for Theory Formation.&#8221; <i>Computer Music Journal</i> 4 (2): 26&#8211;35.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="alphonce_1980"></a>Alphonce, Bo H. 1980. &#8220;Music Analysis by Computer: A Field for Theory Formation.&#8221; <i>Computer Music Journal</i> 4 (2): 26&#8211;35.</p><div id="citediv_ariza_2009" class="flyoverdiv">Ariza, Christopher. 2009. &#8220;The Interrogator as Critic: The Turing Test and the Evaluation of Generative Music Systems.&#8221; <i>Computer Music Journal</i> 33 (2): 48-70.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="ariza_2009"></a>Ariza, Christopher. 2009. &#8220;The Interrogator as Critic: The Turing Test and the Evaluation of Generative Music Systems.&#8221; <i>Computer Music Journal</i> 33 (2): 48-70.</p><div id="citediv_barthelemy_and_bonardi_2001" class="flyoverdiv">Barthelemy, J&eacute;rome, and Alain Bonardi. 2001. &#8220;Figured Bass and Tonality Recognition.&#8221; <i>Proceedings of the Second International Conference on Music Information Retrieval</i>. University of Indiana: 129&#8211;136.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="barthelemy_and_bonardi_2001"></a>Barthelemy, J&eacute;rome, and Alain Bonardi. 2001. &#8220;Figured Bass and Tonality Recognition.&#8221; <i>Proceedings of the Second International Conference on Music Information Retrieval</i>. University of Indiana: 129&#8211;136.</p><div id="citediv_barral_and_martin_2012" class="flyoverdiv">Barral, J&eacute;r&eacute;mie, and Pascal Martin. 2012. &#8220;Phantom Tones and Suppressive Masking by Active Nonlinear Oscillation of the Hair-Cell Bundle.&#8221; <i>Proceedings of the National Academy of Sciences</i> 109 (21): 1344-1351.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="barral_and_martin_2012"></a>Barral, J&eacute;r&eacute;mie, and Pascal Martin. 2012. &#8220;Phantom Tones and Suppressive Masking by Active Nonlinear Oscillation of the Hair-Cell Bundle.&#8221; <i>Proceedings of the National Academy of Sciences</i> 109 (21): 1344-1351.</p><div id="citediv_bellman_2005" class="flyoverdiv">Bellman, H&eacute;ctor. 2005. &#8220;About the Determination of the Key of a Musical Excerpt.&#8221; <i>Proceedings of Computer Music Modeling and Retrieval. </i>Springer: 187&#8211;203.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="bellman_2005"></a>Bellman, H&eacute;ctor. 2005. &#8220;About the Determination of the Key of a Musical Excerpt.&#8221; <i>Proceedings of Computer Music Modeling and Retrieval. </i>Springer: 187&#8211;203.</p><div id="citediv_bharucha_1987" class="flyoverdiv">Bharucha, Jamshed. J. 1987. &#8220;Music Cognition and Perceptual Facilitation: A Connectionist Frame-Work.&#8221; <i>Music Perception</i> 5: 1&#8211;30.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="bharucha_1987"></a>Bharucha, Jamshed. J. 1987. &#8220;Music Cognition and Perceptual Facilitation: A Connectionist Frame-Work.&#8221; <i>Music Perception</i> 5: 1&#8211;30.</p><div id="citediv_bharucha_1991" class="flyoverdiv">Bharucha, Jamshed. J. 1991. &#8220;Pitch, Harmony and Neural Nets: A Psychological Perspective.&#8221; In <i>Music and Connectionism</i>,<i> </i>ed. by P. M. Todd and D. G. Loy. MIT Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="bharucha_1991"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 1991. &#8220;Pitch, Harmony and Neural Nets: A Psychological Perspective.&#8221; In <i>Music and Connectionism</i>,<i> </i>ed. by P. M. Todd and D. G. Loy. MIT Press.</p><div id="citediv_boulanger-lewandowski_bengio_and_vincent_2013" class="flyoverdiv">Boulanger-Lewandowski, Nicolas, Yoshua Bengio, and Pascal Vincent. 2013. &#8220;Audio Chord Recognition with Recurrent Neural Networks.&#8221; <i>Proceedings of the International Society for Music Information Retrieval, Curitiba, Brazil</i>: 335&#8211;340.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="boulanger-lewandowski_bengio_and_vincent_2013"></a>Boulanger-Lewandowski, Nicolas, Yoshua Bengio, and Pascal Vincent. 2013. &#8220;Audio Chord Recognition with Recurrent Neural Networks.&#8221; <i>Proceedings of the International Society for Music Information Retrieval, Curitiba, Brazil</i>: 335&#8211;340.</p><div id="citediv_brown_and_butler_1981" class="flyoverdiv">Brown, Helen, and David Butler. 1981. &#8220;Diatonic Trichords as Minimal Tonal Cue Cells,&#8221;<i> In Theory Only</i> 5 (6-7): 37&#8211;55.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="brown_and_butler_1981"></a>Brown, Helen, and David Butler. 1981. &#8220;Diatonic Trichords as Minimal Tonal Cue Cells,&#8221;<i> In Theory Only</i> 5 (6-7): 37&#8211;55.</p><div id="citediv_brown_butler_and_jones_1994" class="flyoverdiv">Brown, Helen, David Butler, and Mari Riess Jones. 1994. &#8220;Musical and Temporal Influences on Key Discovery.&#8221; <i>Music Perception</i> 11: 371-407.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="brown_butler_and_jones_1994"></a>Brown, Helen, David Butler, and Mari Riess Jones. 1994. &#8220;Musical and Temporal Influences on Key Discovery.&#8221; <i>Music Perception</i> 11: 371-407.</p><div id="citediv_burgoyne_2012" class="flyoverdiv">Burgoyne, John Ashley. 2012. &#8220;Stochastic Processes and Database-Driven Musicology<i>.</i>&#8221; Ph.D. diss., McGill University.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="burgoyne_2012"></a>Burgoyne, John Ashley. 2012. &#8220;Stochastic Processes and Database-Driven Musicology<i>.</i>&#8221; Ph.D. diss., McGill University.</p><div id="citediv_burgoyne_wild_and_fujinaga_2011" class="flyoverdiv">Burgoyne, John Ashley, Jonathan Wild, and Ichiro Fujinaga. 2011. &#8220;An Expert Ground-Truth Set for Audio Chord Recognition and Music Analysis.&#8221; <i>Proceedings of the 12th International Society for Music Information Retrieval Conference, Miami:</i> 633-638.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="burgoyne_wild_and_fujinaga_2011"></a>Burgoyne, John Ashley, Jonathan Wild, and Ichiro Fujinaga. 2011. &#8220;An Expert Ground-Truth Set for Audio Chord Recognition and Music Analysis.&#8221; <i>Proceedings of the 12th International Society for Music Information Retrieval Conference, Miami:</i> 633-638.</p><div id="citediv_burnett_et_al_2012" class="flyoverdiv">Burnett Adam, Evon Khor, Philippe Pasquier, and Arne Eigenfeldt. 2012. &#8220;Validation of Harmonic Progression Generator Using Classical Music,&#8221; <i>Proceedings of the 2012 International Conference on Computational Creativity, Dublin</i>: 126&#8211;133.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="burnett_et_al_2012"></a>Burnett Adam, Evon Khor, Philippe Pasquier, and Arne Eigenfeldt. 2012. &#8220;Validation of Harmonic Progression Generator Using Classical Music,&#8221; <i>Proceedings of the 2012 International Conference on Computational Creativity, Dublin</i>: 126&#8211;133.</p><div id="citediv_byros_2009" class="flyoverdiv">Byros, Vasili. 2009. &#8220;Foundations of Tonality as Situated Cognition, 1730&#8211;1830: An Enquiry into the Culture and Cognition of Eighteenth-Century Tonality, with Beethoven&#8217;s &#8220;Eroica&#8221; Symphony as a Case Study.&#8221; Ph.D. diss., Yale University.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="byros_2009"></a>Byros, Vasili. 2009. &#8220;Foundations of Tonality as Situated Cognition, 1730&#8211;1830: An Enquiry into the Culture and Cognition of Eighteenth-Century Tonality, with Beethoven&#8217;s &#8220;Eroica&#8221; Symphony as a Case Study.&#8221; Ph.D. diss., Yale University.</p><div id="citediv_byros_2012" class="flyoverdiv">Byros, Vasili. 2012. &#8220;Meyer&#8217;s Anvil: Revisiting the Schema Concept.&#8221; <i>Music Analysis, </i>31 (3): 273-346.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="byros_2012"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2012. &#8220;Meyer&#8217;s Anvil: Revisiting the Schema Concept.&#8221; <i>Music Analysis, </i>31 (3): 273-346.</p><div id="citediv_cancino-chacon_grachten_and_agres_2017" class="flyoverdiv">Cancino-Chacon, Carlos, Maarten Grachten, and Kat Agres. 2017. &#8220;From Bach to Beatles: the Simulation of Tonal Expectation Using Ecologically-Trained Predictive Models.&#8221; <i>Proceedings of the International Society for Music Information Retrieval, Suzhou, China: </i>494-501.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="cancino-chacon_grachten_and_agres_2017"></a>Cancino-Chacon, Carlos, Maarten Grachten, and Kat Agres. 2017. &#8220;From Bach to Beatles: the Simulation of Tonal Expectation Using Ecologically-Trained Predictive Models.&#8221; <i>Proceedings of the International Society for Music Information Retrieval, Suzhou, China: </i>494-501.</p><div id="citediv_castellano_bharucha_and_krumhansl_1984" class="flyoverdiv">Castellano, Mary A., Jamshed J. Bharucha, and Carol Krumhansl. 1984. &#8220;Tonal Hierarchies in the Music of North India.&#8221; <i>Journal of Experimental Psychology: General,</i> <i>113</i> (3): 394-412.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="castellano_bharucha_and_krumhansl_1984"></a>Castellano, Mary A., Jamshed J. Bharucha, and Carol Krumhansl. 1984. &#8220;Tonal Hierarchies in the Music of North India.&#8221; <i>Journal of Experimental Psychology: General,</i> <i>113</i> (3): 394-412.</p><div id="citediv_castil-blaze_1810" class="flyoverdiv">Castil-Blaze, Fran&ccedil;ois-Henri-Joseph. 1810. <i>Dictionnaire de Musique Moderne</i>. Au magasin de musique de la Lyre modern.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="castil-blaze_1810"></a>Castil-Blaze, Fran&ccedil;ois-Henri-Joseph. 1810. <i>Dictionnaire de Musique Moderne</i>. Au magasin de musique de la Lyre modern.</p><div id="citediv_clendinning_and_marvin_2011" class="flyoverdiv">Clendinning, Jane P., and Elizabeth. W. Marvin. 2011. <i>The Musician's Guide to Theory and Analysis, </i>(Second Edition). W.W. Norton.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="clendinning_and_marvin_2011"></a>Clendinning, Jane P., and Elizabeth. W. Marvin. 2011. <i>The Musician's Guide to Theory and Analysis, </i>(Second Edition). W.W. Norton.</p><div id="citediv_colombo_et_al_2016" class="flyoverdiv">Colombo, Florian, Samuel P. Muscinelli, Alexander Seeholzer, Johanni Brea, Wulfram Gerstner. 2016. &#8220;Algorithmic Composition of Melodies with Deep Recurrent Neural Networks.&#8221; In <i>Proceedings of the First Conference on Computer Simulation of Musical Creativity</i>. Huddersfield, UK. <a href='https://csmc2016.wordpress.com/proceedings/'>https://csmc2016.wordpress.com/proceedings/</a></div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="colombo_et_al_2016"></a>Colombo, Florian, Samuel P. Muscinelli, Alexander Seeholzer, Johanni Brea, Wulfram Gerstner. 2016. &#8220;Algorithmic Composition of Melodies with Deep Recurrent Neural Networks.&#8221; In <i>Proceedings of the First Conference on Computer Simulation of Musical Creativity</i>. Huddersfield, UK. <a href='https://csmc2016.wordpress.com/proceedings/'>https://csmc2016.wordpress.com/proceedings/</a></p><div id="citediv_cope_1987" class="flyoverdiv">Cope, David. 1987. &#8220;Experiments in Music Intelligence.&#8221; In <i>Proceedings of the 1987 Computer Music Conference</i>. San Francisco: Computer Music Association, 170&#8211;73.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="cope_1987"></a>Cope, David. 1987. &#8220;Experiments in Music Intelligence.&#8221; In <i>Proceedings of the 1987 Computer Music Conference</i>. San Francisco: Computer Music Association, 170&#8211;73.</p><div id="citediv_cope_2005" class="flyoverdiv">Cope, David. 2005. <i>Computer Models of Musical Creativity</i>. MIT Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="cope_2005"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2005. <i>Computer Models of Musical Creativity</i>. MIT Press.</p><div id="citediv_creel_and_newport_2002" class="flyoverdiv">Creel, Sarah C., and Elissa L. Newport. 2002. &#8220;Tonal Profiles of Artificial Scales: Implications for Music Learning.&#8221; In C. Stevens, D. Burnham, G. McPherson, E. Schubert, and J. Renwick (Eds.),<i> Proceedings of the 7th International Conference on Music Perception and Cognition, Sydney:</i> 281-284.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="creel_and_newport_2002"></a>Creel, Sarah C., and Elissa L. Newport. 2002. &#8220;Tonal Profiles of Artificial Scales: Implications for Music Learning.&#8221; In C. Stevens, D. Burnham, G. McPherson, E. Schubert, and J. Renwick (Eds.),<i> Proceedings of the 7th International Conference on Music Perception and Cognition, Sydney:</i> 281-284.</p><div id="citediv_creel_newport_and_aslin_2004" class="flyoverdiv">Creel, Sarah C., Elissa L. Newport, and Richard N. Aslin. 2004. &#8220;Distant Melodies: Statistical Learning of Nonadjacent Dependencies in Tone Sequences.&#8221; <i>Journal of Experimental Psychology: Learning, Memory, and Cognition,</i> 30: 1119 &#8211;1130.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="creel_newport_and_aslin_2004"></a>Creel, Sarah C., Elissa L. Newport, and Richard N. Aslin. 2004. &#8220;Distant Melodies: Statistical Learning of Nonadjacent Dependencies in Tone Sequences.&#8221; <i>Journal of Experimental Psychology: Learning, Memory, and Cognition,</i> 30: 1119 &#8211;1130.</p><div id="citediv_thompson_and_cuddy_1992" class="flyoverdiv">Thompson, William F., and Lola L. Cuddy. 1992. &#8220;Perceived Key Movement in Four-Voice Harmony and Single Voices.&#8221; <i>Music Perception</i> 9: 427&#8211;438.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="thompson_and_cuddy_1992"></a>Thompson, William F., and Lola L. Cuddy. 1992. &#8220;Perceived Key Movement in Four-Voice Harmony and Single Voices.&#8221; <i>Music Perception</i> 9: 427&#8211;438.</p><div id="citediv_cuthbert_and_ariza_2011" class="flyoverdiv">Cuthbert, Michael, and Christopher Ariza. 2011. &#8220;Music21: A Toolkit for Computer&#8211;Aided Musicology and Symbolic Music Data,&#8221; <i>Proceedings of the International Symposium on Music Information Retrieval</i>: 637&#8211;42.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="cuthbert_and_ariza_2011"></a>Cuthbert, Michael, and Christopher Ariza. 2011. &#8220;Music21: A Toolkit for Computer&#8211;Aided Musicology and Symbolic Music Data,&#8221; <i>Proceedings of the International Symposium on Music Information Retrieval</i>: 637&#8211;42.</p><div id="citediv_dahlhaus_1990" class="flyoverdiv">Dahlhaus, Carl. 1990. <i>Studies on the Origin of Harmonic Tonality</i>. Trans. by Robert O. Gjerdingen. Princeton University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="dahlhaus_1990"></a>Dahlhaus, Carl. 1990. <i>Studies on the Origin of Harmonic Tonality</i>. Trans. by Robert O. Gjerdingen. Princeton University Press.</p><div id="citediv_dahlig_and_schaffrath_1998" class="flyoverdiv">Dahlig, Ewa, and Helmut Schaffrath. 1998 &#8220;Judgments of Human and Machine Authorship in Real and Artificial Folksongs,&#8221; <i>Computing in Musicology</i> 11 (1998): 211-218.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="dahlig_and_schaffrath_1998"></a>Dahlig, Ewa, and Helmut Schaffrath. 1998 &#8220;Judgments of Human and Machine Authorship in Real and Artificial Folksongs,&#8221; <i>Computing in Musicology</i> 11 (1998): 211-218.</p><div id="citediv_declercq_2016" class="flyoverdiv">deClercq, Trevor. 2016. &#8220;Big Data, Big Questions: A Closer Look at the Yale&#8211; Classical Archives Corpus.&#8221; <i>Empirical Musicology Review </i>11 (1): 59-67.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="declercq_2016"></a>deClercq, Trevor. 2016. &#8220;Big Data, Big Questions: A Closer Look at the Yale&#8211; Classical Archives Corpus.&#8221; <i>Empirical Musicology Review </i>11 (1): 59-67.</p><div id="citediv_declercq_and_temperley_2011" class="flyoverdiv">deClercq, Trevor, and David Temperley. 2011. &#8220;A Corpus Analysis of Rock Harmony.&#8221; <i>Popular Music</i> 30 (1)<i>:</i> 47&#8211;70.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="declercq_and_temperley_2011"></a>deClercq, Trevor, and David Temperley. 2011. &#8220;A Corpus Analysis of Rock Harmony.&#8221; <i>Popular Music</i> 30 (1)<i>:</i> 47&#8211;70.</p><div id="citediv_dineen_2005" class="flyoverdiv">Dineen, Murray. 2005. &#8220;Schoenberg&#8217;s Modulatory Calculations: Wn Fonds 21 Berg 6/III/66 and Tonality.&#8221; <i>Music Theory Spectrum</i> 27 (1): 97-112.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="dineen_2005"></a>Dineen, Murray. 2005. &#8220;Schoenberg&#8217;s Modulatory Calculations: Wn Fonds 21 Berg 6/III/66 and Tonality.&#8221; <i>Music Theory Spectrum</i> 27 (1): 97-112.</p><div id="citediv_eyben_et_al_2010" class="flyoverdiv">Eyben, Florian, Sebastian Boeck, Bj&ouml;rn Schuller, and Alex Graves. 2010. &#8220;Universal Onset Detection with Bidirectional Long Short-Term Memory Neural Networks.&#8221; In <i>Proceedings of the International Society for Music Information Retrieval. </i>Utrecht, Netherlands: 589&#8211;594.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="eyben_et_al_2010"></a>Eyben, Florian, Sebastian Boeck, Bj&ouml;rn Schuller, and Alex Graves. 2010. &#8220;Universal Onset Detection with Bidirectional Long Short-Term Memory Neural Networks.&#8221; In <i>Proceedings of the International Society for Music Information Retrieval. </i>Utrecht, Netherlands: 589&#8211;594.</p><div id="citediv_f&eacute;tis_1844" class="flyoverdiv">F&eacute;tis, Fran&ccedil;ois-Joseph. 1844. <i>Trait&eacute; Complet de la Th&eacute;orie et de la Pratique de L'harmonie.</i> Eugen Duverger.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="f&eacute;tis_1844"></a>F&eacute;tis, Fran&ccedil;ois-Joseph. 1844. <i>Trait&eacute; Complet de la Th&eacute;orie et de la Pratique de L'harmonie.</i> Eugen Duverger.</p><div id="citediv_feulner_1993" class="flyoverdiv">Feulner, Johannes. 1993. &#8220;Neural Networks that Learn and Reproduce Various Styles of Harmonization,&#8221; in <i>Proceedings of the 1993 Computer Music Conference</i>. San Francisco, Computer Music Association: 236&#8211;239.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="feulner_1993"></a>Feulner, Johannes. 1993. &#8220;Neural Networks that Learn and Reproduce Various Styles of Harmonization,&#8221; in <i>Proceedings of the 1993 Computer Music Conference</i>. San Francisco, Computer Music Association: 236&#8211;239.</p><div id="citediv_forte_1973" class="flyoverdiv">Forte, Alan. 1973. <i>The Structure of Atonal Music.</i> Yale University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="forte_1973"></a>Forte, Alan. 1973. <i>The Structure of Atonal Music.</i> Yale University Press.</p><div id="citediv_gang_lehman_and_wagner_1998" class="flyoverdiv">Gang, Dan, Daniel Lehman, and Naftali Wagner, 1998. &#8220;Tuning a Neural Network for Harmonizing Melodies in Real-Time,&#8221; in <i>Proceedings of the 1998 Computer Music Conference</i>, San Francisco: Computer Music Association.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="gang_lehman_and_wagner_1998"></a>Gang, Dan, Daniel Lehman, and Naftali Wagner, 1998. &#8220;Tuning a Neural Network for Harmonizing Melodies in Real-Time,&#8221; in <i>Proceedings of the 1998 Computer Music Conference</i>, San Francisco: Computer Music Association.</p><div id="citediv_gjerdingen_2007" class="flyoverdiv">Gjerdingen, Robert. 2007. <i>Music in the Galant Style.</i> Oxford University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="gjerdingen_2007"></a>Gjerdingen, Robert. 2007. <i>Music in the Galant Style.</i> Oxford University Press.</p><div id="citediv_harrison_1994" class="flyoverdiv">Harrison, Daniel. 1994. <i>Harmonic Function in Chromatic Music: A Renewed Dualist Theory and an Account of its Precedents</i>. University of Chicago Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="harrison_1994"></a>Harrison, Daniel. 1994. <i>Harmonic Function in Chromatic Music: A Renewed Dualist Theory and an Account of its Precedents</i>. University of Chicago Press.</p><div id="citediv_harrison_2016" class="flyoverdiv">Harrison, Daniel. 2016. <i>Pieces of Tradition: An Analysis of Contemporary Tonality.</i> Oxford University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="harrison_2016"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2016. <i>Pieces of Tradition: An Analysis of Contemporary Tonality.</i> Oxford University Press.</p><div id="citediv_hiraga_et_al_2004" class="flyoverdiv">Hiraga, Rumi, Roberto Bresin, Keiji Hirata, and Haruhiro Katayose. 2004. &#8220;Rencon 2004: Turing Test for Musical Expression.&#8221; <i>Proceedings of the 2004 Conference on New Interfaces for Musical Expression. </i>Hamamatsu, Japan.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="hiraga_et_al_2004"></a>Hiraga, Rumi, Roberto Bresin, Keiji Hirata, and Haruhiro Katayose. 2004. &#8220;Rencon 2004: Turing Test for Musical Expression.&#8221; <i>Proceedings of the 2004 Conference on New Interfaces for Musical Expression. </i>Hamamatsu, Japan.</p><div id="citediv_huron_2006" class="flyoverdiv">Huron, David. 2006. <i>Sweet Anticipation: Music and the Psychology of Expectation. </i>The MIT Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="huron_2006"></a>Huron, David. 2006. <i>Sweet Anticipation: Music and the Psychology of Expectation. </i>The MIT Press.</p><div id="citediv_huron_2016" class="flyoverdiv">Huron, David. 2016. <i>Voice Leading: The Science behind a Musical Art.</i> Cambridge. The MIT Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="huron_2016"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2016. <i>Voice Leading: The Science behind a Musical Art.</i> Cambridge. The MIT Press.</p><div id="citediv_hyer_2002" class="flyoverdiv">Hyer, Brian. 2002. &#8220;Tonality.&#8221; In <i>The Cambridge History of Western Music Theory</i>, ed by Thomas Christensen, 726-52. Cambridge University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="hyer_2002"></a>Hyer, Brian. 2002. &#8220;Tonality.&#8221; In <i>The Cambridge History of Western Music Theory</i>, ed by Thomas Christensen, 726-52. Cambridge University Press.</p><div id="citediv_illescas_rizo_and_i&ntilde;esta_2007" class="flyoverdiv">Illescas, Pl&aacute;cido R., David Rizo , and Jos&eacute; M. I&ntilde;esta. 2007. &#8220;Harmonic, Melodic, and Functional Automatic Analysis.&#8221; <i>Proceedings of the 2007 International Computer Music Conference. </i>San Francisco, Computer Music Association:<i> </i>165&#8211;168.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="illescas_rizo_and_i&ntilde;esta_2007"></a>Illescas, Pl&aacute;cido R., David Rizo , and Jos&eacute; M. I&ntilde;esta. 2007. &#8220;Harmonic, Melodic, and Functional Automatic Analysis.&#8221; <i>Proceedings of the 2007 International Computer Music Conference. </i>San Francisco, Computer Music Association:<i> </i>165&#8211;168.</p><div id="citediv_jurafsky_and_martin_2000" class="flyoverdiv">Jurafsky, Dan. and James H. Martin. 2000. <i>Speech and Language Processing</i>:<i> an Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 1st Edition</i>. Prentice-Hall.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="jurafsky_and_martin_2000"></a>Jurafsky, Dan. and James H. Martin. 2000. <i>Speech and Language Processing</i>:<i> an Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 1st Edition</i>. Prentice-Hall.</p><div id="citediv_kopp_2002" class="flyoverdiv">Kopp, David. 2002. <i>Chromatic Transformations in Nineteenth-Century Music.</i> Cambridge University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="kopp_2002"></a>Kopp, David. 2002. <i>Chromatic Transformations in Nineteenth-Century Music.</i> Cambridge University Press.</p><div id="citediv_georgaki_and_kosteletos_2012" class="flyoverdiv">Georgaki, Anastasia and George Kosteletos. 2012. &#8220;A Turing Test for the Singing Voice as an Anthropological Tool: Epistemological and Technical Issues.&#8221; <i>Proceedings of the 2012 Computer Music Conference</i>. San Francisco: Computer Music Association, 46-51.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="georgaki_and_kosteletos_2012"></a>Georgaki, Anastasia and George Kosteletos. 2012. &#8220;A Turing Test for the Singing Voice as an Anthropological Tool: Epistemological and Technical Issues.&#8221; <i>Proceedings of the 2012 Computer Music Conference</i>. San Francisco: Computer Music Association, 46-51.</p><div id="citediv_kostka_and_payne_2012" class="flyoverdiv">Kostka, Stefan, and Dorothy Payne. 2012. <i>Tonal Harmony with an Introduction to Twentieth-Century Music, </i>4th edition. McGraw-Hill.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="kostka_and_payne_2012"></a>Kostka, Stefan, and Dorothy Payne. 2012. <i>Tonal Harmony with an Introduction to Twentieth-Century Music, </i>4th edition. McGraw-Hill.</p><div id="citediv_krumhansl_1990" class="flyoverdiv">Krumhansl, Carol L. 1990. <i>The Cognitive Foundations of Musical Pitch.</i> Oxford University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="krumhansl_1990"></a>Krumhansl, Carol L. 1990. <i>The Cognitive Foundations of Musical Pitch.</i> Oxford University Press.</p><div id="citediv_krumhansl_bharucha_and_castellano_1982" class="flyoverdiv">Krumhansl, Carol L, Jamshed J. Bharucha, and Mary A. Castellano. 1982. &#8220;Key Distance Effects on Perceived Harmonic Structure in Music.&#8221; <i>Perception &amp; Psychophysics</i> 32: 96&#8211;108</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="krumhansl_bharucha_and_castellano_1982"></a>Krumhansl, Carol L, Jamshed J. Bharucha, and Mary A. Castellano. 1982. &#8220;Key Distance Effects on Perceived Harmonic Structure in Music.&#8221; <i>Perception &amp; Psychophysics</i> 32: 96&#8211;108</p><div id="citediv_krumhansl_and_shepard_1979" class="flyoverdiv">Krumhansl, Carol L., and Roger N. Shepard. 1979. &#8220;Quantification of the Hierarchy of Tonal Functions Within a Diatonic Context.&#8221; <i>Journal of Experimental Psychology: Human Perception and Performance</i>, S (4): 579-594.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="krumhansl_and_shepard_1979"></a>Krumhansl, Carol L., and Roger N. Shepard. 1979. &#8220;Quantification of the Hierarchy of Tonal Functions Within a Diatonic Context.&#8221; <i>Journal of Experimental Psychology: Human Perception and Performance</i>, S (4): 579-594.</p><div id="citediv_kurth_1920" class="flyoverdiv">Kurth, Ernst. 1920. <i>Romantische Harmonik und ihre Krise in Wagners &#8216;Tristan.&#8217;</i> Berne (partial Eng. trans. in <i>Ernst Kurth: Selected Writings</i>, ed. L.A. Rothfarb (Cambridge, 1991): 97&#8211;147).</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="kurth_1920"></a>Kurth, Ernst. 1920. <i>Romantische Harmonik und ihre Krise in Wagners &#8216;Tristan.&#8217;</i> Berne (partial Eng. trans. in <i>Ernst Kurth: Selected Writings</i>, ed. L.A. Rothfarb (Cambridge, 1991): 97&#8211;147).</p><div id="citediv_laitz_2008" class="flyoverdiv">Laitz, Steven. 2008. <i>The Complete Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening</i>, 3rd Edition<i>.</i> Oxford University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="laitz_2008"></a>Laitz, Steven. 2008. <i>The Complete Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening</i>, 3rd Edition<i>.</i> Oxford University Press.</p><div id="citediv_lerdahl_and_jackendoff_1983" class="flyoverdiv">Lerdahl, Fred, and Ray Jackendoff. 1983. <i>A Generative Theory of Tonal Music. A Generative Theory of Tonal Music. </i>MIT Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="lerdahl_and_jackendoff_1983"></a>Lerdahl, Fred, and Ray Jackendoff. 1983. <i>A Generative Theory of Tonal Music. A Generative Theory of Tonal Music. </i>MIT Press.</p><div id="citediv_lerdahl_and_krumhansl_2007" class="flyoverdiv">Lerdahl, Fred, and Carol L. Krumhansl. 2007. &#8220;Modeling Tonal Tension.&#8221; <i>Music Perception</i> 24, 329-66.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="lerdahl_and_krumhansl_2007"></a>Lerdahl, Fred, and Carol L. Krumhansl. 2007. &#8220;Modeling Tonal Tension.&#8221; <i>Music Perception</i> 24, 329-66.</p><div id="citediv_lewin_1987" class="flyoverdiv">Lewin, David. 1987. <i>Generalized Musical Intervals and Transformations.</i> Yale University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="lewin_1987"></a>Lewin, David. 1987. <i>Generalized Musical Intervals and Transformations.</i> Yale University Press.</p><div id="citediv_liu_and_randall_2016" class="flyoverdiv">Liu, I-Ting, and Richard Randall. 2016. &#8220;Predicting Missing Music Components with Bidirectional Long Long Short-Term Memory Neural Networks. <i>Proceedings of the International Conference for Music Perception and Cognition</i>: 103-110.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="liu_and_randall_2016"></a>Liu, I-Ting, and Richard Randall. 2016. &#8220;Predicting Missing Music Components with Bidirectional Long Long Short-Term Memory Neural Networks. <i>Proceedings of the International Conference for Music Perception and Cognition</i>: 103-110.</p><div id="citediv_long_2018" class="flyoverdiv">Long, Megan Kaes. 2018. &#8220;Cadential Syntax and Tonal Expectation in Late Sixteenth-Century Homophony.&#8221; <i>Music Theory Spectrum</i> 40 (1).</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="long_2018"></a>Long, Megan Kaes. 2018. &#8220;Cadential Syntax and Tonal Expectation in Late Sixteenth-Century Homophony.&#8221; <i>Music Theory Spectrum</i> 40 (1).</p><div id="citediv_longuet-higgins_and_steedman_1971" class="flyoverdiv">Longuet-Higgins, H. Christpherz, and Mark Steedman. 1971. &#8220;On Interpreting Bach.&#8221; In <i>Machine Intelligence</i>, B. Meltzer and D. Michie, eds.. Edinburgh University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="longuet-higgins_and_steedman_1971"></a>Longuet-Higgins, H. Christpherz, and Mark Steedman. 1971. &#8220;On Interpreting Bach.&#8221; In <i>Machine Intelligence</i>, B. Meltzer and D. Michie, eds.. Edinburgh University Press.</p><div id="citediv_loui_2012" class="flyoverdiv">Loui, Psyche. 2012. &#8220;Learning and Liking of Melody and Harmony: Further Studies in Artificial Grammar Learning.&#8221; <i>Topics in Cognitive Science</i> 4: 1-14</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="loui_2012"></a>Loui, Psyche. 2012. &#8220;Learning and Liking of Melody and Harmony: Further Studies in Artificial Grammar Learning.&#8221; <i>Topics in Cognitive Science</i> 4: 1-14</p><div id="citediv_loui_wessel_and_hudson_kam_2010" class="flyoverdiv">Loui, Psyche, David L. Wessel, and Carla L. Hudson Kam. 2010. &#8220;Humans Rapidly Learn Grammatical Structure in a New Musical Scale.&#8221; <i>Music Perception </i>27 (5): 377-388.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="loui_wessel_and_hudson_kam_2010"></a>Loui, Psyche, David L. Wessel, and Carla L. Hudson Kam. 2010. &#8220;Humans Rapidly Learn Grammatical Structure in a New Musical Scale.&#8221; <i>Music Perception </i>27 (5): 377-388.</p><div id="citediv_louis_and_thuille_1907" class="flyoverdiv">Louis, Rudolf and Ludwig Thuille. 1907. <i>Harmonielehre.</i> Carl Grüninger.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="louis_and_thuille_1907"></a>Louis, Rudolf and Ludwig Thuille. 1907. <i>Harmonielehre.</i> Carl Grüninger.</p><div id="citediv_matsunaga_and_abe_2005" class="flyoverdiv">Matsunaga, Rie, and Jun-Ichi Abe. 2005. &#8220;Cues for Key Perception of a Melody: Pitch Set Alone?&#8221; <i>Music Perception</i> 23: 153-164</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="matsunaga_and_abe_2005"></a>Matsunaga, Rie, and Jun-Ichi Abe. 2005. &#8220;Cues for Key Perception of a Melody: Pitch Set Alone?&#8221; <i>Music Perception</i> 23: 153-164</p><div id="citediv_matsunaga_and_abe_2012" class="flyoverdiv">Matsunaga, Rie, and Jun-Ichi Abe. 2012. &#8220;Dynamic Cues in Key Perception.&#8221; <i>International Journal of Psychological Studies</i> 4: 3-21.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="matsunaga_and_abe_2012"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2012. &#8220;Dynamic Cues in Key Perception.&#8221; <i>International Journal of Psychological Studies</i> 4: 3-21.</p><div id="citediv_matsunaga_hartono_and_abe_2015" class="flyoverdiv">Matsunaga, Rie, Pitoyo Hartono, and Jun-Ichi Abe. 2015. &#8220;The Acquisition Process of Musical Tonal Schema: Implications from Connectionist Modeling.&#8221; <i>Frontiers in Psychology (Cognitive Science)</i> 6:1348: <a href='http://dx.doi.org/10.3389/fpsyg.2015.01348'>http://dx.doi.org/10.3389/fpsyg.2015.01348</a>.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="matsunaga_hartono_and_abe_2015"></a>Matsunaga, Rie, Pitoyo Hartono, and Jun-Ichi Abe. 2015. &#8220;The Acquisition Process of Musical Tonal Schema: Implications from Connectionist Modeling.&#8221; <i>Frontiers in Psychology (Cognitive Science)</i> 6:1348: <a href='http://dx.doi.org/10.3389/fpsyg.2015.01348'>http://dx.doi.org/10.3389/fpsyg.2015.01348</a>.</p><div id="citediv_moore_2012" class="flyoverdiv">Moore, Brian C.J. 2012. <i>An Introduction to the Psychology of Hearing.</i> Bingley, Emerald.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="moore_2012"></a>Moore, Brian C.J. 2012. <i>An Introduction to the Psychology of Hearing.</i> Bingley, Emerald.</p><div id="citediv_oram_and_cuddy_1995" class="flyoverdiv">Oram, Nicholas, and Lola L Cuddy. 1995. &#8220;Responsiveness of Western Adults to Pitch-Distributional Information in Melodic Sequences.&#8221; <i>Psychological Research</i> 57: 103-118.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="oram_and_cuddy_1995"></a>Oram, Nicholas, and Lola L Cuddy. 1995. &#8220;Responsiveness of Western Adults to Pitch-Distributional Information in Melodic Sequences.&#8221; <i>Psychological Research</i> 57: 103-118.</p><div id="citediv_pachet_2000" class="flyoverdiv">Pachet, Fran&ccedil;ois. 2000. &#8220;Computer Analysis of Jazz Chord Sequences: Is Solar a Blues?&#8221; In <i>Readings in Music and Artificial Intelligence</i>, ed. by E. Miranda. Harwood Academic Publishers.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="pachet_2000"></a>Pachet, Fran&ccedil;ois. 2000. &#8220;Computer Analysis of Jazz Chord Sequences: Is Solar a Blues?&#8221; In <i>Readings in Music and Artificial Intelligence</i>, ed. by E. Miranda. Harwood Academic Publishers.</p><div id="citediv_pardo_and_birmingham_1999" class="flyoverdiv">Pardo, Bryan, and William P. Birmingham. 1999. &#8220;Automated Partitioning of Tonal Music.&#8221; <i>Technical report, Electrical Engineering and Computer Science Department.</i> University of Michigan.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="pardo_and_birmingham_1999"></a>Pardo, Bryan, and William P. Birmingham. 1999. &#8220;Automated Partitioning of Tonal Music.&#8221; <i>Technical report, Electrical Engineering and Computer Science Department.</i> University of Michigan.</p><div id="citediv_pearce_and_wiggins_2004" class="flyoverdiv">Pearce, Marcus T., and Geraint A. Wiggins. 2004. &#8220;Improved Methods for Statistical Modelling of Monophonic Music,&#8221; <i>Journal of New Music Research</i>. 33 (4): 367&#8211;385.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="pearce_and_wiggins_2004"></a>Pearce, Marcus T., and Geraint A. Wiggins. 2004. &#8220;Improved Methods for Statistical Modelling of Monophonic Music,&#8221; <i>Journal of New Music Research</i>. 33 (4): 367&#8211;385.</p><div id="citediv_pearce_mullensiefen_and_wiggins_2008" class="flyoverdiv">Pearce, Marcus T., Daniel Mullensiefen, and Geraint A. Wiggins. 2008. &#8220;Perceptual Segmentation of Melodies: Ambiguity, Rules and Statistical Learning.&#8221; <i>The 10th International Conference on Music Perception and Cognition</i>. Sapporo, Japan.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="pearce_mullensiefen_and_wiggins_2008"></a>Pearce, Marcus T., Daniel Mullensiefen, and Geraint A. Wiggins. 2008. &#8220;Perceptual Segmentation of Melodies: Ambiguity, Rules and Statistical Learning.&#8221; <i>The 10th International Conference on Music Perception and Cognition</i>. Sapporo, Japan.</p><div id="citediv_pearce_et_al_2010" class="flyoverdiv">Pearce, Marcus T., Mar&iacute;a Herrojo Ruiz, Selina Kapasi, Geraint A.Wiggins, Joydeep Bhattacharyade. 2010. &#8220;Unsupervised Statistical Learning Underpins Computational, Behavioural, and Neural Manifestations of Musical Expectation.&#8221; <i>NeuroImage</i> 50 (1): 302&#8211;313.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="pearce_et_al_2010"></a>Pearce, Marcus T., Mar&iacute;a Herrojo Ruiz, Selina Kapasi, Geraint A.Wiggins, Joydeep Bhattacharyade. 2010. &#8220;Unsupervised Statistical Learning Underpins Computational, Behavioural, and Neural Manifestations of Musical Expectation.&#8221; <i>NeuroImage</i> 50 (1): 302&#8211;313.</p><div id="citediv_piston_1941" class="flyoverdiv">Piston, Walter. 1941. <i>Harmony</i>. Norton.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="piston_1941"></a>Piston, Walter. 1941. <i>Harmony</i>. Norton.</p><div id="citediv_plomp_1964" class="flyoverdiv">Plomp, Reinier. 1964 &#8220;The Ear as a Frequency Analyzer.&#8221; <i>Journal of the Acoustical Society of America </i>36 (9): 1628&#8211;1636.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="plomp_1964"></a>Plomp, Reinier. 1964 &#8220;The Ear as a Frequency Analyzer.&#8221; <i>Journal of the Acoustical Society of America </i>36 (9): 1628&#8211;1636.</p><div id="citediv_povel_and_jansen_2002" class="flyoverdiv">Povel, Dirk-Jan, and Erik Jansen. 2002. &#8220;Harmonic Factors in the Perception of Tonal Melodies.&#8221; <i>Music Perception</i> 20 (1): 51-85.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="povel_and_jansen_2002"></a>Povel, Dirk-Jan, and Erik Jansen. 2002. &#8220;Harmonic Factors in the Perception of Tonal Melodies.&#8221; <i>Music Perception</i> 20 (1): 51-85.</p><div id="citediv_prince_and_schmuckler_2014" class="flyoverdiv">Prince, Jon B., and Mark A. Schmuckler. 2014. &#8220;The Tonal-Metric Hierarchy.&#8221; <i>Music Perception</i> 31 (3), 254&#8211;270.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="prince_and_schmuckler_2014"></a>Prince, Jon B., and Mark A. Schmuckler. 2014. &#8220;The Tonal-Metric Hierarchy.&#8221; <i>Music Perception</i> 31 (3), 254&#8211;270.</p><div id="citediv_prince_thompson_and_schmuckler_2009" class="flyoverdiv">Prince, Jon B., William F. Thompson, and Mark. A. Schmuckler. 2009. &#8220;Pitch and Time, Tonality and Meter: How Do Musical Dimensions Combine?,&#8221; <i>Journal of Experimental Psychology</i> 35 (5), 1598&#8211;1617.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="prince_thompson_and_schmuckler_2009"></a>Prince, Jon B., William F. Thompson, and Mark. A. Schmuckler. 2009. &#8220;Pitch and Time, Tonality and Meter: How Do Musical Dimensions Combine?,&#8221; <i>Journal of Experimental Psychology</i> 35 (5), 1598&#8211;1617.</p><div id="citediv_de_prisco_et_al_2010" class="flyoverdiv">De Prisco, Roberto, Antonio Eletto, Antonio Torre, and Rocco Zaccagnino 2010. &#8220;A Neural Network for Bass Functional Harmonization.&#8221; In <i>Applications of Evolutionary Computation, </i>351-360<i>.</i> Springer.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="de_prisco_et_al_2010"></a>De Prisco, Roberto, Antonio Eletto, Antonio Torre, and Rocco Zaccagnino 2010. &#8220;A Neural Network for Bass Functional Harmonization.&#8221; In <i>Applications of Evolutionary Computation, </i>351-360<i>.</i> Springer.</p><div id="citediv_quick_2014" class="flyoverdiv">Quick, Donya. 2014. <i>Kulitta: A Framework for Automated Music Composition. </i>Ph.D. diss., Yale University.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="quick_2014"></a>Quick, Donya. 2014. <i>Kulitta: A Framework for Automated Music Composition. </i>Ph.D. diss., Yale University.</p><div id="citediv_quinn_2010" class="flyoverdiv">Quinn, Ian. 2010. &#8220;What&#8217;s &#8216;Key for Key&#8217;: A Theoretically Naive Key&#8211;Finding Model for Bach Chorales.&#8221; <i>Zeitschrift der Gesellschaft f&uuml;r Musiktheorie</i> 7 (ii): 151&#8211;63.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="quinn_2010"></a>Quinn, Ian. 2010. &#8220;What&#8217;s &#8216;Key for Key&#8217;: A Theoretically Naive Key&#8211;Finding Model for Bach Chorales.&#8221; <i>Zeitschrift der Gesellschaft f&uuml;r Musiktheorie</i> 7 (ii): 151&#8211;63.</p><div id="citediv_quinn_and_mavromatis_2011" class="flyoverdiv">Quinn, Ian, and Panayotis Mavromatis. 2011. &#8220;Voice Leading and Harmonic Function in Two Chorale Corpora.&#8221; In <i>Mathematics and Computation in Music</i>, ed. by Carlos Agon, 230-240. Springer.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="quinn_and_mavromatis_2011"></a>Quinn, Ian, and Panayotis Mavromatis. 2011. &#8220;Voice Leading and Harmonic Function in Two Chorale Corpora.&#8221; In <i>Mathematics and Computation in Music</i>, ed. by Carlos Agon, 230-240. Springer.</p><div id="citediv_rahn_1980" class="flyoverdiv">Rahn, John. 1980. &#8220;On Some Computational Models of Music Theory.&#8221; <i>Computer Music Journal </i>4 (2), Artifical Intelligence and Music Part 1: 66-72.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="rahn_1980"></a>Rahn, John. 1980. &#8220;On Some Computational Models of Music Theory.&#8221; <i>Computer Music Journal </i>4 (2), Artifical Intelligence and Music Part 1: 66-72.</p><div id="citediv_raphael_and_stoddard_2003" class="flyoverdiv">Raphael, Christopher, and Joshua Stoddard. 2003. &#8220;Harmonic Analysis with Probabilistic Graphical Models.&#8221; Retrieved from <a href='https://jscholarship.library.jhu.edu/handle/1774.2/25'>https://jscholarship.library.jhu.edu/handle/1774.2/25</a></div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="raphael_and_stoddard_2003"></a>Raphael, Christopher, and Joshua Stoddard. 2003. &#8220;Harmonic Analysis with Probabilistic Graphical Models.&#8221; Retrieved from <a href='https://jscholarship.library.jhu.edu/handle/1774.2/25'>https://jscholarship.library.jhu.edu/handle/1774.2/25</a></p><div id="citediv_raphael_and_stoddard_2004" class="flyoverdiv">Raphael, Christopher, and Joshua Stoddard. 2004. &#8220;Functional Analysis Using Probabilistic Models.&#8221; <i>Computer Music Journal</i> 28 (3):<i> </i>45&#8211;52.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="raphael_and_stoddard_2004"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2004. &#8220;Functional Analysis Using Probabilistic Models.&#8221; <i>Computer Music Journal</i> 28 (3):<i> </i>45&#8211;52.</p><div id="citediv_rameau_1726" class="flyoverdiv">Rameau, Jean Phillipe. 1726. <i>Nouveau Syst&egrave;me de Musique Th&eacute;orique</i>. Ballard.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="rameau_1726"></a>Rameau, Jean Phillipe. 1726. <i>Nouveau Syst&egrave;me de Musique Th&eacute;orique</i>. Ballard.</p><div id="citediv_reicha_1818" class="flyoverdiv">Reicha, Anton. 1818. <i>Cours de cCmposition Musicale, ou Trait&eacute; Complet et Raisonn&eacute; d&#8217;harmonie Pratique.</i> Gambaro.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="reicha_1818"></a>Reicha, Anton. 1818. <i>Cours de cCmposition Musicale, ou Trait&eacute; Complet et Raisonn&eacute; d&#8217;harmonie Pratique.</i> Gambaro.</p><div id="citediv_riemann_1893" class="flyoverdiv">Riemann, Hugo. 1893. <i>Vereinfachte Harmonielehre, oder die Lehre von den Tonalen Funktionen der Akkorde</i>. Augener.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="riemann_1893"></a>Riemann, Hugo. 1893. <i>Vereinfachte Harmonielehre, oder die Lehre von den Tonalen Funktionen der Akkorde</i>. Augener.</p><div id="citediv_rohrmeier_2007" class="flyoverdiv">Rohrmeier, Martin. 2007. &#8220;A Generative Grammar Approach to Diatonic Harmonic Structure.&#8221; <i>Proceedings of the 4th Sound and Music Computing Conference, </i>SMC, Lefkada, Greece: 97&#8211;100.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="rohrmeier_2007"></a>Rohrmeier, Martin. 2007. &#8220;A Generative Grammar Approach to Diatonic Harmonic Structure.&#8221; <i>Proceedings of the 4th Sound and Music Computing Conference, </i>SMC, Lefkada, Greece: 97&#8211;100.</p><div id="citediv_rohrmeier_and_cross_2008" class="flyoverdiv">Rohrmeier, Martin, and Ian Cross. 2008. &#8220;Statistical Properties of Tonal Harmony in Bach&#8217;s Chorales.&#8221; in <i>Proceedings of the 10th International Conference on Music Perception and Cognition</i>. Sapporo: ICMPC: 619&#8211;627.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="rohrmeier_and_cross_2008"></a>Rohrmeier, Martin, and Ian Cross. 2008. &#8220;Statistical Properties of Tonal Harmony in Bach&#8217;s Chorales.&#8221; in <i>Proceedings of the 10th International Conference on Music Perception and Cognition</i>. Sapporo: ICMPC: 619&#8211;627.</p><div id="citediv_saffran_et_al_1999" class="flyoverdiv">Saffran, Jenny R., Elizabeth K. Johnson, Richard N. Aslin, and Elissa L. Newport. 1999. &#8220;Statistical Learning of Tone Sequences by Human Infants and Adults.&#8221; <i>Cognition</i> 70: 27&#8211;52.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="saffran_et_al_1999"></a>Saffran, Jenny R., Elizabeth K. Johnson, Richard N. Aslin, and Elissa L. Newport. 1999. &#8220;Statistical Learning of Tone Sequences by Human Infants and Adults.&#8221; <i>Cognition</i> 70: 27&#8211;52.</p><div id="citediv_saffran_et_al_2005" class="flyoverdiv">Saffran, Jenny R., Karelyn Reeck, Aimee Niebuhr, and Diana Wilson. 2005. &#8220;Changing the Tune: the Structure of the Input Affects Infants&#8217; Use of Absolute and Relative Pitch.&#8221; <i>Developmental Science</i> 8 (1): 1&#8211;7.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="saffran_et_al_2005"></a>Saffran, Jenny R., Karelyn Reeck, Aimee Niebuhr, and Diana Wilson. 2005. &#8220;Changing the Tune: the Structure of the Input Affects Infants&#8217; Use of Absolute and Relative Pitch.&#8221; <i>Developmental Science</i> 8 (1): 1&#8211;7.</p><div id="citediv_sapp_2011" class="flyoverdiv">Sapp, Craig S. 2011. &#8220;Computational Methods for the Analysis of Musical Structure.&#8221; Ph.D. diss., Stanford University.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="sapp_2011"></a>Sapp, Craig S. 2011. &#8220;Computational Methods for the Analysis of Musical Structure.&#8221; Ph.D. diss., Stanford University.</p><div id="citediv_schoenberg_1978" class="flyoverdiv">Schoenberg, Arnold. 1978. <i>Theory of Harmony. </i>Translated by R. Carter. University of California Press. Translation of <i>Harmonielehre, 3. ed. </i>Universal Edition, 1922.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="schoenberg_1978"></a>Schoenberg, Arnold. 1978. <i>Theory of Harmony. </i>Translated by R. Carter. University of California Press. Translation of <i>Harmonielehre, 3. ed. </i>Universal Edition, 1922.</p><div id="citediv_sechter_1853" class="flyoverdiv">Sechter, Simon. 1853. <i>Die Grunds&auml;tze der musikalischen Komposition</i>. Breitkopf und H&auml;rtel.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="sechter_1853"></a>Sechter, Simon. 1853. <i>Die Grunds&auml;tze der musikalischen Komposition</i>. Breitkopf und H&auml;rtel.</p><div id="citediv_shanahan_and_albrecht_2013" class="flyoverdiv">Shanahan, Daniel, and Joshua Albrecht. 2013. &#8220;The Acquisition and Validation of Large Web-Based Corpora.&#8221; Presented at the <i>Conference for the Society for Music Perception and Cognition</i>, Toronto, Canada.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="shanahan_and_albrecht_2013"></a>Shanahan, Daniel, and Joshua Albrecht. 2013. &#8220;The Acquisition and Validation of Large Web-Based Corpora.&#8221; Presented at the <i>Conference for the Society for Music Perception and Cognition</i>, Toronto, Canada.</p><div id="citediv_smith_and_schmuckler_2004" class="flyoverdiv">Smith, Nicholas A., and Mark A. Schmuckler. 2004. &#8220;The Perception of Tonal Structure through the Differentiation and Organization of Pitches.&#8221; <i>Journal of Experimental Psychology: Human Perception and Performance </i>30:<i> </i>268&#8211;286.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="smith_and_schmuckler_2004"></a>Smith, Nicholas A., and Mark A. Schmuckler. 2004. &#8220;The Perception of Tonal Structure through the Differentiation and Organization of Pitches.&#8221; <i>Journal of Experimental Psychology: Human Perception and Performance </i>30:<i> </i>268&#8211;286.</p><div id="citediv_stoddard_raphael_and_utgoff_2004" class="flyoverdiv">Stoddard, Joshua, Christopher Raphael, and Paul E. Utgoff. 2004. &#8220;Well-Tempered Spelling: A Key Invariant Pitch Spelling Algorithm.&#8221; In the <i>Proceedings of the International Society for Music Information Retrieval.</i></div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="stoddard_raphael_and_utgoff_2004"></a>Stoddard, Joshua, Christopher Raphael, and Paul E. Utgoff. 2004. &#8220;Well-Tempered Spelling: A Key Invariant Pitch Spelling Algorithm.&#8221; In the <i>Proceedings of the International Society for Music Information Retrieval.</i></p><div id="citediv_straus_1987" class="flyoverdiv">Straus, Joseph N. 1987. &#8220;The Problem of Prolongation in Post-Tonal Music.&#8221; <i>The Journal of Music Theory </i>31 (1): 1-21.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="straus_1987"></a>Straus, Joseph N. 1987. &#8220;The Problem of Prolongation in Post-Tonal Music.&#8221; <i>The Journal of Music Theory </i>31 (1): 1-21.</p><div id="citediv_stumpf_1883" class="flyoverdiv">Stumpf, Carl. 1883. <i>Tonpsychologie</i>. Breitkopf und H&auml;rtel.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="stumpf_1883"></a>Stumpf, Carl. 1883. <i>Tonpsychologie</i>. Breitkopf und H&auml;rtel.</p><div id="citediv_temperley_1997" class="flyoverdiv">Temperley, David. 1997. &#8220;An Algorithm for Harmonic Analysis.&#8221; <i>Music Perception</i> 15 (1): 31&#8211;68.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="temperley_1997"></a>Temperley, David. 1997. &#8220;An Algorithm for Harmonic Analysis.&#8221; <i>Music Perception</i> 15 (1): 31&#8211;68.</p><div id="citediv_temperley_1999" class="flyoverdiv">Temperley, David. 1999. &#8220;What's Key for Key? The Krumhansl-Schmuckler Key-Finding Algorithm Reconsidered.&#8221; <i>Music Perception: An Interdisciplinary Journal</i> 17 (1): 65-100.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="temperley_1999"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 1999. &#8220;What's Key for Key? The Krumhansl-Schmuckler Key-Finding Algorithm Reconsidered.&#8221; <i>Music Perception: An Interdisciplinary Journal</i> 17 (1): 65-100.</p><div id="citediv_temperley_2007" class="flyoverdiv">Temperley, David. 2007. <i>Music and Probability</i>. The MIT Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="temperley_2007"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2007. <i>Music and Probability</i>. The MIT Press.</p><div id="citediv_temperley_2009a" class="flyoverdiv">Temperley, David. 2009a. &#8220;A Statistical Analysis of Tonal Harmony.&#8221; <a href='http://www.theory.esm.rochester.edu/temperley/kp-stats/'>http://www.theory.esm.rochester.edu/temperley/kp-stats/</a>.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="temperley_2009a"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2009a. &#8220;A Statistical Analysis of Tonal Harmony.&#8221; <a href='http://www.theory.esm.rochester.edu/temperley/kp-stats/'>http://www.theory.esm.rochester.edu/temperley/kp-stats/</a>.</p><div id="citediv_temperley_2009b" class="flyoverdiv">Temperley, David. 2009b. &ldquo;Distributional Stress Regularity: A Corpus Study.&rdquo; <i>Journal of Psycholinguistic Research </i>38<i>: </i>75-92.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="temperley_2009b"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2009b. &ldquo;Distributional Stress Regularity: A Corpus Study.&rdquo; <i>Journal of Psycholinguistic Research </i>38<i>: </i>75-92.</p><div id="citediv_temperley_2010" class="flyoverdiv">Temperley, David. 2010. &#8220;Modeling Common-Practice Rhythm.&#8221; <i>Music Perception</i> 27 (5): 355&#8211;376.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="temperley_2010"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2010. &#8220;Modeling Common-Practice Rhythm.&#8221; <i>Music Perception</i> 27 (5): 355&#8211;376.</p><div id="citediv_temperley_2012" class="flyoverdiv">Temperley, David. 2012. &#8220;Computational Models of Music Cognition.&#8221; In Diana Deutsch (Ed.),<i> The Psychology of Music</i>, (3rd edition), 327-368. Elsevier.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="temperley_2012"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2012. &#8220;Computational Models of Music Cognition.&#8221; In Diana Deutsch (Ed.),<i> The Psychology of Music</i>, (3rd edition), 327-368. Elsevier.</p><div id="citediv_temperley_and_marvin_2008" class="flyoverdiv">Temperley, David, and Elizabeth W. Marvin. 2008. &#8220;Pitch&#8211;Class Distribution and the Identification of Key.&#8221; <i>Music</i> <i>Perception </i>25 (3):<i> </i>193&#8211;212.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="temperley_and_marvin_2008"></a>Temperley, David, and Elizabeth W. Marvin. 2008. &#8220;Pitch&#8211;Class Distribution and the Identification of Key.&#8221; <i>Music</i> <i>Perception </i>25 (3):<i> </i>193&#8211;212.</p><div id="citediv_temperley_and_sleator_1999" class="flyoverdiv">Temperley, David, and Daniel Sleator. 1999. &#8220;Modeling Meter and Harmony: A Preference-Rule Approach.&#8221; <i>Computer Music Journal</i> 23 (1):<i> </i>10&#8211;27.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="temperley_and_sleator_1999"></a>Temperley, David, and Daniel Sleator. 1999. &#8220;Modeling Meter and Harmony: A Preference-Rule Approach.&#8221; <i>Computer Music Journal</i> 23 (1):<i> </i>10&#8211;27.</p><div id="citediv_thompson_and_cuddy_1989" class="flyoverdiv">Thompson, William. F., and Lola L. Cuddy. 1989. &#8220;Sensitivity to Key Change in Chorale Sequences: A Comparison of Single Voices and Four-Voice Harmony.&#8221; <i>Music Perception</i> 7 (2): 151&#8211;168.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="thompson_and_cuddy_1989"></a>Thompson, William. F., and Lola L. Cuddy. 1989. &#8220;Sensitivity to Key Change in Chorale Sequences: A Comparison of Single Voices and Four-Voice Harmony.&#8221; <i>Music Perception</i> 7 (2): 151&#8211;168.</p><div id="citediv_tillmann_bharucha_and_bigand_2001" class="flyoverdiv">Tillmann, Barbara, Jamshed J. Bharucha, and Emmanuel Bigand. 2001. &#8220;Implicit Learning of Regularities in Western Tonal Music by Self-Organization.&#8221; In <i>Connectionist Models of Learning, Development and Evolution </i>(R.M. French and J.P. Sougn&eacute;, eds), 175-184<i>.</i> Springer.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="tillmann_bharucha_and_bigand_2001"></a>Tillmann, Barbara, Jamshed J. Bharucha, and Emmanuel Bigand. 2001. &#8220;Implicit Learning of Regularities in Western Tonal Music by Self-Organization.&#8221; In <i>Connectionist Models of Learning, Development and Evolution </i>(R.M. French and J.P. Sougn&eacute;, eds), 175-184<i>.</i> Springer.</p><div id="citediv_tillmann_et_al_2003" class="flyoverdiv">Tillmann, Barbara, Petr Janata, Jeffrey Birk, and Jamshed J. Bharucha. 2003. &#8220;The Costs and Benefits of Tonal Centers for Chord Processing.&#8221; <i>Journal of Experimental Psychology: Human Perception and Performance </i>29 (2): 470-482.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="tillmann_et_al_2003"></a>Tillmann, Barbara, Petr Janata, Jeffrey Birk, and Jamshed J. Bharucha. 2003. &#8220;The Costs and Benefits of Tonal Centers for Chord Processing.&#8221; <i>Journal of Experimental Psychology: Human Perception and Performance </i>29 (2): 470-482.</p><div id="citediv_trainor_and_trehub_1994" class="flyoverdiv">Trainor, Laurel J., and Sandra E. Trehub. 1994. &#8220;Key Membership and Implied Harmony in Western Tonal Music: Developmental perspectives.&#8221; <i>Perception &amp; Psychophysics</i> 56 (2): 125-132.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="trainor_and_trehub_1994"></a>Trainor, Laurel J., and Sandra E. Trehub. 1994. &#8220;Key Membership and Implied Harmony in Western Tonal Music: Developmental perspectives.&#8221; <i>Perception &amp; Psychophysics</i> 56 (2): 125-132.</p><div id="citediv_turing_1950" class="flyoverdiv">Turing, Alan. 1950. &#8220;Computing Machinery and Intelligence.&#8221; <i>Mind </i>59 (236): 433-466.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="turing_1950"></a>Turing, Alan. 1950. &#8220;Computing Machinery and Intelligence.&#8221; <i>Mind </i>59 (236): 433-466.</p><div id="citediv_tymoczko_2011" class="flyoverdiv">Tymoczko, Dmitri. 2011. <i>A Geometry of Music: Harmony and Counterpoint in the Extended Common Practice</i>. Oxford University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="tymoczko_2011"></a>Tymoczko, Dmitri. 2011. <i>A Geometry of Music: Harmony and Counterpoint in the Extended Common Practice</i>. Oxford University Press.</p><div id="citediv_viterbi_1967" class="flyoverdiv">Viterbi, Andrew J. 1967. &#8220;Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.&#8221; <i>IEEE Transactions on Information Theory</i> 13 (2): 260&#8211;269.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="viterbi_1967"></a>Viterbi, Andrew J. 1967. &#8220;Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.&#8221; <i>IEEE Transactions on Information Theory</i> 13 (2): 260&#8211;269.</p><div id="citediv_vogler_1776" class="flyoverdiv">Vogler, Georg J. 1776. <i>Tonwissenshcaft und Tonsetzkunst</i>. der kuhrf&uuml;rstlichen    Hofbuchdruckerei.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="vogler_1776"></a>Vogler, Georg J. 1776. <i>Tonwissenshcaft und Tonsetzkunst</i>. der kuhrf&uuml;rstlichen    Hofbuchdruckerei.</p><div id="citediv_vos_2000" class="flyoverdiv">Vos, Piet G. 2000. &#8220;Tonality Induction: Theoretical Problems and Dilemmas.&#8221; <i>Music Perception</i> 17 (4): 403-416.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="vos_2000"></a>Vos, Piet G. 2000. &#8220;Tonality Induction: Theoretical Problems and Dilemmas.&#8221; <i>Music Perception</i> 17 (4): 403-416.</p><div id="citediv_vos_and_van_geenen_1996" class="flyoverdiv">Vos, Piet G., and Erwin W. Van Geenen. 1996. &#8220;A Parallel-Processing Key-Finding Model.&#8221; <i>Music Perception</i> 14 (2): 185&#8211;223.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="vos_and_van_geenen_1996"></a>Vos, Piet G., and Erwin W. Van Geenen. 1996. &#8220;A Parallel-Processing Key-Finding Model.&#8221; <i>Music Perception</i> 14 (2): 185&#8211;223.</p><div id="citediv_weber_1817" class="flyoverdiv">Weber, Gottfried. 1817. <i>Versuch einer geordneten Theorie der Tonsetzkunst</i>. B. Schott's S&ouml;hne.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="weber_1817"></a>Weber, Gottfried. 1817. <i>Versuch einer geordneten Theorie der Tonsetzkunst</i>. B. Schott's S&ouml;hne.</p><div id="citediv_white_2013a" class="flyoverdiv">White, Christopher Wm. 2013a. &#8220;An Alphabet-Reduction Algorithm for Chordal n-grams.&#8221; In <i>Proceedings of the 4th International Conference on Mathematics and Computation in Music</i>. Heidelberg: Springer, 201&#8211;212.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="white_2013a"></a>White, Christopher Wm. 2013a. &#8220;An Alphabet-Reduction Algorithm for Chordal n-grams.&#8221; In <i>Proceedings of the 4th International Conference on Mathematics and Computation in Music</i>. Heidelberg: Springer, 201&#8211;212.</p><div id="citediv_white_2013b" class="flyoverdiv">White, Christopher Wm. 2013b. <i>Some Statistical Properties of Tonality, 1650-1900</i>. Ph.D. diss., Yale University.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="white_2013b"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2013b. <i>Some Statistical Properties of Tonality, 1650-1900</i>. Ph.D. diss., Yale University.</p><div id="citediv_white_2014" class="flyoverdiv">White, Christopher Wm. 2014. &#8220;Changing styles, changing corpora, changing tonal models.&#8221;<i> Music Perception </i>31 (2): 244&#8211;253.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="white_2014"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2014. &#8220;Changing styles, changing corpora, changing tonal models.&#8221;<i> Music Perception </i>31 (2): 244&#8211;253.</p><div id="citediv_white_2015" class="flyoverdiv">White, Christopher Wm. 2015. &#8220;A Corpus-Sensitive Algorithm for Automated Tonal Analysis.&#8221; In <i>Mathematics and Computation in Music, LNAI, 9110</i>, T. Collins, D. Meredith, A. Volk, eds., 115-121. Springer.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="white_2015"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2015. &#8220;A Corpus-Sensitive Algorithm for Automated Tonal Analysis.&#8221; In <i>Mathematics and Computation in Music, LNAI, 9110</i>, T. Collins, D. Meredith, A. Volk, eds., 115-121. Springer.</p><div id="citediv_white_and_quinn_2016a" class="flyoverdiv">White, Christopher Wm., and Ian Quinn. 2016. &#8220;Deriving and Evaluating SPOKE, a Set-Based Probabilistic Key Finder.&#8221; <i>Proceedings of the International Conference for Music Perception and Cognition. </i>San Fransisco: 68&#8211;73.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="white_and_quinn_2016a"></a>White, Christopher Wm., and Ian Quinn. 2016. &#8220;Deriving and Evaluating SPOKE, a Set-Based Probabilistic Key Finder.&#8221; <i>Proceedings of the International Conference for Music Perception and Cognition. </i>San Fransisco: 68&#8211;73.</p><div id="citediv_white_and_quinn_2016b" class="flyoverdiv">White, Christopher Wm., and Ian Quinn. 2016. &#8220;The Yale-Classical Archives Corpus.&#8221; <i>Empirical Musicology Review</i>: 50-58.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="white_and_quinn_2016b"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2016. &#8220;The Yale-Classical Archives Corpus.&#8221; <i>Empirical Musicology Review</i>: 50-58.</p><div id="citediv_white_and_quinn_forthcoming" class="flyoverdiv">White, Christopher Wm., and Ian Quinn. Forthcoming. &#8220;Chord Context and Harmonic Function in Tonal Music.&#8221; <i>Music Theory Spectrum.</i></div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="white_and_quinn_forthcoming"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. Forthcoming. &#8220;Chord Context and Harmonic Function in Tonal Music.&#8221; <i>Music Theory Spectrum.</i></p><div id="citediv_wiggins_2012" class="flyoverdiv">Wiggins, Geraint A. 2012. &#8220;The Future of (Mathematical) Music Theory.&#8221; <i>Journal of Mathematics and Music</i> 6 (2): 135-144.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="wiggins_2012"></a>Wiggins, Geraint A. 2012. &#8220;The Future of (Mathematical) Music Theory.&#8221; <i>Journal of Mathematics and Music</i> 6 (2): 135-144.</p><div id="citediv_wiggins_m&uuml;llensiefen_and_pearce_2010" class="flyoverdiv">Wiggins, Geraint A., Daniel M&uuml;llensiefen, and Marcus Pearce. 2010. &#8220;On the Non-Existence of Music: Why Music Theory is a Figment of the Imagination.&#8221; <i>Musicae Scientiae </i>14 (1): 231&#8211;255.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="wiggins_m&uuml;llensiefen_and_pearce_2010"></a>Wiggins, Geraint A., Daniel M&uuml;llensiefen, and Marcus Pearce. 2010. &#8220;On the Non-Existence of Music: Why Music Theory is a Figment of the Imagination.&#8221; <i>Musicae Scientiae </i>14 (1): 231&#8211;255.</p><div id="citediv_winograd_1968" class="flyoverdiv">Winograd, Terry. 1968. &#8220;Linguistics and the Computer Analysis of Tonal Harmony.&#8221; <i>Journal of Music Theory</i> 12 (1): 2&#8211;49.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="winograd_1968"></a>Winograd, Terry. 1968. &#8220;Linguistics and the Computer Analysis of Tonal Harmony.&#8221; <i>Journal of Music Theory</i> 12 (1): 2&#8211;49.</p><div id="citediv_zikanov_2014" class="flyoverdiv">Zikanov, Kirill. 2014. &#8220;Metric Properties of Mensural Music: An Autocorrelation Approach.&#8221; <i>Presented at The National Meeting of the American Musicological Society, Milwaukee.</i></div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="zikanov_2014"></a>Zikanov, Kirill. 2014. &#8220;Metric Properties of Mensural Music: An Autocorrelation Approach.&#8221; <i>Presented at The National Meeting of the American Musicological Society, Milwaukee.</i></p>
       
	<div style="height:24px;width:150px;background-color:#4c7381;float:left;text-align: center;vertical-align: middle;line-height: 24px;">
		&nbsp;&nbsp;&nbsp;
		<a style="color:white;" onmouseover="this.style.color='#0000ff';text-decoration:none" 
		onmouseout="this.style.color='white';" href="#Beginning">Return to beginning</a>
		&nbsp;&nbsp;&nbsp;
	</div><br><br>

	
<!-------------------------------- Footnotes List -------------------------------------------->

    	
	<hr>
	
	<h3><a name="Footnotes">Footnotes</a></h3>
	
	           <p><a name="FN0">*</a> I would like to thank Megan Kaes Long, Pat McCreless, Jessica Racco, Ian Quinn and <i>MTO</i>&#8217;s anonymous reviewers for their input on earlier drafts. All experiments were undertaken with clearance from the IRB at the University of North Carolina at Greensboro.<br><a href="#FN0REF">Return to text</a></p><div id="fndiv0" class="flyoverdiv">I would like to thank Megan Kaes Long, Pat McCreless, Jessica Racco, Ian Quinn and <i>MTO</i>&#8217;s anonymous reviewers for their input on earlier drafts. All experiments were undertaken with clearance from the IRB at the University of North Carolina at Greensboro.</div><p><a name="FN1">1.</a> In this article, I will adopt Temperley&#8217;s understanding of &#8220;key&#8221; as the perception, cognition, or identification of some tonal center. Of course, the terms &#8220;tonality&#8221; and &#8220;key&#8221; have many different subtle definitions, depending upon the repertoire being considered; and, even in contemporary work in music cognition, &#8220;tonality&#8221; can be defined in a wide variety of ways. Vos (<a href="#vos_2000" id="citation_vos_2000_67dc99bb1494d">2000</a>), for instance, argues that &#8220;tonality&#8221; can mean so many different things&#8212;from the simple perception of some privileged pitch class to a complex hierarchy of tones and chords&#8212;that the word is at risk of losing any definitive meaning. It is outside the bounds of this article to parse out all the different meanings associated with that concept; however, every theory that makes some kind of claim about the mechanics behind tonal orientation does so using either a feedforward or feedback logic. I do, however, return to some definitional aspects of &#8220;key&#8221; at the end of this essay.<br><a href="#FN1REF">Return to text</a></p><p><a name="FN2">2.</a> Furthermore, surface or perceived musical events are <i>actually observed</i> within whatever musical medium is being used: a rhythm is <i>seen</i> in a score and a pitch is <i>heard</i> in a performance. On the other hand structures or cognized musical events are the result of some interpreted act: rhythms <i>interpreted</i> in some meter and pitches are <i>interpreted</i> in some key.<br><a href="#FN2REF">Return to text</a></p><p><a name="FN3">3.</a> This discussion conflates the sound signal with the score as well as the &#8220;in-the-moment&#8221; understanding of a key with a self-reflective/analytical understanding of key. I will discuss how visual and aural modes (and momentary and reflective processes) of music perception function in my argument below. However it should also be noted that the aural/visual domains and the listening/analytical acts are usefully and playfully intertwined in much music theory discourse (Lewin 1987), and using explicit and implicit tasks in key-finding behavioral experiments produces generally the same outcomes (<a href="#aarden_2003" id="citation_aarden_2003_67dc99bb14956">Aarden 2003</a>).<br><a href="#FN3REF">Return to text</a></p><p><a name="FN4">4.</a> Clearly, the feedforward/feedback dichotomy overlaps with other classic dichotomies, particularly bottom-up/top-down and connectionist/rule-based approaches; however, these mappings are by no means one-to-one. For instance, just as top-down and rule-based approaches use some pre-formed set of expectations or templates to make sense of some phenomena, so too do feedback systems use the organizations of broader structures to inform key finding. But a feedforward approach could also use some broader structure to identify a key if that structure was not, in turn, affected by the key center. In other words, these more classic dichotomies hinge upon the relationship between a complex phenomenon and the evidence used to support this phenomenon, while my dichotomy hinges upon whether there is a relationship between more than one type of high-level organization.<br><a href="#FN4REF">Return to text</a></p><p><a name="FN5">5.</a> For the computationally minded: I implemented this key-profile analysis with Bellmann-Budge weightings and a correlation procedure implemented in music 21 (<a href="#cuthbert_and_ariza_2011" id="citation_cuthbert_and_ariza_2011_67dc99bb1495b">Cuthbert and Ariza 2011</a>). The procedure uses the number of quarter-note durations of each pitch-class as a 12-member vector and then calculates the correlation coefficient between that and the ideal vector of each of the 24 major and minor keys. The annotations for the first two measures use that two-measure window (mm. 5&ndash;6), and measure 7&#8217;s annotations use a three-measure window (mm. 5&ndash;7).<br><a href="#FN5REF">Return to text</a></p><p><a name="FN6">6.</a> These multiple solutions are possible because both models assign probabilities for each key: in this instance, the various probabilities of the different key assessments are comparable. Specifically, &#8220;probabilistic&#8221; understandings of musical keys allow for multiple key centers to vie for prominence within a passage (<a href="#temperley_2007" id="citation_temperley_2007_67dc99bb1495f">Temperley 2007</a>). We might say, for instance, that when we hear or analyze a passage unambiguously in C major, we might be 95% sure of a C tonal center, with the remaining 5% allowing for an impending surprisingly modulation. Tonally ambiguous passages, then, might have the probability mass comparably divided amongst several keys.<br><a href="#FN6REF">Return to text</a></p><p><a name="FN7">7.</a> The chord-progression model ignores the <nobr><span style= 'letter-spacing:-1px'>G<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266d;</span><span></nobr> of measure 6 to find a V7&ndash;i progression in F, and it ignores the <nobr><span style= 'letter-spacing:-1px'>E<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266e;</span><span></nobr> of measure 5 to construct a ii&ndash;V7&ndash;i progression in D flat.<br><a href="#FN7REF">Return to text</a></p><p><a name="FN8">8.</a> Computational formalization often overlaps with behavioral testing, in that the former can be designed to model or mimic a cognitive process or predict human behavior. See Temperley (2012) for an overview of computational modeling of music cognition.<br><a href="#FN8REF">Return to text</a></p><p><a name="FN9">9.</a> For a full account of the rich history of computational implementations for music analysis, see <a href="#temperley_2007" id="citation_temperley_2007_67dc99bb14965">Temperley 2007</a> or <a href="#white_2013a" id="citation_white_2013a_67dc99bb14968">White 2013a</a>.<br><a href="#FN9REF">Return to text</a></p><p><a name="FN10">10.</a> Generally, these profiles show tonic pcs occurring most frequently, followed by the other members of the tonic triad, other members of the diatonic set, and finally the chromatic pitches. Such a model can then match a passage&#8217;s pitch-class distribution to the ideal profile of one of the 24 major and minor keys, thereby assigning the best-fitting key interpretation to the vector that best correlates to the observed distribution of a musical surface.  This sort of analysis arose from &#8220;probe-tone studies&#8221; in which participants were asked to rate how well various pitch classes fit into a tonal context. (See, for instance, <a href="#krumhansl_and_shepard_1979" id="citation_krumhansl_and_shepard_1979_67dc99bb1496d">Krumhansl and Shepard 1979</a>, <a href="#castellano_bharucha_and_krumhansl_1984" id="citation_castellano_bharucha_and_krumhansl_1984_67dc99bb14971">Castellano, Bharucha, and Krumhansl 1984</a>, <a href="#oram_and_cuddy_1995" id="citation_oram_and_cuddy_1995_67dc99bb14975">Oram and Cuddy 1995</a>, <a href="#creel_and_newport_2002" id="citation_creel_and_newport_2002_67dc99bb14979">Creel and Newport 2002</a>, <a href="#smith_and_schmuckler_2004" id="citation_smith_and_schmuckler_2004_67dc99bb1497c">Smith and Schmuckler 2004</a>, and <a href="#huron_2006" id="citation_huron_2006_67dc99bb14980">Huron 2006</a>). In her collaboration with Mark Schmuckler, Krumhansl (<a href="#krumhansl_1990" id="citation_krumhansl_1990_67dc99bb14984">1990</a>) noted that the relative distribution of scale-degrees within tonal music seemed to mostly conform to the goodness-of-fit ratings found in probe-tone studies, and that this correspondence could be used in automated key-finding models.<br><a href="#FN10REF">Return to text</a></p><p><a name="FN11">11.</a> We would imagine that identifying a tritone as a particular pitch-class spelling (i.e., a diminished fifth versus an augmented fourth) to be specifically a visual cue; however, this is not always the case. Tuning and surrounding pitch cues can assist in the aural interpretation; several of the above-cited studies also suggest that presenting a tritone in ascending versus descending patterns contributes to how it is interpreted.<br><a href="#FN11REF">Return to text</a></p><p><a name="FN12">12.</a> Metrical Preference Rule 9 states, &#8220;Prefer a metrical analysis that minimizes conflict with the [harmonic] time-span reduction&#8221; (<a href="#lerdahl_and_jackendoff_1983" id="citation_lerdahl_and_jackendoff_1983_67dc99bb14988">90</a>), and Time Span Reduction Preference Rule 1 states, &#8220;Of the possible choices for head of a time­span T, prefer a choice that is in a relatively strong metrical position.&#8221; (<a href="#lerdahl_and_jackendoff_1983" id="citation_lerdahl_and_jackendoff_1983_67dc99bb1498c">160</a>).<br><a href="#FN12REF">Return to text</a></p><p><a name="FN13">13.</a> See <a href="#dineen_2005" id="citation_dineen_2005_67dc99bb14990">Dineen 2005</a> for a review of Schoenberg&#8217;s tonal theories.<br><a href="#FN13REF">Return to text</a></p><p><a name="FN14">14.</a> It is worth reflecting on the omnipresence of schemata in theories of key and how they underpin feedforward and feedback models. Scales or key profiles, for instance, are learned ways to organize pitches in much the same way cadential gestures are. Energetic relationships between scale degrees are as much preformed schematic templates as Gjerdingen&#8217;s (<a href="#gjerdingen_2007" id="citation_gjerdingen_2007_67dc99bb14994">2007</a>) <i>partimenti</i> patterns. Even interpreting a pitch involves a (albeit subconscious) process of parsing and grouping variegated sound stimuli into a single apperception (<a href="#plomp_1964" id="citation_plomp_1964_67dc99bb14997">Plomp 1964</a>, <a href="#moore_2012" id="citation_moore_2012_67dc99bb1499a">Moore 2012</a>). More sophisticatedly, recognizing that a <nobr><span style= 'letter-spacing:-1px'>G<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr>-D tritone might indicate an A tonal center involves recognizing that the diminished fifth participates in some schema (i.e., as part of a major scale, as invoking an expectation of a particular resolution, etc.). Clearly, schemata underpin both types of key-finding models; the difference between feedforward and feedback approaches turns on whether the key is used as evidence for the schematic structures. Claims like, &#8220;all white notes are present, and therefore a passage is in C major&#8221; are different from claims like, &#8220;This chord progression makes the most sense in C major.&#8221; In the former, the claim arises from a stable phenomenon: a perception like &#8220;these are all white notes&#8221; would never be revised if you rethought the passage&#8217;s key. In contrast, a chord progression can be drastically revised if the tonal orientation is rethought.<br><a href="#FN14REF">Return to text</a></p><p><a name="FN15">15.</a> While I will engage with this concept in more depth below, it is important to note that the differences between these models hinge upon the different definitions of tonality these authors propose.<br><a href="#FN15REF">Return to text</a></p><p><a name="FN16">16.</a> Laitz, for instance, recommends his readers identify modulations when chord-progression analyses &#8220;become more complex or are nonsense&#8221; in relation to the prior tonic (<a href="#laitz_2008" id="citation_laitz_2008_67dc99bb149a0">2008</a>, 572). Clendinning and Marvin (<a href="#clendinning_and_marvin_2011" id="citation_clendinning_and_marvin_2011_67dc99bb149a4">2011</a>) instruct their readers to use cadences to identify keys, yet their instructions on cadence identification assume a student has already identified the underlying key.<br><a href="#FN16REF">Return to text</a></p><p><a name="FN17">17.</a> It should be noted that a theorist&#8217;s tonal logics need not exclusively be either feedback or feedforward. Clendinning and Marvin (<a href="#clendinning_and_marvin_2011" id="citation_clendinning_and_marvin_2011_67dc99bb149a9">2011</a>) first introduce key as connected to diatonic scales, then nuance their discussion later to include beginning and ending notes, and finally to include cadence identification. Piston (<a href="#piston_1941" id="citation_piston_1941_67dc99bb149ac">1941</a>) distinguishes between tonality and modality by saying, &#8220;Tonality is the organized relationship of tones in music. . . . Modality refers to the choice of the tones between which these relationships exist. Tonality is synonymous with key, modality with scale. . . [Any of the] modes may be transposed into all tonalities, simply by changing the pitch of the tonic note and preserving the interval relationships&#8221; (30&#8211;31). When describing how tonal relationships are obtained, he writes, &#8220;The strongest tonal factor in music is the dominant effect. Standing alone, it determines the key much more decisively than the tonic chord itself. . .&#8221; (34) This passage suggests that identifying a dominant chord yields a tonal center, something that would place the theory on the distributional side (as one simply asks whether there is a dominant harmony in the distribution of chords), and would use a feedback logic (since knowing a key would help identify a &#8220;dominant effect&#8221;). Piston, however, goes on to write, &#8220;The greatest strength of tonality in harmonic progressions involving only triads lies in those progressions which combine dominant harmony with harmony from the subdominant side. . . The progressions IV-V and II-V cannot be interpreted in more than one tonality, without chromatic alteration.&#8221; (36) The theory is not a linear one, and therefore is structural. However, given that Piston&#8217;s logic seems to be that the pitch content of these successions immediately point to a single diatonic set, the logic is now feedforward: identifying two triads linearly related by T5 or T2 places you in a single key.<br><a href="#FN17REF">Return to text</a></p><p><a name="FN18">18.</a> Indeed, Raphael and Stoddard&#8217;s (<a href="#raphael_and_stoddard_2003" id="citation_raphael_and_stoddard_2003_67dc99bb149b1">2003</a>, <a href="#raphael_and_stoddard_2004" id="citation_raphael_and_stoddard_2004_67dc99bb149b5">2004</a>) implementations of an Expectation Maximization procedure for their study heavily influenced my own implementation.<br><a href="#FN18REF">Return to text</a></p><p><a name="FN19">19.</a> Vos and Van Geenen&#8217;s (<a href="#vos_and_van_geenen_1996" id="citation_vos_and_van_geenen_1996_67dc99bb149ba">1996</a>) algorithm also assigns key by aligning a melody to a scale collection and to chords membership in parallel in a way very reminiscent of the current article; however, their algorithm is designed specifically for monophonic contexts.<br><a href="#FN19REF">Return to text</a></p><p><a name="FN20">20.</a> The literature within music information retrieval and artificial musical intelligence is also replete with so-called &#8220;feedback&#8221; systems. In fact, the current emphasis within this literature on Deep Learning (especially Recurrent Neural Networks) is heavily reliant on this logic, be it for harmonization (e.g., <a href="#feulner_1993" id="citation_feulner_1993_67dc99bb149bd">Feulner 1993</a>, <a href="#gang_lehman_and_wagner_1998" id="citation_gang_lehman_and_wagner_1998_67dc99bb149c1">Gang, Lehman, and Wagner 1998</a>, <a href="#de_prisco_et_al_2010" id="citation_de_prisco_et_al_2010_67dc99bb149c5">De Prisco et al. 2010</a>), pattern completion (<a href="#liu_and_randall_2016" id="citation_liu_and_randall_2016_67dc99bb149c9">Liu and Randall 2016</a>), melodic composition (<a href="#colombo_et_al_2016" id="citation_colombo_et_al_2016_67dc99bb149cd">Colombo et al. 2016</a>), or even the analysis of musical characteristics (<a href="#eyben_et_al_2010" id="citation_eyben_et_al_2010_67dc99bb149d1">Eyben et al. 2010</a>, <a href="#boulanger-lewandowski_bengio_and_vincent_2013" id="citation_boulanger-lewandowski_bengio_and_vincent_2013_67dc99bb149d5">Boulanger-Lewandowski, Bengio, and Vincent 2013</a>). In these models&#8217; engineering, many different parameters are maximized concomitantly: key might be dependent upon meter, which is dependent on the harmonic structure, which is dependent on key, and so on. This literature is important because it is useful: this kind of modeling underpins technologies like Spotify, Google Music, algorithmic DJs, and the like. However, these models&#8217; complexity and their emphasis on usefulness/production make it difficult to dissect and test their connection to music cognition and music theory. A notable exception is Cancino-Chacon, Grachten, and Agres (<a href="#cancino-chacon_grachten_and_agres_2017" id="citation_cancino-chacon_grachten_and_agres_2017_67dc99bb149da">2017</a>), which uses an RNN to simulate actual listeners&#8217; expectations by training their model on audio recordings of tonal music.<br><a href="#FN20REF">Return to text</a></p><p><a name="FN21">21.</a> Even more generally, we could consider any cognitive study of tonal harmony to implicitly involve key information.<br><a href="#FN21REF">Return to text</a></p><p><a name="FN22">22.</a> In fact <a href="#vos_and_van_geenen_1996" id="citation_vos_and_van_geenen_1996_67dc99bb149de">Vos and Van Geenen 1996</a> explicitly put their finger on this issue (see their discussion on pp. 207&#8211;8, for instance). Alternately, from a modeling perspective, Lerdahl and Jackendoff&#8217;s (<a href="#lerdahl_and_jackendoff_1983" id="citation_lerdahl_and_jackendoff_1983_67dc99bb149e2">1983</a>) <i>Generative Theory of Tonal Music</i> attempts a rigorous formal theory of tonal music via a generative grammar accompanied by preference rules. Several empirical tests of certain aspects of this theory have been performed (for instance, <a href="#lerdahl_and_krumhansl_2007" id="citation_lerdahl_and_krumhansl_2007_67dc99bb149e6">Lerdahl and Krumhansl 2007</a>) as well as attempts to computationally implement the theory (<a href="#temperley_and_sleator_1999" id="citation_temperley_and_sleator_1999_67dc99bb149e9">Temperley and Sleator 1999</a>). While these studies remain provocative, the learning of these preference rules and the particulars of the integration of various domains remain to be accounted for.<br><br>Given that the experimental designs are traditionally feedforward in their logic, the lack of empirical tests on feedback models is not surprising. Experimental designs generally test a parameter&#8217;s effect on some outcome by varying that parameter and testing the resulting changes in outcome: different outcomes are generated (unidirectionally) by different parameters&#8217; settings. Consider, for instance, the straightforward parallels between a key-profile model and an experiment that tests the effects of pitch-class distribution on key assessment. Changing the pitch-class distribution modifies the model&#8217;s output, and such changes correspond to the hypothesized behavior of the participant. This crisp mapping of a theoretical model onto an experimental design likely has some influence on the popularity of these sorts of tonal models in music cognition research.<br><a href="#FN22REF">Return to text</a></p><p><a name="FN23">23.</a> There are two main benefits of computational analysis that I rely upon in this article: formal rigor and cognitive mimicry. To the first: in the words of Bo Alphonce,  &#8220;the computer unrelentingly demands theory&#8221; (<a href="#alphonce_1980" id="citation_alphonce_1980_67dc99bb149ee">1980</a>, 26) and in the words of John Rahn, &#8220;to explicate something is, ultimately, to formalize it, that is, to make it into a machine at whose metaphorically whirring and clicking parts we are happy to stare, and be enlightened&#8221; (<a href="#rahn_1980" id="citation_rahn_1980_67dc99bb149f2">1980</a>, 66).  Formalizing key-finding into computational algorithms then, challenges us to a specificity and precision that allows us to inspect and test our assumptions about some topic. Second, a computational model can potentially mimic some aspect of a cognitive process. As described by Temperley (<a href="#temperley_2012" id="citation_temperley_2012_67dc99bb149f5">2012</a>), if a program takes in the same data as a human, manipulates the data in the same way as a human, and produces the same output as a human, then claims of human/computer parallelism can be made. (This idea is provocatively framed by Wiggins, M&uuml;llensiefen, and Pearce&#8217;s  (<a href="#wiggins_m&uuml;llensiefen_and_pearce_2010" id="citation_wiggins_m&uuml;llensiefen_and_pearce_2010_67dc99bb149fa">2010</a>) and Wiggins&#8217;s (<a href="#wiggins_2012" id="citation_wiggins_2012_67dc99bb149fe">2012</a>) understanding of traditional music theory as an informal system that provides fodder for empirical testing.) Additionally, certain programming techniques can not only claim to undertake a process that mimics human cognition, but can also imitate a human&#8217;s learning of that process. Such &#8220;machine-learning&#8221; techniques imitate aspects of humans&#8217; &#8220;statistical learning,&#8221; or the idea that listeners can form expectations by being exposed to sequences (musical or otherwise) with certain statistical regularities. (For examples in music, see <a href="#saffran_et_al_1999" id="citation_saffran_et_al_1999_67dc99bb14a02">Saffran et al. 1999</a>; <a href="#creel_newport_and_aslin_2004" id="citation_creel_newport_and_aslin_2004_67dc99bb14a06">Creel, Newport, and Aslin 2004</a>; <a href="#saffran_et_al_2005" id="citation_saffran_et_al_2005_67dc99bb14a0a">Saffran et al. 2005</a>; <a href="#huron_2006" id="citation_huron_2006_67dc99bb14a0d">Huron 2006</a>; <a href="#pearce_mullensiefen_and_wiggins_2008" id="citation_pearce_mullensiefen_and_wiggins_2008_67dc99bb14a11">Pearce, Mullensiefen, and Wiggins 2008</a>; <a href="#loui_wessel_and_hudson_kam_2010" id="citation_loui_wessel_and_hudson_kam_2010_67dc99bb14a15">Loui, Wessel, and Hudson Kam 2010</a>; <a href="#loui_2012" id="citation_loui_2012_67dc99bb14a18">Loui 2012</a>; <a href="#pearce_et_al_2010" id="citation_pearce_et_al_2010_67dc99bb14a1c">Pearce et al. 2010</a>.) Being based on the statistical regularities of some dataset by their very nature, corpus-based models (or models based on large datasets) can map the ways in which exposure to a repertoire might allow listeners to form expectations or create some cognitive model. The engineering of such models, then, can make claims not only about a cognitive process, but also about the properties of a corpus that might allow humans to learn to perform that process. (More in-depth discussions of connections between musical statistics and learning can be found in Bharucha <a href="#bharucha_1987" id="citation_bharucha_1987_67dc99bb14a20">1987</a> and <a href="#bharucha_1991" id="citation_bharucha_1991_67dc99bb14a24">1991</a>, <a href="#temperley_2007" id="citation_temperley_2007_67dc99bb14a27">Temperley 2007</a>, <a href="#rohrmeier_and_cross_2008" id="citation_rohrmeier_and_cross_2008_67dc99bb14a2a">Rohrmeier and Cross 2008</a>, and <a href="#byros_2009" id="citation_byros_2009_67dc99bb14a2e">Byros 2009</a>.)<br><a href="#FN23REF">Return to text</a></p><p><a name="FN24">24.</a> There are certain limits to this computational methodology. First, using computer-readable formats to represent aspects of &#8220;human perception&#8221; conflates visual (score-based) and aural (listener-oriented) experiences and perceptions. This conflation can exclude important distinctions: for instance, the information gleaned from hearing a tritone can be very different than that gleaned from visually identifying an interval as a diminished fifth. Relatedly, the current study conflates the concept of <i>musical surface</i> with the raw elements of a computer-readable score. While I discuss my data formatting in more depth below, this generally entails something like a &#8220;piano roll&#8221; representation, with each note&#8217;s pitch, duration, timepoint, loudness, and instrumentation as being the essential elements. The present study aims to honestly embrace these conflations; it will remain for future work to manipulate the computational formatting to highlight different aspects of the visual/aural dichotomy and the musical surface. Third, just because a computer does something that adheres to human behavior does not mean the computer is undertaking the same processes as the human. While the input and output of a computational process can be somewhat rigorously tested in an experimental setting, understanding the process is somewhat more obscure: pinpointing the specifics of a cognitive process is difficult at best and impossible at worst. Computational modeling of cognitive or theoretical systems therefore is undertaken with the caveat that their most important claims&#8212;the mechanics of how they produce their output&#8212;are usually orthogonally testable experimentally against human behavior rather than directly. This orthogonal relationship between computer and cognitive engineering is a primary reason to frame my model as a &#8220;proof of concept&#8221; rather than conclusive evidence. Finally, what computational analysis gains in specificity it also gains in complexity. Each minute step in an action creates a to-do list that becomes quite long. Consider the prototypical frustration a seasoned musician might encounter when describing &#8220;key&#8221; to a non-musician: this exemplifies the complexity and minutiae of the computational parameterization of such a process. Using computational modeling for music analysis, then, can often seem ungainly, but only insomuch as it reflects the complexities of some theoretic, analytical, or cognitive process.<br><a href="#FN24REF">Return to text</a></p><p><a name="FN25">25.</a> Functionally, converting pcs to these mod-12 scale degrees transposes all pcs to the key of C, since in all keys the tonic pcs will be transposed to 0, the supertonic will become 2, etc. Importantly, while the natural environment for diatonic scale degrees is modulo 7 space, using mod-12 space 1) allows for chromatically altered scale degrees (e.g., V/V would be &lt;269&gt;, 2) uses the same space for the major and minor mode (a I chord would be &lt;047&gt; while a i chord would be &lt;037&gt;, and 3) uses the same size universe for scale degrees and pcs. This follows <a href="#white_2015" id="citation_white_2015_67dc99bb14a31">White 2015</a>.<br><a href="#FN25REF">Return to text</a></p><p><a name="FN26">26.</a> Note that these Roman numerals are stand-ins for scale-degree sets: the algorithm under consideration neither interprets nor outputs a &#8220;I&#8221; chord <i>per se</i>, but rather the set &lt;0, 4, 7&gt;. This is the same for letter names used in the text and examples, as the program only ever deals in numerical pitch classes. Also note that since the algorithm uses unordered sets, the Roman numerals are always represented here without inversion. The implications of this are discussed below. I am also imagining this toy as treating sevenths and triads identically (i.e., V7 has the same 2-gram properties as V), and for simplicity I have removed the iii chord from the original formulation in the Kostka-Payne textbook.<br><a href="#FN26REF">Return to text</a></p><p><a name="FN27">27.</a> Using informatics &#8220;<i>n</i>-gram&#8221; parlance in which <i>n</i> equals the number of chords in a sequence, these sequences would be 2-grams, since the arrows express the probability of two-chord successions. That is, the probability of a chord appearing within this syntax is contingent only on the chord that appears before it: this process therefore assigns probabilities using only successions of two chords.<br><a href="#FN27REF">Return to text</a></p><p><a name="FN28">28.</a> Following previous work in White (<a href="#white_2013b" id="citation_white_2013b_67dc99bb14a35">2013b</a>), the prototypes used here were produced by applying Equation 3 to the raw data of the Yale Classical Archives corpus (<a href="#white_and_quinn_2016b" id="citation_white_and_quinn_2016b_67dc99bb14a39">White and Quinn 2016b</a>). The corpus contains 14,051,144 <i>salami slices</i>, or sets of notes occurring each time a pitch is added or subtracted from the texture, from music across the Western-European common practice. The reduction process was allowed to iterate until no more changes were made, and any repeated chords that arose during the reduction process were conflated into a single chord event. Even though the corpus is noisy and error-laden (<a href="#shanahan_and_albrecht_2013" id="citation_shanahan_and_albrecht_2013_67dc99bb14a3c">Shanahan and Albrecht 2013</a>, <a href="#declercq_2016" id="citation_declercq_2016_67dc99bb14a40">deClercq 2016</a>), the corpus is quite large, and as such is ideal for a machine-learning process.<br><br>White&#8217;s (<a href="#white_2013b" id="citation_white_2013b_67dc99bb14a43">2013b</a>) process produces a distribution of chords that occurs most frequently in the YCAC corpus. Following Pardo and Birmingham (<a href="#pardo_and_birmingham_1999" id="citation_pardo_and_birmingham_1999_67dc99bb14a48">1999</a>), Bellmann (<a href="#bellman_2005" id="citation_bellman_2005_67dc99bb14a4b">2005</a>), and Quinn and Mavromatis (<a href="#quinn_and_mavromatis_2011" id="citation_quinn_and_mavromatis_2011_67dc99bb14a4f">2011</a>), I exclude the <i>long tail</i> of the distribution, or those chords that make up the majority of chord types, but the minority  of chord instances of the distribution. The top 22 chords account for 85.3% of all chord occurrences, with the remaining chords accounting for only 14.7%. These infrequent chords were removed from the series, and chord transitions were normalized. For the purposes of this paper, I exclude the non-tertian vocabulary members.<br><a href="#FN28REF">Return to text</a></p><p><a name="FN29">29.</a> Contrast this with the encircled pitch content ending the second window, {D, <nobr><span style= 'letter-spacing:-1px'>F<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr>, <nobr><span style= 'letter-spacing:-1px'>G<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr>, A, B}. While the example chooses the {<nobr><span style= 'letter-spacing:-1px'>G<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr>, B, D} subset as the salient subset, the passage also contains the subset {B, D, <nobr><span style= 'letter-spacing:-1px'>F<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr>}. However, choosing this subset would yield the chord progression D-b-E7-b, which&#8212;while <i>possible</i>&#8212;would be instantiated by scale-degree sets that occur less frequently in the model&#8217;s corpus-derived syntax (e.g., A:  IV-ii-V7-ii) than do those progressions that use the {<nobr><span style= 'letter-spacing:-1px'>G<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr>, B, D} subset (e.g., a subset of the previous beat&#8217;s V7).<br><a href="#FN29REF">Return to text</a></p><p><a name="FN30">30.</a> This computational analysis ignores certain musical parameters in order to focus on others. Readers will likely have punctuated the preceding discussion with such thoughts as, &#8220;surely the voicing of the Alberti bass encourages us to hear certain pitches as the salient chord tones!&#8221;, and &#8220;the metrical emphasis clearly favors some notes over others!&#8221; This study&#8217;s focus on certain organizational structures will necessarily exclude the inclusion of others. Musical characteristics such as the bass voice, and the presence of arpeggios, melodic gestures, and the like all help listeners and analysts distinguish chord tones from non-chord tones; however, these parameters are not considered in this study and will remain for future investigation.<br><br>Additionally, given the above-cited literature on the importance of meter to the key-finding task as well as the difficulties I encountered in early implementations of meter-blind implementations, I incorporated a metric parameter into this model. In particular, my early experimental models would often divide the pitch contents of a beat between the surrounding chords to create more probable &#8211; but unintuitive &#8211; chord sequences. Consider the Mozart example: by eliding the barline and considering the <nobr><span style= 'letter-spacing:-1px'>F<span style='font-family: Arial Unicode MS, Lucida Sans Unicode;'>&#x266f;</span><span></nobr> a passing seventh, the algorithm can produce the more-probable I-IV-V7 progression in the first phrase; by grouping together the last three beats of the second phrase, the algorithm can read the progression simply as IV-V7, dispensing with the less-probable intermittent submediant chord. Therefore, just as Example 2 divides the surface into half-note durations, the reduction model is provided with some basic notion of the underlying meter in order to divide the surface into its constituent chords. This assumption is not without support in cognitive research: the relationship between meter and key is investigated in Prince, Thompson, and Schmuckler (<a href="#prince_thompson_and_schmuckler_2009" id="citation_prince_thompson_and_schmuckler_2009_67dc99bb14a57">2009</a>) and Prince and Schmuckler (<a href="#prince_and_schmuckler_2014" id="citation_prince_and_schmuckler_2014_67dc99bb14a5b">2014</a>).<br><a href="#FN30REF">Return to text</a></p><p><a name="FN31">31.</a> A more technical report on how I implement this engineering can be found in <a href="#white_2015" id="citation_white_2015_67dc99bb14a5f">White 2015</a>.<br><a href="#FN31REF">Return to text</a></p><p><a name="FN32">32.</a> The corpus contains 46 examples of Western European common-practice music; five pieces were removed due to difficulties converting them into digital representations usable by the methods described below. The removed pieces all used pulses that were problematic to quantize&#8212;I did not want the findings unduly influenced by meter, a musical parameter that this article does not address. The reason for the difficulties occurred when adding repeating decimals: a triplet converts to 0.33333 of a quarter note, three of which do not equal 1.0, but rather 0.999999. While this difficulty could certainly be overcome, it was decided to forgo those examples in the current analysis.<br><a href="#FN32REF">Return to text</a></p><p><a name="FN33">33.</a> The metric parameter here divided the music into eighth-note pulses. While the implementation is explained in more detail in the supplement, the essential problem is that of constraining all the possible ways one can divide a musical surface into chords. (Earlier implementations of the analysis model used no such segregation, and their precision suffered from this complication.) Dividing a musical surface into a metrical grid (instead of just salami slices, for instance) constrains the possible ways a surface can be segmented: in my implementation, pitches that appear on pulses that are more metrically emphasized are first considered in the reduction method, and the less emphasized pulses are reduced to the surrounding stronger pulses. For this, I use the meter-finding algorithm of Zikanov (<a href="#zikanov_2014" id="citation_zikanov_2014_67dc99bb14a63">2014</a>), and the pulse closest to the eighth note was selected as the metric division. This level was chosen because it excludes pulses at which harmonies almost never change but places it conservatively below the levels at which harmonies tend to change: the corpus analysis of Temperley (<a href="#temperley_2009b" id="citation_temperley_2009b_67dc99bb14a67">2009b</a>) found that harmonies change on the notated &#8220;beat&#8221; (the quarter note in <nobr><math><msubsup><mtext></mtext><mn>4</mn><mn>4</mn></msubsup></math></nobr>) 22% of the time, the next broader (longer) pulse 71% of the time, and the next lower (shorter) pulse only 2% of the time.<br><a href="#FN33REF">Return to text</a></p><p><a name="FN34">34.</a> Tymoczko separates his models by mode; since the current model operates by agglomerating both modes, I combine his two datasets into one. Again, more details can be found in the technical report.<br><a href="#FN34REF">Return to text</a></p><p><a name="FN35">35.</a> Two other peculiarities should be noted. The corpus also includes chords with no discernible root (often augmented 6th chords and passing chords) within the corpus: these are removed. The corpus designates chords by their root shown as chromatic scale degrees, such that chords on the minor mediant and on the raised supertonic would both be represented as having the same chromatic root.<br><a href="#FN35REF">Return to text</a></p><p><a name="FN36">36.</a> The 2-grams that occurred less than 3 times in the corpus were discarded to simplify the model (following <a href="#quinn_and_mavromatis_2011" id="citation_quinn_and_mavromatis_2011_67dc99bb14a6d">Quinn and Mavromatis 2011</a>).<br><a href="#FN36REF">Return to text</a></p><p><a name="FN37">37.</a> Given that they represent a vastly different musical style, selecting popular music corpora may seem unusual; however, their difference will provide a useful guidepost for assessing other models. That is, if a supposedly common-practice model acts similarly to a popular-music corpus, this would call that model into question.<br><a href="#FN37REF">Return to text</a></p><p><a name="FN38">38.</a> Since many corpora had somewhat different definitions of a &#8220;chord,&#8221; the most frequent chord in the assessing corpus was identified with each of the Kostka-Payne chord roots. For instance, when the Quinn-YCAC was used for the cross-entropy measurements, its most frequent chord with a dominant root, V7, was mapped onto each of the Kostka-Payne <span style='font-size:0.8em'><math><mover><mn>5</mn><mo>&#710;</mo></mover></math></span> annotations; for the Tymoczko-Bach, V was used, and so on.<br><a href="#FN38REF">Return to text</a></p><p><a name="FN39">39.</a> Applying the logarithm here rescales the unworkably small numbers that would result from taking the product of so many successive probabilities.<br><a href="#FN39REF">Return to text</a></p><p><a name="FN40">40.</a> In other words, if <i>m</i>(o<sub><i>n</i></sub>) = 0 at some point n in the sequence, the probability of the whole sequence would be zero.<br><a href="#FN40REF">Return to text</a></p><p><a name="FN41">41.</a> This was done using the Laplace smoothing method, with an additive factor of 1 (<a href="#jurafsky_and_martin_2000" id="citation_jurafsky_and_martin_2000_67dc99bb14a72">Jurafsky and Martin 2000</a>).<br><a href="#FN41REF">Return to text</a></p><p><a name="FN42">42.</a> Note that these four values together provide a multifaceted depiction of a model&#8217;s similarity to the observed analyses. Two low cross-entropy values with a low exclusion percentage would indicate that the model successfully predicts the observed chord progressions using parameters that overlap with those of the observation stream, while a low without-smoothing cross entropy coupled with a high smoothed value indicates that the model predicts a fraction of the observed progressions very well but cannot account for a large portion of them. Finally, the number of vocabulary items will help capture each model&#8217;s relative complexity: a low smoothed cross entropy derived from a model with a large vocabulary is unsurprising, given that a complex model would be expected to account for any number of surface progressions. On the other hand, a low smoothed cross entropy from a smaller model would indicate an economy within its vocabulary, with its small vocabulary accounting for a large portion of the observed chord progressions.<br><a href="#FN42REF">Return to text</a></p><p><a name="FN43">43.</a> As before, the excerpt was metrically analyzed, and the pulse closest to the notated eighth note was chosen to be the broadest metric division.<br><a href="#FN43REF">Return to text</a></p><p><a name="FN44">44.</a> Given that the metric division chosen (the eighth note) is shorter than most harmonic rhythms, this extension happened at most timepoints: the process produced windows with an average of 11.4 metric slices.<br><a href="#FN44REF">Return to text</a></p><p><a name="FN45">45.</a> To be sure there were no unusual aberrations in data (e.g., graders performing vastly differently from one another), an Analysis of Variance (ANOVA) statistical test was run between the three primary factors in this model (the grader, the example&#8217;s characteristic, and whether the analysis had been produced by a human or computer) and the grades given to the examples, but the results were not significant (<i>F</i>(1, 25) = .123, <i>MSE</i> = .615, <i>p</i> = .729). There also was no interaction between the grade and the computer/human producer (<i>F</i>(1, 25) = .265, <i>MSE</i> = 1.324,  <i>p</i> =.611).<br><a href="#FN45REF">Return to text</a></p><p><a name="FN46">46.</a> In particular, the bars show the standard error given the number of responses and variation in those responses, or how different the averages must be in order for those differences to be only 5% (or less) likely to have arisen by chance.<br><a href="#FN46REF">Return to text</a></p><p><a name="FN47">47.</a> Using a two-sided t-test, the results are nearly significant (<i>p</i> =. 07).<br><a href="#FN47REF">Return to text</a></p><p><a name="FN48">48.</a> The one significant effect was the interaction between grade and the example&#8217;s tagged characteristic (<i>F</i>(3, 25) = 3.053, <i>MSE</i> = 15.27, <i>p</i> = .047), due to the fact that &#8220;simple&#8221; grades were significantly higher than others.<br><a href="#FN48REF">Return to text</a></p><p><a name="FN49">49.</a> Turing (<a href="#turing_1950" id="citation_turing_1950_67dc99bb14a76">1950</a>) proposes this percentage to be 70% of the time. He imagines this &#8220;imitation game&#8221; as a proxy to begin to ask whether computers can think in the same way a human can. Importantly, even though this was a motivating question to this test, it is not my own: here, I am simply interested in the computer&#8217;s ability to approximate a particular human behavior.<br><a href="#FN49REF">Return to text</a></p><p><a name="FN50">50.</a> Dartmouth Neukom Institute&#8217;s even hosts a musical Turing Test competition. <a href='http://bregman.dartmouth.edu/turingtests/'>http://bregman.dartmouth.edu/turingtests/</a><br><a href="#FN50REF">Return to text</a></p><p><a name="FN51">51.</a> Imagine that if you flipped a coin 20 times, it is most likely to come up heads 10 times and tails 10 times; however, if you flipped heads 11 or 12 times, you wouldn&#8217;t believe your coin is rigged.  However, if it came up heads, say, 18 times, you would be almost certain the coin flip was not operating with a 50/50 probability of heads and tails. The binomial test works in exactly this way. If the computer and humans were indistinguishable, the probability of either being chosen would be 50/50; if they were distinguishable, the results would act like the rigged coin, and skew in one direction.<br><a href="#FN51REF">Return to text</a></p><p><a name="FN52">52.</a> <i>p</i> = .0384<br><a href="#FN52REF">Return to text</a></p><p><a name="FN53">53.</a> Another aspect of this idea is its connection to &#8220;statistical learning.&#8221; By deriving its vocabulary and syntax from a machine learning procedure, this work not only suggests the structure of a cognitive task, but how that task is learned via musical exposure.<br><a href="#FN53REF">Return to text</a></p><p><a name="FN54">54.</a> I want to highlight that the &#8220;process&#8221; aspect of this concept shifts the definitional location of &#8220;tonality&#8221; from a formal model to a human cognitive process. For instance, the analysis of Example 3 is &#8220;correct&#8221; insomuch as it is well-formed in terms of a formal model; however, it does not conform to some human process, and was therefore revised to incorporate meter into the model.<br><a href="#FN54REF">Return to text</a></p><p><a name="FN55">55.</a> The fact of the matter is that feedforward methods are very successful predictors of human key-finding behavior and underpin many compelling psychological and computational studies. Given that the above results do suggest the validity of a feedback-based approach, there are four logical explanations: 1) that feedforward theories are correct, and the current study&#8217;s results are aberrations and feedback-based theories are incorrect; 2) the success of feedforward approaches are statistical aberrations, arising by chance; 3) the success of feedforward theories captures their partial participation within an feedback approach &#8211; they model a cog within the greater feedback machine; or, 4) Both feedforward and feedback logics are used, but in different situations. Given the consistent success of key-profile-based psychological studies and the analytical usefulness of several feedforward theories, the second solution seems unlikely. And while the first option is a statistical possibility, given the dearth of quantifiable work associated with it, the third and fourth solutions seem to provide the most compelling and musically interesting option. Under the former, feedforward theories adequately model musical phenomena because they capture a subset of the concept of key. For instance, in terms of key profiles, tonic, mediant, and dominant scales degrees are the most frequent because tonic triads are the most frequent vocabulary item to which other vocabulary items progress. To the latter, the way people hear &#8220;key&#8221; may potentially modulate depending upon the particular situation: reading a key signature versus listening to a chord progression versus hearing an unaccompanied melody might all encourage different key-finding strategies.<br><a href="#FN55REF">Return to text</a></p><div id="fndiv1" class="flyoverdiv">In this article, I will adopt Temperley&#8217;s understanding of &#8220;key&#8221; as the perception, cognition, or identification of some tonal center. Of course, the terms &#8220;tonality&#8221; and &#8220;key&#8221; have many different subtle definitions, depending upon the repertoire being considered; and, even in contemporary work in music cognition, &#8220;tonality&#8221; can be defined in a wide variety of ways. Vos (<a href="#vos_2000" id="citation_vos_2000_67dc99bb1494d">2000</a>), for instance, argues that &#8220;tonality&#8221; can mean so many different things&#8212;from the simple perception of some privileged pitch class to a complex hierarchy of tones and chords&#8212;that the word is at risk of losing any definitive meaning. It is outside the bounds of this article to parse out all the different meanings associated with that concept; however, every theory that makes some kind of claim about the mechanics behind tonal orientation does so using either a feedforward or feedback logic. I do, however, return to some definitional aspects of &#8220;key&#8221; at the end of this essay.</div><div id="fndiv2" class="flyoverdiv">Furthermore, surface or perceived musical events are <i>actually observed</i> within whatever musical medium is being used: a rhythm is <i>seen</i> in a score and a pitch is <i>heard</i> in a performance. On the other hand structures or cognized musical events are the result of some interpreted act: rhythms <i>interpreted</i> in some meter and pitches are <i>interpreted</i> in some key.</div><div id="fndiv3" class="flyoverdiv">This discussion conflates the sound signal with the score as well as the &#8220;in-the-moment&#8221; understanding of a key with a self-reflective/analytical understanding of key. I will discuss how visual and aural modes (and momentary and reflective processes) of music perception function in my argument below. However it should also be noted that the aural/visual domains and the listening/analytical acts are usefully and playfully intertwined in much music theory discourse (Lewin 1987), and using explicit and implicit tasks in key-finding behavioral experiments produces generally the same outcomes (<a href="#aarden_2003" id="citation_aarden_2003_67dc99bb14956">Aarden 2003</a>).</div><div id="fndiv4" class="flyoverdiv">Clearly, the feedforward/feedback dichotomy overlaps with other classic dichotomies, particularly bottom-up/top-down and connectionist/rule-based approaches; however, these mappings are by no means one-to-one. For instance, just as top-down and rule-based approaches use some pre-formed set of expectations or templates to make sense of some phenomena, so too do feedback systems use the organizations of broader structures to inform key finding. But a feedforward approach could also use some broader structure to identify a key if that structure was not, in turn, affected by the key center. In other words, these more classic dichotomies hinge upon the relationship between a complex phenomenon and the evidence used to support this phenomenon, while my dichotomy hinges upon whether there is a relationship between more than one type of high-level organization.</div><div id="fndiv5" class="flyoverdiv">For the computationally minded: I implemented this key-profile analysis with Bellmann-Budge weightings and a correlation procedure implemented in music 21 (<a href="#cuthbert_and_ariza_2011" id="citation_cuthbert_and_ariza_2011_67dc99bb1495b">Cuthbert and Ariza 2011</a>). The procedure uses the number of quarter-note durations of each pitch-class as a 12-member vector and then calculates the correlation coefficient between that and the ideal vector of each of the 24 major and minor keys. The annotations for the first two measures use that two-measure window (mm. 5&ndash;6), and measure 7&#8217;s annotations use a three-measure window (mm. 5&ndash;7).</div><div id="fndiv6" class="flyoverdiv">These multiple solutions are possible because both models assign probabilities for each key: in this instance, the various probabilities of the different key assessments are comparable. Specifically, &#8220;probabilistic&#8221; understandings of musical keys allow for multiple key centers to vie for prominence within a passage (<a href="#temperley_2007" id="citation_temperley_2007_67dc99bb1495f">Temperley 2007</a>). We might say, for instance, that when we hear or analyze a passage unambiguously in C major, we might be 95% sure of a C tonal center, with the remaining 5% allowing for an impending surprisingly modulation. Tonally ambiguous passages, then, might have the probability mass comparably divided amongst several keys.</div><div id="fndiv7" class="flyoverdiv">The chord-progression model ignores the G&#x266d; of measure 6 to find a V7&ndash;i progression in F, and it ignores the E&#x266e; of measure 5 to construct a ii&ndash;V7&ndash;i progression in D flat.</div><div id="fndiv8" class="flyoverdiv">Computational formalization often overlaps with behavioral testing, in that the former can be designed to model or mimic a cognitive process or predict human behavior. See Temperley (2012) for an overview of computational modeling of music cognition.</div><div id="fndiv9" class="flyoverdiv">For a full account of the rich history of computational implementations for music analysis, see <a href="#temperley_2007" id="citation_temperley_2007_67dc99bb14965">Temperley 2007</a> or <a href="#white_2013a" id="citation_white_2013a_67dc99bb14968">White 2013a</a>.</div><div id="fndiv10" class="flyoverdiv">Generally, these profiles show tonic pcs occurring most frequently, followed by the other members of the tonic triad, other members of the diatonic set, and finally the chromatic pitches. Such a model can then match a passage&#8217;s pitch-class distribution to the ideal profile of one of the 24 major and minor keys, thereby assigning the best-fitting key interpretation to the vector that best correlates to the observed distribution of a musical surface.  This sort of analysis arose from &#8220;probe-tone studies&#8221; in which participants were asked to rate how well various pitch classes fit into a tonal context. (See, for instance, <a href="#krumhansl_and_shepard_1979" id="citation_krumhansl_and_shepard_1979_67dc99bb1496d">Krumhansl and Shepard 1979</a>, <a href="#castellano_bharucha_and_krumhansl_1984" id="citation_castellano_bharucha_and_krumhansl_1984_67dc99bb14971">Castellano, Bharucha, and Krumhansl 1984</a>, <a href="#oram_and_cuddy_1995" id="citation_oram_and_cuddy_1995_67dc99bb14975">Oram and Cuddy 1995</a>, <a href="#creel_and_newport_2002" id="citation_creel_and_newport_2002_67dc99bb14979">Creel and Newport 2002</a>, <a href="#smith_and_schmuckler_2004" id="citation_smith_and_schmuckler_2004_67dc99bb1497c">Smith and Schmuckler 2004</a>, and <a href="#huron_2006" id="citation_huron_2006_67dc99bb14980">Huron 2006</a>). In her collaboration with Mark Schmuckler, Krumhansl (<a href="#krumhansl_1990" id="citation_krumhansl_1990_67dc99bb14984">1990</a>) noted that the relative distribution of scale-degrees within tonal music seemed to mostly conform to the goodness-of-fit ratings found in probe-tone studies, and that this correspondence could be used in automated key-finding models.</div><div id="fndiv11" class="flyoverdiv">We would imagine that identifying a tritone as a particular pitch-class spelling (i.e., a diminished fifth versus an augmented fourth) to be specifically a visual cue; however, this is not always the case. Tuning and surrounding pitch cues can assist in the aural interpretation; several of the above-cited studies also suggest that presenting a tritone in ascending versus descending patterns contributes to how it is interpreted.</div><div id="fndiv12" class="flyoverdiv">Metrical Preference Rule 9 states, &#8220;Prefer a metrical analysis that minimizes conflict with the [harmonic] time-span reduction&#8221; (<a href="#lerdahl_and_jackendoff_1983" id="citation_lerdahl_and_jackendoff_1983_67dc99bb14988">90</a>), and Time Span Reduction Preference Rule 1 states, &#8220;Of the possible choices for head of a time­span T, prefer a choice that is in a relatively strong metrical position.&#8221; (<a href="#lerdahl_and_jackendoff_1983" id="citation_lerdahl_and_jackendoff_1983_67dc99bb1498c">160</a>).</div><div id="fndiv13" class="flyoverdiv">See <a href="#dineen_2005" id="citation_dineen_2005_67dc99bb14990">Dineen 2005</a> for a review of Schoenberg&#8217;s tonal theories.</div><div id="fndiv14" class="flyoverdiv">It is worth reflecting on the omnipresence of schemata in theories of key and how they underpin feedforward and feedback models. Scales or key profiles, for instance, are learned ways to organize pitches in much the same way cadential gestures are. Energetic relationships between scale degrees are as much preformed schematic templates as Gjerdingen&#8217;s (<a href="#gjerdingen_2007" id="citation_gjerdingen_2007_67dc99bb14994">2007</a>) <i>partimenti</i> patterns. Even interpreting a pitch involves a (albeit subconscious) process of parsing and grouping variegated sound stimuli into a single apperception (<a href="#plomp_1964" id="citation_plomp_1964_67dc99bb14997">Plomp 1964</a>, <a href="#moore_2012" id="citation_moore_2012_67dc99bb1499a">Moore 2012</a>). More sophisticatedly, recognizing that a G&#x266f;-D tritone might indicate an A tonal center involves recognizing that the diminished fifth participates in some schema (i.e., as part of a major scale, as invoking an expectation of a particular resolution, etc.). Clearly, schemata underpin both types of key-finding models; the difference between feedforward and feedback approaches turns on whether the key is used as evidence for the schematic structures. Claims like, &#8220;all white notes are present, and therefore a passage is in C major&#8221; are different from claims like, &#8220;This chord progression makes the most sense in C major.&#8221; In the former, the claim arises from a stable phenomenon: a perception like &#8220;these are all white notes&#8221; would never be revised if you rethought the passage&#8217;s key. In contrast, a chord progression can be drastically revised if the tonal orientation is rethought.</div><div id="fndiv15" class="flyoverdiv">While I will engage with this concept in more depth below, it is important to note that the differences between these models hinge upon the different definitions of tonality these authors propose.</div><div id="fndiv16" class="flyoverdiv">Laitz, for instance, recommends his readers identify modulations when chord-progression analyses &#8220;become more complex or are nonsense&#8221; in relation to the prior tonic (<a href="#laitz_2008" id="citation_laitz_2008_67dc99bb149a0">2008</a>, 572). Clendinning and Marvin (<a href="#clendinning_and_marvin_2011" id="citation_clendinning_and_marvin_2011_67dc99bb149a4">2011</a>) instruct their readers to use cadences to identify keys, yet their instructions on cadence identification assume a student has already identified the underlying key.</div><div id="fndiv17" class="flyoverdiv">It should be noted that a theorist&#8217;s tonal logics need not exclusively be either feedback or feedforward. Clendinning and Marvin (<a href="#clendinning_and_marvin_2011" id="citation_clendinning_and_marvin_2011_67dc99bb149a9">2011</a>) first introduce key as connected to diatonic scales, then nuance their discussion later to include beginning and ending notes, and finally to include cadence identification. Piston (<a href="#piston_1941" id="citation_piston_1941_67dc99bb149ac">1941</a>) distinguishes between tonality and modality by saying, &#8220;Tonality is the organized relationship of tones in music. . . . Modality refers to the choice of the tones between which these relationships exist. Tonality is synonymous with key, modality with scale. . . [Any of the] modes may be transposed into all tonalities, simply by changing the pitch of the tonic note and preserving the interval relationships&#8221; (30&#8211;31). When describing how tonal relationships are obtained, he writes, &#8220;The strongest tonal factor in music is the dominant effect. Standing alone, it determines the key much more decisively than the tonic chord itself. . .&#8221; (34) This passage suggests that identifying a dominant chord yields a tonal center, something that would place the theory on the distributional side (as one simply asks whether there is a dominant harmony in the distribution of chords), and would use a feedback logic (since knowing a key would help identify a &#8220;dominant effect&#8221;). Piston, however, goes on to write, &#8220;The greatest strength of tonality in harmonic progressions involving only triads lies in those progressions which combine dominant harmony with harmony from the subdominant side. . . The progressions IV-V and II-V cannot be interpreted in more than one tonality, without chromatic alteration.&#8221; (36) The theory is not a linear one, and therefore is structural. However, given that Piston&#8217;s logic seems to be that the pitch content of these successions immediately point to a single diatonic set, the logic is now feedforward: identifying two triads linearly related by T5 or T2 places you in a single key.</div><div id="fndiv18" class="flyoverdiv">Indeed, Raphael and Stoddard&#8217;s (<a href="#raphael_and_stoddard_2003" id="citation_raphael_and_stoddard_2003_67dc99bb149b1">2003</a>, <a href="#raphael_and_stoddard_2004" id="citation_raphael_and_stoddard_2004_67dc99bb149b5">2004</a>) implementations of an Expectation Maximization procedure for their study heavily influenced my own implementation.</div><div id="fndiv19" class="flyoverdiv">Vos and Van Geenen&#8217;s (<a href="#vos_and_van_geenen_1996" id="citation_vos_and_van_geenen_1996_67dc99bb149ba">1996</a>) algorithm also assigns key by aligning a melody to a scale collection and to chords membership in parallel in a way very reminiscent of the current article; however, their algorithm is designed specifically for monophonic contexts.</div><div id="fndiv20" class="flyoverdiv">The literature within music information retrieval and artificial musical intelligence is also replete with so-called &#8220;feedback&#8221; systems. In fact, the current emphasis within this literature on Deep Learning (especially Recurrent Neural Networks) is heavily reliant on this logic, be it for harmonization (e.g., <a href="#feulner_1993" id="citation_feulner_1993_67dc99bb149bd">Feulner 1993</a>, <a href="#gang_lehman_and_wagner_1998" id="citation_gang_lehman_and_wagner_1998_67dc99bb149c1">Gang, Lehman, and Wagner 1998</a>, <a href="#de_prisco_et_al_2010" id="citation_de_prisco_et_al_2010_67dc99bb149c5">De Prisco et al. 2010</a>), pattern completion (<a href="#liu_and_randall_2016" id="citation_liu_and_randall_2016_67dc99bb149c9">Liu and Randall 2016</a>), melodic composition (<a href="#colombo_et_al_2016" id="citation_colombo_et_al_2016_67dc99bb149cd">Colombo et al. 2016</a>), or even the analysis of musical characteristics (<a href="#eyben_et_al_2010" id="citation_eyben_et_al_2010_67dc99bb149d1">Eyben et al. 2010</a>, <a href="#boulanger-lewandowski_bengio_and_vincent_2013" id="citation_boulanger-lewandowski_bengio_and_vincent_2013_67dc99bb149d5">Boulanger-Lewandowski, Bengio, and Vincent 2013</a>). In these models&#8217; engineering, many different parameters are maximized concomitantly: key might be dependent upon meter, which is dependent on the harmonic structure, which is dependent on key, and so on. This literature is important because it is useful: this kind of modeling underpins technologies like Spotify, Google Music, algorithmic DJs, and the like. However, these models&#8217; complexity and their emphasis on usefulness/production make it difficult to dissect and test their connection to music cognition and music theory. A notable exception is Cancino-Chacon, Grachten, and Agres (<a href="#cancino-chacon_grachten_and_agres_2017" id="citation_cancino-chacon_grachten_and_agres_2017_67dc99bb149da">2017</a>), which uses an RNN to simulate actual listeners&#8217; expectations by training their model on audio recordings of tonal music.</div><div id="fndiv21" class="flyoverdiv">Even more generally, we could consider any cognitive study of tonal harmony to implicitly involve key information.</div><div id="fndiv22" class="flyoverdiv">In fact <a href="#vos_and_van_geenen_1996" id="citation_vos_and_van_geenen_1996_67dc99bb149de">Vos and Van Geenen 1996</a> explicitly put their finger on this issue (see their discussion on pp. 207&#8211;8, for instance). Alternately, from a modeling perspective, Lerdahl and Jackendoff&#8217;s (<a href="#lerdahl_and_jackendoff_1983" id="citation_lerdahl_and_jackendoff_1983_67dc99bb149e2">1983</a>) <i>Generative Theory of Tonal Music</i> attempts a rigorous formal theory of tonal music via a generative grammar accompanied by preference rules. Several empirical tests of certain aspects of this theory have been performed (for instance, <a href="#lerdahl_and_krumhansl_2007" id="citation_lerdahl_and_krumhansl_2007_67dc99bb149e6">Lerdahl and Krumhansl 2007</a>) as well as attempts to computationally implement the theory (<a href="#temperley_and_sleator_1999" id="citation_temperley_and_sleator_1999_67dc99bb149e9">Temperley and Sleator 1999</a>). While these studies remain provocative, the learning of these preference rules and the particulars of the integration of various domains remain to be accounted for.<br><br>Given that the experimental designs are traditionally feedforward in their logic, the lack of empirical tests on feedback models is not surprising. Experimental designs generally test a parameter&#8217;s effect on some outcome by varying that parameter and testing the resulting changes in outcome: different outcomes are generated (unidirectionally) by different parameters&#8217; settings. Consider, for instance, the straightforward parallels between a key-profile model and an experiment that tests the effects of pitch-class distribution on key assessment. Changing the pitch-class distribution modifies the model&#8217;s output, and such changes correspond to the hypothesized behavior of the participant. This crisp mapping of a theoretical model onto an experimental design likely has some influence on the popularity of these sorts of tonal models in music cognition research.</div><div id="fndiv23" class="flyoverdiv">There are two main benefits of computational analysis that I rely upon in this article: formal rigor and cognitive mimicry. To the first: in the words of Bo Alphonce,  &#8220;the computer unrelentingly demands theory&#8221; (<a href="#alphonce_1980" id="citation_alphonce_1980_67dc99bb149ee">1980</a>, 26) and in the words of John Rahn, &#8220;to explicate something is, ultimately, to formalize it, that is, to make it into a machine at whose metaphorically whirring and clicking parts we are happy to stare, and be enlightened&#8221; (<a href="#rahn_1980" id="citation_rahn_1980_67dc99bb149f2">1980</a>, 66).  Formalizing key-finding into computational algorithms then, challenges us to a specificity and precision that allows us to inspect and test our assumptions about some topic. Second, a computational model can potentially mimic some aspect of a cognitive process. As described by Temperley (<a href="#temperley_2012" id="citation_temperley_2012_67dc99bb149f5">2012</a>), if a program takes in the same data as a human, manipulates the data in the same way as a human, and produces the same output as a human, then claims of human/computer parallelism can be made. (This idea is provocatively framed by Wiggins, M&uuml;llensiefen, and Pearce&#8217;s  (<a href="#wiggins_m&uuml;llensiefen_and_pearce_2010" id="citation_wiggins_m&uuml;llensiefen_and_pearce_2010_67dc99bb149fa">2010</a>) and Wiggins&#8217;s (<a href="#wiggins_2012" id="citation_wiggins_2012_67dc99bb149fe">2012</a>) understanding of traditional music theory as an informal system that provides fodder for empirical testing.) Additionally, certain programming techniques can not only claim to undertake a process that mimics human cognition, but can also imitate a human&#8217;s learning of that process. Such &#8220;machine-learning&#8221; techniques imitate aspects of humans&#8217; &#8220;statistical learning,&#8221; or the idea that listeners can form expectations by being exposed to sequences (musical or otherwise) with certain statistical regularities. (For examples in music, see <a href="#saffran_et_al_1999" id="citation_saffran_et_al_1999_67dc99bb14a02">Saffran et al. 1999</a>; <a href="#creel_newport_and_aslin_2004" id="citation_creel_newport_and_aslin_2004_67dc99bb14a06">Creel, Newport, and Aslin 2004</a>; <a href="#saffran_et_al_2005" id="citation_saffran_et_al_2005_67dc99bb14a0a">Saffran et al. 2005</a>; <a href="#huron_2006" id="citation_huron_2006_67dc99bb14a0d">Huron 2006</a>; <a href="#pearce_mullensiefen_and_wiggins_2008" id="citation_pearce_mullensiefen_and_wiggins_2008_67dc99bb14a11">Pearce, Mullensiefen, and Wiggins 2008</a>; <a href="#loui_wessel_and_hudson_kam_2010" id="citation_loui_wessel_and_hudson_kam_2010_67dc99bb14a15">Loui, Wessel, and Hudson Kam 2010</a>; <a href="#loui_2012" id="citation_loui_2012_67dc99bb14a18">Loui 2012</a>; <a href="#pearce_et_al_2010" id="citation_pearce_et_al_2010_67dc99bb14a1c">Pearce et al. 2010</a>.) Being based on the statistical regularities of some dataset by their very nature, corpus-based models (or models based on large datasets) can map the ways in which exposure to a repertoire might allow listeners to form expectations or create some cognitive model. The engineering of such models, then, can make claims not only about a cognitive process, but also about the properties of a corpus that might allow humans to learn to perform that process. (More in-depth discussions of connections between musical statistics and learning can be found in Bharucha <a href="#bharucha_1987" id="citation_bharucha_1987_67dc99bb14a20">1987</a> and <a href="#bharucha_1991" id="citation_bharucha_1991_67dc99bb14a24">1991</a>, <a href="#temperley_2007" id="citation_temperley_2007_67dc99bb14a27">Temperley 2007</a>, <a href="#rohrmeier_and_cross_2008" id="citation_rohrmeier_and_cross_2008_67dc99bb14a2a">Rohrmeier and Cross 2008</a>, and <a href="#byros_2009" id="citation_byros_2009_67dc99bb14a2e">Byros 2009</a>.)</div><div id="fndiv24" class="flyoverdiv">There are certain limits to this computational methodology. First, using computer-readable formats to represent aspects of &#8220;human perception&#8221; conflates visual (score-based) and aural (listener-oriented) experiences and perceptions. This conflation can exclude important distinctions: for instance, the information gleaned from hearing a tritone can be very different than that gleaned from visually identifying an interval as a diminished fifth. Relatedly, the current study conflates the concept of <i>musical surface</i> with the raw elements of a computer-readable score. While I discuss my data formatting in more depth below, this generally entails something like a &#8220;piano roll&#8221; representation, with each note&#8217;s pitch, duration, timepoint, loudness, and instrumentation as being the essential elements. The present study aims to honestly embrace these conflations; it will remain for future work to manipulate the computational formatting to highlight different aspects of the visual/aural dichotomy and the musical surface. Third, just because a computer does something that adheres to human behavior does not mean the computer is undertaking the same processes as the human. While the input and output of a computational process can be somewhat rigorously tested in an experimental setting, understanding the process is somewhat more obscure: pinpointing the specifics of a cognitive process is difficult at best and impossible at worst. Computational modeling of cognitive or theoretical systems therefore is undertaken with the caveat that their most important claims&#8212;the mechanics of how they produce their output&#8212;are usually orthogonally testable experimentally against human behavior rather than directly. This orthogonal relationship between computer and cognitive engineering is a primary reason to frame my model as a &#8220;proof of concept&#8221; rather than conclusive evidence. Finally, what computational analysis gains in specificity it also gains in complexity. Each minute step in an action creates a to-do list that becomes quite long. Consider the prototypical frustration a seasoned musician might encounter when describing &#8220;key&#8221; to a non-musician: this exemplifies the complexity and minutiae of the computational parameterization of such a process. Using computational modeling for music analysis, then, can often seem ungainly, but only insomuch as it reflects the complexities of some theoretic, analytical, or cognitive process.</div><div id="fndiv25" class="flyoverdiv">Functionally, converting pcs to these mod-12 scale degrees transposes all pcs to the key of C, since in all keys the tonic pcs will be transposed to 0, the supertonic will become 2, etc. Importantly, while the natural environment for diatonic scale degrees is modulo 7 space, using mod-12 space 1) allows for chromatically altered scale degrees (e.g., V/V would be &lt;269&gt;, 2) uses the same space for the major and minor mode (a I chord would be &lt;047&gt; while a i chord would be &lt;037&gt;, and 3) uses the same size universe for scale degrees and pcs. This follows <a href="#white_2015" id="citation_white_2015_67dc99bb14a31">White 2015</a>.</div><div id="fndiv26" class="flyoverdiv">Note that these Roman numerals are stand-ins for scale-degree sets: the algorithm under consideration neither interprets nor outputs a &#8220;I&#8221; chord <i>per se</i>, but rather the set &lt;0, 4, 7&gt;. This is the same for letter names used in the text and examples, as the program only ever deals in numerical pitch classes. Also note that since the algorithm uses unordered sets, the Roman numerals are always represented here without inversion. The implications of this are discussed below. I am also imagining this toy as treating sevenths and triads identically (i.e., V7 has the same 2-gram properties as V), and for simplicity I have removed the iii chord from the original formulation in the Kostka-Payne textbook.</div><div id="fndiv27" class="flyoverdiv">Using informatics &#8220;<i>n</i>-gram&#8221; parlance in which <i>n</i> equals the number of chords in a sequence, these sequences would be 2-grams, since the arrows express the probability of two-chord successions. That is, the probability of a chord appearing within this syntax is contingent only on the chord that appears before it: this process therefore assigns probabilities using only successions of two chords.</div><div id="fndiv28" class="flyoverdiv">Following previous work in White (<a href="#white_2013b" id="citation_white_2013b_67dc99bb14a35">2013b</a>), the prototypes used here were produced by applying Equation 3 to the raw data of the Yale Classical Archives corpus (<a href="#white_and_quinn_2016b" id="citation_white_and_quinn_2016b_67dc99bb14a39">White and Quinn 2016b</a>). The corpus contains 14,051,144 <i>salami slices</i>, or sets of notes occurring each time a pitch is added or subtracted from the texture, from music across the Western-European common practice. The reduction process was allowed to iterate until no more changes were made, and any repeated chords that arose during the reduction process were conflated into a single chord event. Even though the corpus is noisy and error-laden (<a href="#shanahan_and_albrecht_2013" id="citation_shanahan_and_albrecht_2013_67dc99bb14a3c">Shanahan and Albrecht 2013</a>, <a href="#declercq_2016" id="citation_declercq_2016_67dc99bb14a40">deClercq 2016</a>), the corpus is quite large, and as such is ideal for a machine-learning process.<br><br>White&#8217;s (<a href="#white_2013b" id="citation_white_2013b_67dc99bb14a43">2013b</a>) process produces a distribution of chords that occurs most frequently in the YCAC corpus. Following Pardo and Birmingham (<a href="#pardo_and_birmingham_1999" id="citation_pardo_and_birmingham_1999_67dc99bb14a48">1999</a>), Bellmann (<a href="#bellman_2005" id="citation_bellman_2005_67dc99bb14a4b">2005</a>), and Quinn and Mavromatis (<a href="#quinn_and_mavromatis_2011" id="citation_quinn_and_mavromatis_2011_67dc99bb14a4f">2011</a>), I exclude the <i>long tail</i> of the distribution, or those chords that make up the majority of chord types, but the minority  of chord instances of the distribution. The top 22 chords account for 85.3% of all chord occurrences, with the remaining chords accounting for only 14.7%. These infrequent chords were removed from the series, and chord transitions were normalized. For the purposes of this paper, I exclude the non-tertian vocabulary members.</div><div id="fndiv29" class="flyoverdiv">Contrast this with the encircled pitch content ending the second window, {D, F&#x266f;, G&#x266f;, A, B}. While the example chooses the {G&#x266f;, B, D} subset as the salient subset, the passage also contains the subset {B, D, F&#x266f;}. However, choosing this subset would yield the chord progression D-b-E7-b, which&#8212;while <i>possible</i>&#8212;would be instantiated by scale-degree sets that occur less frequently in the model&#8217;s corpus-derived syntax (e.g., A:  IV-ii-V7-ii) than do those progressions that use the {G&#x266f;, B, D} subset (e.g., a subset of the previous beat&#8217;s V7).</div><div id="fndiv30" class="flyoverdiv">This computational analysis ignores certain musical parameters in order to focus on others. Readers will likely have punctuated the preceding discussion with such thoughts as, &#8220;surely the voicing of the Alberti bass encourages us to hear certain pitches as the salient chord tones!&#8221;, and &#8220;the metrical emphasis clearly favors some notes over others!&#8221; This study&#8217;s focus on certain organizational structures will necessarily exclude the inclusion of others. Musical characteristics such as the bass voice, and the presence of arpeggios, melodic gestures, and the like all help listeners and analysts distinguish chord tones from non-chord tones; however, these parameters are not considered in this study and will remain for future investigation.<br><br>Additionally, given the above-cited literature on the importance of meter to the key-finding task as well as the difficulties I encountered in early implementations of meter-blind implementations, I incorporated a metric parameter into this model. In particular, my early experimental models would often divide the pitch contents of a beat between the surrounding chords to create more probable &#8211; but unintuitive &#8211; chord sequences. Consider the Mozart example: by eliding the barline and considering the F&#x266f; a passing seventh, the algorithm can produce the more-probable I-IV-V7 progression in the first phrase; by grouping together the last three beats of the second phrase, the algorithm can read the progression simply as IV-V7, dispensing with the less-probable intermittent submediant chord. Therefore, just as Example 2 divides the surface into half-note durations, the reduction model is provided with some basic notion of the underlying meter in order to divide the surface into its constituent chords. This assumption is not without support in cognitive research: the relationship between meter and key is investigated in Prince, Thompson, and Schmuckler (<a href="#prince_thompson_and_schmuckler_2009" id="citation_prince_thompson_and_schmuckler_2009_67dc99bb14a57">2009</a>) and Prince and Schmuckler (<a href="#prince_and_schmuckler_2014" id="citation_prince_and_schmuckler_2014_67dc99bb14a5b">2014</a>).</div><div id="fndiv31" class="flyoverdiv">A more technical report on how I implement this engineering can be found in <a href="#white_2015" id="citation_white_2015_67dc99bb14a5f">White 2015</a>.</div><div id="fndiv32" class="flyoverdiv">The corpus contains 46 examples of Western European common-practice music; five pieces were removed due to difficulties converting them into digital representations usable by the methods described below. The removed pieces all used pulses that were problematic to quantize&#8212;I did not want the findings unduly influenced by meter, a musical parameter that this article does not address. The reason for the difficulties occurred when adding repeating decimals: a triplet converts to 0.33333 of a quarter note, three of which do not equal 1.0, but rather 0.999999. While this difficulty could certainly be overcome, it was decided to forgo those examples in the current analysis.</div><div id="fndiv33" class="flyoverdiv">The metric parameter here divided the music into eighth-note pulses. While the implementation is explained in more detail in the supplement, the essential problem is that of constraining all the possible ways one can divide a musical surface into chords. (Earlier implementations of the analysis model used no such segregation, and their precision suffered from this complication.) Dividing a musical surface into a metrical grid (instead of just salami slices, for instance) constrains the possible ways a surface can be segmented: in my implementation, pitches that appear on pulses that are more metrically emphasized are first considered in the reduction method, and the less emphasized pulses are reduced to the surrounding stronger pulses. For this, I use the meter-finding algorithm of Zikanov (<a href="#zikanov_2014" id="citation_zikanov_2014_67dc99bb14a63">2014</a>), and the pulse closest to the eighth note was selected as the metric division. This level was chosen because it excludes pulses at which harmonies almost never change but places it conservatively below the levels at which harmonies tend to change: the corpus analysis of Temperley (<a href="#temperley_2009b" id="citation_temperley_2009b_67dc99bb14a67">2009b</a>) found that harmonies change on the notated &#8220;beat&#8221; (the quarter note in 44) 22% of the time, the next broader (longer) pulse 71% of the time, and the next lower (shorter) pulse only 2% of the time.</div><div id="fndiv34" class="flyoverdiv">Tymoczko separates his models by mode; since the current model operates by agglomerating both modes, I combine his two datasets into one. Again, more details can be found in the technical report.</div><div id="fndiv35" class="flyoverdiv">Two other peculiarities should be noted. The corpus also includes chords with no discernible root (often augmented 6th chords and passing chords) within the corpus: these are removed. The corpus designates chords by their root shown as chromatic scale degrees, such that chords on the minor mediant and on the raised supertonic would both be represented as having the same chromatic root.</div><div id="fndiv36" class="flyoverdiv">The 2-grams that occurred less than 3 times in the corpus were discarded to simplify the model (following <a href="#quinn_and_mavromatis_2011" id="citation_quinn_and_mavromatis_2011_67dc99bb14a6d">Quinn and Mavromatis 2011</a>).</div><div id="fndiv37" class="flyoverdiv">Given that they represent a vastly different musical style, selecting popular music corpora may seem unusual; however, their difference will provide a useful guidepost for assessing other models. That is, if a supposedly common-practice model acts similarly to a popular-music corpus, this would call that model into question.</div><div id="fndiv38" class="flyoverdiv">Since many corpora had somewhat different definitions of a &#8220;chord,&#8221; the most frequent chord in the assessing corpus was identified with each of the Kostka-Payne chord roots. For instance, when the Quinn-YCAC was used for the cross-entropy measurements, its most frequent chord with a dominant root, V7, was mapped onto each of the Kostka-Payne 5&#710; annotations; for the Tymoczko-Bach, V was used, and so on.</div><div id="fndiv39" class="flyoverdiv">Applying the logarithm here rescales the unworkably small numbers that would result from taking the product of so many successive probabilities.</div><div id="fndiv40" class="flyoverdiv">In other words, if <i>m</i>(o<i>n</i>) = 0 at some point n in the sequence, the probability of the whole sequence would be zero.</div><div id="fndiv41" class="flyoverdiv">This was done using the Laplace smoothing method, with an additive factor of 1 (<a href="#jurafsky_and_martin_2000" id="citation_jurafsky_and_martin_2000_67dc99bb14a72">Jurafsky and Martin 2000</a>).</div><div id="fndiv42" class="flyoverdiv">Note that these four values together provide a multifaceted depiction of a model&#8217;s similarity to the observed analyses. Two low cross-entropy values with a low exclusion percentage would indicate that the model successfully predicts the observed chord progressions using parameters that overlap with those of the observation stream, while a low without-smoothing cross entropy coupled with a high smoothed value indicates that the model predicts a fraction of the observed progressions very well but cannot account for a large portion of them. Finally, the number of vocabulary items will help capture each model&#8217;s relative complexity: a low smoothed cross entropy derived from a model with a large vocabulary is unsurprising, given that a complex model would be expected to account for any number of surface progressions. On the other hand, a low smoothed cross entropy from a smaller model would indicate an economy within its vocabulary, with its small vocabulary accounting for a large portion of the observed chord progressions.</div><div id="fndiv43" class="flyoverdiv">As before, the excerpt was metrically analyzed, and the pulse closest to the notated eighth note was chosen to be the broadest metric division.</div><div id="fndiv44" class="flyoverdiv">Given that the metric division chosen (the eighth note) is shorter than most harmonic rhythms, this extension happened at most timepoints: the process produced windows with an average of 11.4 metric slices.</div><div id="fndiv45" class="flyoverdiv">To be sure there were no unusual aberrations in data (e.g., graders performing vastly differently from one another), an Analysis of Variance (ANOVA) statistical test was run between the three primary factors in this model (the grader, the example&#8217;s characteristic, and whether the analysis had been produced by a human or computer) and the grades given to the examples, but the results were not significant (<i>F</i>(1, 25) = .123, <i>MSE</i> = .615, <i>p</i> = .729). There also was no interaction between the grade and the computer/human producer (<i>F</i>(1, 25) = .265, <i>MSE</i> = 1.324,  <i>p</i> =.611).</div><div id="fndiv46" class="flyoverdiv">In particular, the bars show the standard error given the number of responses and variation in those responses, or how different the averages must be in order for those differences to be only 5% (or less) likely to have arisen by chance.</div><div id="fndiv47" class="flyoverdiv">Using a two-sided t-test, the results are nearly significant (<i>p</i> =. 07).</div><div id="fndiv48" class="flyoverdiv">The one significant effect was the interaction between grade and the example&#8217;s tagged characteristic (<i>F</i>(3, 25) = 3.053, <i>MSE</i> = 15.27, <i>p</i> = .047), due to the fact that &#8220;simple&#8221; grades were significantly higher than others.</div><div id="fndiv49" class="flyoverdiv">Turing (<a href="#turing_1950" id="citation_turing_1950_67dc99bb14a76">1950</a>) proposes this percentage to be 70% of the time. He imagines this &#8220;imitation game&#8221; as a proxy to begin to ask whether computers can think in the same way a human can. Importantly, even though this was a motivating question to this test, it is not my own: here, I am simply interested in the computer&#8217;s ability to approximate a particular human behavior.</div><div id="fndiv50" class="flyoverdiv">Dartmouth Neukom Institute&#8217;s even hosts a musical Turing Test competition. <a href='http://bregman.dartmouth.edu/turingtests/'>http://bregman.dartmouth.edu/turingtests/</a></div><div id="fndiv51" class="flyoverdiv">Imagine that if you flipped a coin 20 times, it is most likely to come up heads 10 times and tails 10 times; however, if you flipped heads 11 or 12 times, you wouldn&#8217;t believe your coin is rigged.  However, if it came up heads, say, 18 times, you would be almost certain the coin flip was not operating with a 50/50 probability of heads and tails. The binomial test works in exactly this way. If the computer and humans were indistinguishable, the probability of either being chosen would be 50/50; if they were distinguishable, the results would act like the rigged coin, and skew in one direction.</div><div id="fndiv52" class="flyoverdiv"><i>p</i> = .0384</div><div id="fndiv53" class="flyoverdiv">Another aspect of this idea is its connection to &#8220;statistical learning.&#8221; By deriving its vocabulary and syntax from a machine learning procedure, this work not only suggests the structure of a cognitive task, but how that task is learned via musical exposure.</div><div id="fndiv54" class="flyoverdiv">I want to highlight that the &#8220;process&#8221; aspect of this concept shifts the definitional location of &#8220;tonality&#8221; from a formal model to a human cognitive process. For instance, the analysis of Example 3 is &#8220;correct&#8221; insomuch as it is well-formed in terms of a formal model; however, it does not conform to some human process, and was therefore revised to incorporate meter into the model.</div><div id="fndiv55" class="flyoverdiv">The fact of the matter is that feedforward methods are very successful predictors of human key-finding behavior and underpin many compelling psychological and computational studies. Given that the above results do suggest the validity of a feedback-based approach, there are four logical explanations: 1) that feedforward theories are correct, and the current study&#8217;s results are aberrations and feedback-based theories are incorrect; 2) the success of feedforward approaches are statistical aberrations, arising by chance; 3) the success of feedforward theories captures their partial participation within an feedback approach &#8211; they model a cog within the greater feedback machine; or, 4) Both feedforward and feedback logics are used, but in different situations. Given the consistent success of key-profile-based psychological studies and the analytical usefulness of several feedforward theories, the second solution seems unlikely. And while the first option is a statistical possibility, given the dearth of quantifiable work associated with it, the third and fourth solutions seem to provide the most compelling and musically interesting option. Under the former, feedforward theories adequately model musical phenomena because they capture a subset of the concept of key. For instance, in terms of key profiles, tonic, mediant, and dominant scales degrees are the most frequent because tonic triads are the most frequent vocabulary item to which other vocabulary items progress. To the latter, the way people hear &#8220;key&#8221; may potentially modulate depending upon the particular situation: reading a key signature versus listening to a chord progression versus hearing an unaccompanied melody might all encourage different key-finding strategies.</div>
       
	<div style="height:24px;width:150px;background-color:#4c7381;float:left;text-align: center;vertical-align: middle;line-height: 24px;">
		&nbsp;&nbsp;&nbsp;
		<a style="color:white;" onmouseover="this.style.color='#0000ff';text-decoration:none" 
		onmouseout="this.style.color='white';" href="#Beginning">Return to beginning</a>
		&nbsp;&nbsp;&nbsp;
	</div><br><br>

	
<!-------------------------------- FOOTER -------------------------------------------->

    <hr>
<h3>Copyright Statement</h3>
<p><h4>Copyright &copy; 2018 by the Society for Music Theory. All rights reserved.</h4></p>
<p class="small">[1] Copyrights for individual items published in  <i>Music Theory Online</i> (<i>MTO</i>) 
are held by their authors. Items appearing in  <i>MTO</i> may be saved and stored in electronic or paper form, and may be shared among individuals for purposes of 
scholarly research or discussion, but may  <i>not</i>  be republished in any form, electronic or print, without prior, written permission from the author(s), and advance 
notification of the editors of  <i>MTO.</i></p>
<p class="small">[2] Any redistributed form of items published in  <i>MTO</i> must include the following information in a form appropriate to the medium in which the items are 
to appear: </p>
<blockquote>
<p class="small">This item appeared in  <i>Music Theory Online</i> in [VOLUME #, ISSUE #] on [DAY/MONTH/YEAR]. It was authored by [FULL NAME, EMAIL ADDRESS], with whose written 
permission it is reprinted here.</p>
</blockquote>
<p class="small">[3] Libraries may archive issues of  <i>MTO</i> in electronic or paper form for public access so long as each issue is stored in its entirety, and no access fee 
is charged. Exceptions to these requirements must be approved in writing by the editors of  <i>MTO,</i> who will act in accordance with the decisions of the Society 
for Music Theory. </p>
<p class="small">This document and all portions thereof are protected by U.S. and international copyright laws. Material contained herein may be copied and/or distributed for research 
purposes only. </p>
       
	<div style="height:24px;width:150px;background-color:#4c7381;float:left;text-align: center;vertical-align: middle;line-height: 24px;">
		&nbsp;&nbsp;&nbsp;
		<a style="color:white;" onmouseover="this.style.color='#0000ff';text-decoration:none" 
		onmouseout="this.style.color='white';" href="#Beginning">Return to beginning</a>
		&nbsp;&nbsp;&nbsp;
	</div><br><br>

	
    	

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 


<div style="width:55%;float:right"><a href="https://societymusictheory.org">
<img alt="SMT" longdesc="Society for Music Theory" src="https://mtosmt.org/gifs/smtlogo_black.png" width="180"></a></div>
	
<div>
<p style='font-size:1.1rem'>Prepared by Michael McClimon, Senior Editorial Assistant  


<br>
		
			<br>Number of visits:  

		10626		
	</p><br><br>
</i>		

</div>
</div>
</article>
</body>
</html>

