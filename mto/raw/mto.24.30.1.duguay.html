
 
 
 

<!-------------------------------- HEADER -------------------------------------------->

    
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> 
<head>

<title> MTO 30.1: Duguay, Response to Trevor de Clercq</title>

<link rel="SHORTCUT ICON" href="https://www.mtosmt.org/gifs/favicon.ico">
<link rel="stylesheet" href="https://www.mtosmt.org/scripts/colorbox.css">
<link rel=StyleSheet href="https://www.mtosmt.org/scripts/mto-tufte.css" type="text/css" media=all>
<link rel="stylesheet" href="//code.jquery.com/ui/1.11.4/themes/smoothness/jquery-ui.css">

<script src="https://www.google-analytics.com/urchin.js" type="text/javascript"></script>
<script type="text/javascript">_uacct = "UA-968147-1"; urchinTracker();</script>

<script type="text/javascript" src="https://www.mtosmt.org/scripts/expandingMenu.js"></script>
<script type="text/javascript" src="https://www.mtosmt.org/scripts/dropdownMenu.js"></script>
<!--<script language="JavaScript" type="text/javascript" src="https://www.mtosmt.org/scripts/AC_QuickTime.js"></script>-->
<!--<script type="text/javascript" src="https://www.mtosmt.org/scripts/examples.js"></script>-->
<script type="text/javascript" src="https://www.mtosmt.org/scripts/hover.js"></script>  
<script src="https://code.jquery.com/jquery-1.10.2.js"></script>
<script src="https://code.jquery.com/ui/1.11.4/jquery-ui.js"></script>
<script src="https://www.mtosmt.org/scripts/colorbox-master/jquery.colorbox.js"></script>
<script type="text/javascript" src="https://www.mtosmt.org/scripts/jQueryRotate.2.2.js"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
<script>
MathJax.Hub.Config({
    TeX: { noErrors: { disabled: true } }
});
</script>

  <script>
   $(function () {
      $(document).tooltip({
        position: { my: "center bottom-10", at: "center top", },
    content: function () {
              return $(this).prop('title');
          }
      });
  });
  </script>

  <style>
    .ui-tooltip {
      color: #3a3a3a;
      font: 300 14px/20px "Lato", "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
      max-width: 600px;
      box-shadow: 0 0 7px gray;
    }
    ol.mto-alpha {
        list-style: lower-alpha none outside;
    }
   ol.mto-alpha li {
       margin-bottom: 0.75em;
       margin-left: 2em;
       padding-left: 0.5em;
    }
  </style>

    <script language="Javascript">
        $(document).ready(function() {
            $(".mp3").colorbox({iframe:true, internalWidth:360, width:400, internalHeight:100, rel:'mp3', height:150, opacity:0.1, onComplete: function(e) {
                $('#colorbox').on({
                    mousedown: function(e){
                        if (~$.inArray(e.target, $('input, textarea, button, a, .no_drag', $('#colorbox')))) return;
                        var os = $('#colorbox').offset(),
                            dx = e.pageX-os.left, dy = e.pageY-os.top;
                        $(document).on('mousemove.drag', function(e){
                            $('#colorbox').offset({ top: e.pageY-dy, left: e.pageX-dx } );
                        });
                    },
                    mouseup: function(){ $(document).unbind('mousemove.drag'); }
                });
            }
        });
            $(".youtube").colorbox({iframe:true, innerWidth:640, innerHeight:390, opacity:0.1, rel:'youtube', onComplete: function(e) {
                $('#colorbox').on({
                    mousedown: function(e){
                        if (~$.inArray(e.target, $('input, textarea, button, a, .no_drag', $('#colorbox')))) return;
                        var os = $('#colorbox').offset(),
                            dx = e.pageX-os.left, dy = e.pageY-os.top;
                        $(document).on('mousemove.drag', function(e){
                            $('#colorbox').offset({ top: e.pageY-dy, left: e.pageX-dx } );
                        });
                    },
                    mouseup: function(){ $(document).unbind('mousemove.drag'); }
                });
            }
        });

      $("a[id^=footnote]").each(function(){
        var fnnum = $(this).attr('id').substring(8);
	var foot_me = '#fndiv'+fnnum;
        $("#footnote" + fnnum).attr('title', $(foot_me).html());

        });


        $("a[id^=citation]").each(function(){
         var separatorPos = $(this).attr('id').lastIndexOf('_');
         var linkid = $(this).attr('id');
         var citeref = $(this).attr('id').substring(8,separatorPos);
         var cite_me = '#citediv'+citeref;
         $("#" + linkid).attr('title', $(cite_me).html());

        });
    });

    </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-968147-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-968147-1');
</script>


<meta http-equiv="Content-Language" content="en-us">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
 

<meta name="citation_title" content="Response to Trevor de Clercq’s &ldquo;Some Proposed Enhancements to the Operationalization of Prominence: Commentary on Michèle Duguay’s &lsquo;Analyzing Vocal Placement in Recorded Virtual Space&rsquo;&rdquo;">

    <meta name="citation_author" content="Duguay, Michèle">
      
<meta name="citation_publication_date" content="2024/03/01">
<meta name="citation_journal_title" content="Music Theory Online">
<meta name="citation_volume" content="30">
<meta name="citation_issue" content="1">

</head>

<body>
<div class="bannertop">
	<a id="smt-link" alt="Society for Music Theory" href="https://www.societymusictheory.org">&nbsp;</a>
</div>
		
		<div style = "height:160px; width:900px; background-image: url('../../gifs/banner_blue_grey_900px.png'); background-repeat: no-repeat; background-position: 0px 0px"></div>
		
<!-------------------------------- MENU -------------------------------------------->

  
<div class="dropdown_menu">

<ul class="fullwidth" id="ddm">
    <li><a href="https://www.mtosmt.org/index.php">MTO Home</a>
    </li>
    <li><a href="https://www.mtosmt.org/issues/mto.24.30.4/toc.30.4.html">Current Issue</a>    </li>
    <li><a href="https://www.mtosmt.org/issues/issues.php"
    	onmouseover="mopen('m3')" 
        onmouseout="mclosetime()">Previous Issues</a>
        <div id="m3" 
            onmouseover="mcancelclosetime()" 
            onmouseout="mclosetime()">
	        <a href="https://www.mtosmt.org/docs/index-author.php">By Author</a>
	        <a href="https://www.mtosmt.org/issues/issues.php">By Volume&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
        </div>
	</li>
	
    <li><a href="https://www.mtosmt.org/docs/authors.html.php"
    	onmouseover="mopen('m4')" 
        onmouseout="mclosetime()">For Authors</a>
        <div id="m4" 
            onmouseover="mcancelclosetime()" 
            onmouseout="mclosetime()">
	        <a href="https://www.mtosmt.org/docs/mto-editorial-policy.html">MTO Editorial Policy</a>
	      <a href="https://www.mtosmt.org/docs/mto-style-guidelines.html">MTO Style Guidelines</a>
	      <a href="https://www.mtosmt.org/docs/how-to-submit-an-article-to-mto.html">How to Submit an Article</a>
	      <a href="https://www.mtosmt.org/ojs">Submit Article Online</a>
	      <a href="https://www.mtosmt.org/docs/reviewers.html">Book Review Guidelines</a>
        </div>
	</li>

 <!--   <li><a href="https://www.mtosmt.org/docs/authors.html">Submit</a>
	</li> -->
	
    <li><a href="https://www.mtosmt.org/mto-jobs.php"
    	onmouseover="mopen('m6')" 
        onmouseout="mclosetime()">Jobs</a>
        <div id="m6" 
            onmouseover="mcancelclosetime()" 
            onmouseout="mclosetime()">
	        <a href="https://www.mtosmt.org/mto-jobs.php">Current Job Listings</a>
	        <a href="https://www.mtosmt.org/mto-job-post.php">Submit Job Listing</a>
        </div>
	</li>
    <li><a href="https://www.mtosmt.org/docs/diss-index.php"
    	onmouseover="mopen('m7')" 
        onmouseout="mclosetime()">Dissertations</a>
        <div id="m7" 
            onmouseover="mcancelclosetime()" 
            onmouseout="mclosetime()">
	        <a href="https://www.mtosmt.org/docs/diss-index.php">All Dissertations</a>
	        <a href="https://www.mtosmt.org/docs/diss-index.php?new=true">New Dissertations</a>
	        <a href="https://www.mtosmt.org/mto-diss-post.php">List Your Dissertation</a>
        </div>
	</li>
    <li><a href="https://www.mtosmt.org/about.html">About</a>
	</li>
<!--    <li><a href="https://www.mtosmt.org/mto_links.html">Journals</a>  
	</li> -->
    <li><a href="https://societymusictheory.org">SMT</a>
	</li>
   <!-- <li><a href="https://societymusictheory.org/announcement/contest-new-mto-logo-2024-02"><span style="color:yellow">Logo Design Contest</span></a>
	</li>-->

</ul>

</div>


<!-------------------------------- TITLE -------------------------------------------->

  <article>

<div id="content">
<a name="Beginning"></a>
			
	<h1 style="width:900px; margin-top:1em">Response to Trevor de Clercq’s &ldquo;Some Proposed Enhancements to the Operationalization of Prominence: Commentary on Michèle Duguay’s &lsquo;Analyzing Vocal Placement in Recorded Virtual Space&rsquo;&rdquo;</h1>
	<div style="width:900px">
				</div>

	
				<h2><span style="font-weight: 400"><font size="5"><a style="color:black" href="#AUTHORNOTE1">Michèle Duguay</a></font></span></h2><br><br><p><font size='4'>KEYWORDS: Virtual Space, Voice, Prominence, Popular Music, Gender</font></p><p><small>DOI: 10.30535/mto.30.1.3</small></p>			
	<div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'><a href="https://mtosmt.org/issues/mto.24.30.1/mto.24.30.1.duguay.pdf">PDF text </a> | <a href="https://mtosmt.org/issues/mto.24.30.1/duguay_examples.pdf">PDF examples </a></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div><div style='width:800px'><div style='float:right; font-size:1.2rem;'></div></div>
			<div style="float:left; font-size:1.1rem;"><i>Received February 2024</i></div>
		<div style="width:850px">
	<div style="text-align:center; font-size: 1.1rem; margin-bottom:2em;margin-top:4em;margin-right:auto;margin-left:auto;width:870px">
		Volume 30, Number 1, March 2024 <br> Copyright &#0169; 2024 Society for Music Theory	</div>
	</div>

<hr style="width:850px"><br>
<section>
<!-------------------------------- ARTICLE BODY (begin) -------------------------------------->

<p>[1] My <a href="#duguay_2022" id="citation_duguay_2022_67dc99142fa50">2022</a> article, &ldquo;Analyzing Vocal Placement in Recorded Space,&rdquo; outlines five parameters that affect the positioning of a voice within a virtual space: width, environment, layering, pitch, and prominence. In it, I propose that analysts use the following method to determine the prominence of a vocal track:

  <ol><li>Calculate the average of RMS (Root Mean Square) amplitude values in the isolated vocal track, omitting values under 0.02 to remove moments of silence or near-silence in the vocal track.</li>
  <li>Calculate the average of RMS amplitude values in the full mix.</li>
  <li>Express the prominence of the vocals as a percentage with the following formula:</li></ol>

<span style="margin-left:10em">
<math>
  <mrow>
    <mtext>Prominence</mtext>
    <mo>=</mo>
    <mrow>
      <mo fence="true" form="prefix">(</mo>
      <mfrac>
        <mtext>Average RMS of isolated vocal track</mtext>
        <mtext>Average RMS of full track</mtext>
      </mfrac>
      <mo fence="true" form="postfix">)</mo>
    </mrow>
    <mo>×</mo>
    <mn>100</mn>
  </mrow>
</math></span>
</p>

<p>[2] In his response, Trevor de Clercq proposes an alternate approach for calculating prominence:

  <ol><li>Determine the integrated LUFS (Loudness Units relative to Full Scale) of the isolated vocal track. </li>
  <li>Generate a track of the full mix minus the vocals (i.e., an instrumental track) by reversing the polarity of the isolated vocal track and mixing it with the full track.<sup><a name="FN1REF" href="#FN1" id="footnote1">(1)</a></sup></li>
  <li>Determine the integrated LUFS of the instrumental track.</li>
  <li>Express the prominence  of the vocals in Loudness Units (LU) with the following formula: </li></ol>

<span style="margin-left:6em">

<math>
  <mi>P</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mtext>&nbsp;</mtext>
  <mo>=</mo>
  <mi>I</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi><mtext>&nbsp;</mtext>
  <mi>L</mi><mi>U</mi><mi>F</mi><mi>S</mi><mtext>&nbsp;</mtext>
  <mi>o</mi><mi>f</mi><mtext>&nbsp;</mtext>
  <mi>i</mi><mi>s</mi><mi>o</mi><mi>l</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi><mtext>&nbsp;</mtext>
  <mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>l</mi><mtext>&nbsp;</mtext>
  <mi>t</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>k</mi>
  <mo>-</mo>
  <mi>I</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi><mtext>&nbsp;</mtext>
  <mi>L</mi><mi>U</mi><mi>F</mi><mi>S</mi><mtext>&nbsp;</mtext>
  <mi>o</mi><mi>f</mi><mtext>&nbsp;</mtext>
  <mi>i</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>r</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>l</mi><mtext>&nbsp;</mtext>
  <mi>t</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>k</mi>
</math></span></p>

<p>[3] de Clercq’s approach departs from mine in two ways. First, de Clercq quantifies  prominence with LUFS because it better captures the perceptual experience of loudness than average RMS Amplitude. Second, his methodology relies on different types of tracks than mine to determine prominence. My approach compares the isolated vocal track to the full mix and expresses the prominence of the vocals as a percentage. The resulting value represents the proportion of the full track’s average RMS Amplitude that is occupied by the vocals. de Clercq’s compares the isolated vocal track with an instrumental track that contains no vocals and expresses the prominence of the vocals in LU. The resulting value represents the difference between the loudness of the vocals and the loudness of the rest of the mix. It is important to note that de Clercq’s suggested method slightly shifts the original definition of prominence by measuring the extent to which the vocals are louder or softer than the other combined remaining sound sources in the mix. His method also implicitly suggests that the x axis be graduated according to LU in visual representations of the virtual space.</p>

<p>[4] There are several advantages to de Clercq’s approach. His solution is more elegant, as he explains, in that it requires less manual computation than my method. Instead of manipulating a high amount of datapoints to calculate the average RMS amplitude of two tracks, an analyst simply needs to subtract two LUFS values. His workflow adheres to the intent of my methodology by relying on user-friendly tools that do not require extensive training in audio feature analysis.<sup><a name="FN2REF" href="#FN2" id="footnote2">(2)</a></sup> As de Clercq writes in paragraph 4.14, moreover, his method can be used to measure the prominence of other sound sources in a recording. This approach to prominence therefore enables more detailed ventures into the study of virtual space, potentially generating visual representations that encapsulate the placement of all sound sources in a mix.</p>

<p>[5] I will start by saying that I wholeheartedly welcome de Clercq’s attention to the perceptual experience of prominence. I agree that his approach provides a straightforward and intuitive means of quantifying the relative loudness of a voice against the rest of the mix. His contribution also carries the potential to inform further research into the perceptual components of virtual space. I can envision, for instance, an experiment assessing the extent to which listeners perceive minute differences in vocal prominence. Other studies could explore how one’s listening environment and audio equipment impact the perception of prominence and of the other parameters that constitute the virtual space. Such studies could be instrumental in the development of analytical tools that model the way listeners experience popular music.</p>

<p>[6] My response focuses on two points of discussion raised in de Clercq’s commentary. First, I contextualize his claims about the replicability of my method. de Clercq was unable to replicate my results because we used different isolated vocal tracks. For the same reason, I was also unable to replicate his prominence measurements. Ultimately, I show that these discrepancies are not caused by methodological flaws or by calculation errors, but rather by the imperfect state of source separation technologies. Second, I elaborate on the relationship between gender and vocal placement. de Clercq misrepresents my original argument by stating that I attribute differences in vocal placement to gender. The 2022 article made the opposite claim, stating that vocal placement <i>contributes</i> to the formation of a gendered soundscape in musical collaborations. I situate this approach within recent work on the topic, with the goal of generating further discussion on the relationship between gender and spatialization in recorded popular music.</p>

<h2>Replicability of Prominence Measurements</h2>
<p>[7] In paragraphs [2.11–2.13] of his response, de Clercq explains that he encountered some difficulties in replicating the prominence measurements provided in my article. The right column of his Example 2 shows the prominence values I originally calculated, while the column directly to its left displays the results that de Clercq obtained using my method. In eight out of nine cases, our prominence values differ by 6% or less. Rihanna’s first verse in &ldquo;Love The Way You Lie (Part II)&rdquo; is the sole exception. I calculated a prominence level of 92%, while de Clercq obtained a 74%.</p>

<p>[8] Given that we worked with the same methodology, he surmises that the discrepancies between our results may be due to variations in our workflow. We may have had mismatched time stamps or used different versions of software.<sup><a name="FN3REF" href="#FN3" id="footnote3">(3)</a></sup> He also proposes that a miscalculation on my end might be at the root of our mismatched results. &ldquo;Ultimately,&rdquo; he writes, &ldquo;it seems that the calculation of vocal prominence following the workflow outlined by Duguay may vary from user to user&rdquo; [2.13]. His methodology is intended to provide a more consistent alternative to the measurement of prominence.</p>

<p>[9] I would emphasize that the discrepancies between our results are not caused by a miscalculation nor by an issue with the original method. Rather, they arose because de Clercq and I were working from different audio files. de Clercq attempted to replicate my results with isolated vocal tracks generated by Open-Unmix, but my analysis relied on tracks created with iZotope RX7’s Music Rebalance (<a href="#duguay_2022" id="citation_duguay_2022_67dc99142fa69">Duguay 2022</a>, [3.8]). Since isolated vocal tracks produced by different software vary from one another, it is natural that our results would differ. My article show two isolated vocal tracks (<a href="https://mtosmt.org/issues/mto.22.28.4/mto.22.28.4.duguay.html#link">Audio Examples 4 and 5 in the original article</a>) generated from the same recording to illustrate the extent to which different software can generate dissimilar results. In Audio Example 4, which was created with iZotope RX7, non-vocal sound sources bleed into the separated vocal track. In Audio Example 5, which was created with Open-Unmix, the isolated track does not capture all the reverberated images of a voice. The source separation model instead categorizes these vocal components as instrumental parts. In the verse of &ldquo;Love the Way You Lie (Part II),&rdquo; Rihanna’s voice is set in a highly reverberant environment. It is likely that our different software simply categorized some of the reverberated images differently, therefore creating more variation in our prominence results.</p>

<fig>
 <p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 1</b></p><p class='fullwidth' style="text-align: center; margin-bottom:0em"><a class='youtube'  target="blank" href="duguay_examples.php?id=0&nonav=true"><img border="1" alt="Example 1 thumbnail" src="duguay_ex1_small.png"></a></p><p class='fullwidth' style="text-align: center; margin-top:0em"><font size="2">(click to enlarge)</font></p>  <p class='fullwidth' style="text-align: center; margin-top:0em; margin-bottom:0.5em"><b>Audio Example 1</b></p><p class='fullwidth' style="text-align: center; margin-top:0em;"><audio preload="metadata" controls style="width:300px"><source src="duguay_audio_ex1.mp3" type="audio/mpeg"><source src="duguay_audio_ex1.ogg" type="audio/ogg"><script language="JavaScript" type="text/javascript"></script></audio></p>  <p class='fullwidth' style="text-align: center; margin-top:0em; margin-bottom:0.5em"><b>Audio Example 2</b></p><p class='fullwidth' style="text-align: center; margin-top:0em;"><audio preload="metadata" controls style="width:300px"><source src="duguay_audio_ex2.mp3" type="audio/mpeg"><source src="duguay_audio_ex2.ogg" type="audio/ogg"><script language="JavaScript" type="text/javascript"></script></audio></p>  <p class='fullwidth' style="text-align: center; margin-top:0em; margin-bottom:0.5em"><b>Audio Example 3</b></p><p class='fullwidth' style="text-align: center; margin-top:0em;"><audio preload="metadata" controls style="width:300px"><source src="duguay_audio_ex3.mp3" type="audio/mpeg"><source src="duguay_audio_ex3.ogg" type="audio/ogg"><script language="JavaScript" type="text/javascript"></script></audio></p>  <p class='fullwidth' style="text-align: center; margin-top:0em; margin-bottom:0.5em"><b>Audio Example 4</b></p><p class='fullwidth' style="text-align: center; margin-top:0em;"><audio preload="metadata" controls style="width:300px"><source src="duguay_audio_ex4.mp3" type="audio/mpeg"><source src="duguay_audio_ex4.ogg" type="audio/ogg"><script language="JavaScript" type="text/javascript"></script></audio></p>  <p class='fullwidth' style="text-align: center; margin-top:0em; margin-bottom:0.5em"><b>Audio Example 5</b></p><p class='fullwidth' style="text-align: center; margin-top:0em;"><audio preload="metadata" controls style="width:300px"><source src="duguay_audio_ex5.mp3" type="audio/mpeg"><source src="duguay_audio_ex5.ogg" type="audio/ogg"><script language="JavaScript" type="text/javascript"></script></audio></p></fig>

<p>[10] It is true that de Clercq’s methodology requires only one calculation, which presents fewer opportunities for error. Importantly, this feature does not automatically guarantee replicable results between users who are working from different isolated vocal tracks. Using de Clercq’s method, I measured the prominence in LU of Rihanna’s vocals in the first chorus of &ldquo;Love the Way You Lie.&rdquo; With the aid of seven source separation models, I created different isolated vocal tracks of the same excerpt. The results are displayed in <b>Example 1</b>.<sup><a name="FN4REF" href="#FN4" id="footnote4">(4)</a></sup> Row 1 reproduces de Clercq’s results as reported in his Example 8. Rows 2 and 3 measure the prominence of the two isolated vocal tracks included in my original article. The remaining rows use vocal tracks obtained with the following models:

<ul>  <li>Open-Unmix 1.2.1’s default source separation model, umxl (<b>Audio Example 1</b>).</li>
  <li>Moises, version 1.0.49, &ldquo;2 tracks&rdquo; option (<b>Audio Example 2</b>). I measured the prominence of this vocal track twice by comparing it to 1) an instrumental track also created by Moises, and to 2) an instrumental track obtained through polarity reversal. </li>
  <li>Audionamix’s source separation application XTRAX STEMS, &ldquo;4 Stems&rdquo; option (<b>Audio Example 3</b>). </li>
  <li>MDX-Net Inst HQ3 (<b>Audio Example 4</b>)<sup><a name="FN5REF" href="#FN5" id="footnote5">(5)</a></sup></li>
<li>htdemucs v4 (<a href="#rouard_massa_and_défossez_2023" id="citation_rouard_massa_and_défossez_2023_67dc99142faf8">Rouard, Massa, and Défossez 2023</a>) (<b>Audio Example 5</b>)<sup><a name="FN6REF" href="#FN6" id="footnote6">(6)</a></sup></p></li>
</ul>

<p>[11] As Example 1 shows, de Clercq’s methodology yields varying results when applied to different isolated vocal tracks. Although I was able to replicate de Clercq&rsquo;s results by using a vocal track created by the most recent version of Open-Unmix (rows 1 and 4), the prominence values I obtained for other tracks, while roughly similar, all differ from one another. The lack of replicability observed in both of our methodologies is indicative of a broader issue in the analysis of vocal placement: the imperfect state of source separation technologies. Until models yield consistent, nearly identical results, analysts will be unable to obtain matching prominence results even if they use the same methodology. Fortunately, source separation software is evolving quickly and producing increasingly accurate isolated vocal tracks. For example, when I isolated the vocal tracks for my article, the 2019 initial release of Open-Unmix (which then offered state-of-the-art technology in source separation) still yielded imperfect results. When preparing this response, I was pleased to observe that the newly created isolated files were much more accurate than the ones presented in my article.<sup><a name="FN7REF" href="#FN7" id="footnote7">(7)</a></sup> We may soon reach a point where analysts can provide precise and accurate measurements of prominence with minimal variability from one isolated track to the next.<sup><a name="FN8REF" href="#FN8" id="footnote8">(8)</a></sup></p>

<fig>
 <p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 2</b></p><p class='fullwidth' style="text-align: center; margin-bottom:0em"><a class='youtube'  target="blank" href="duguay_examples.php?id=1&nonav=true"><img border="1" alt="Example 2 thumbnail" src="duguay_ex2_small.png"></a></p><p class='fullwidth' style="text-align: center; margin-top:0em"><font size="2">(click to enlarge)</font></p></fig>

<p>[12] In the interests of advancing this discussion toward further productive ends, I would suggest two potential solutions for ensuring more replicable prominence measurements. The first is to have analysts make their isolated vocal tracks publicly available to ensure replicability. The second is to work to acculturate analysts to expect and account for a reasonable level of variation from one analysis to the other. While the LU values in Example 1 exhibit variability, for instance, they all invite the same observation: Rihanna’s vocals in the first chorus of &ldquo;Love the Way You Lie&rdquo; are more prominent, by at least 6 LUs, than the instrumental track that accompanies her. To account for the variations in LUFS that arise when analyzing different isolated or instrumental tracks, analysts could rely on a generally agreed-upon system such as the five-point scale suggested in <b>Example 2</b>. According to this scale, for instance, all LU values in Example 1 belong to the &ldquo;Prominence +2&rdquo; category. The scale assumes that different isolated vocal tracks, while not identical, are similar enough to provide meaningful prominence results.<sup><a name="FN9REF" href="#FN9" id="footnote9">(9)</a></sup> In this sketch of a model, the thresholds between categories remain arbitrary. Obviously, much further work would be needed to determine stricter scale criteria to allow the model to better align with the perceptual experience of prominence.</p>

<h2>Vocal Placement and Gender</h2>

<p>[13] My second area of concern relates to de Clercq’s representation of my argument on the potential relationship between vocal placement and gender. My 2022 article demonstrates how Rihanna’s and Eminem’s recorded voices are starkly differentiated across their four collaborations: Rihanna’s voice tends to be positioned in a more reverberated, layered, and wide space than Eminem’s voice. As a result, his vocals sound relatively centered and stable in comparison with Rihanna’s more diffuse, mobile vocal placement. I suggest that &ldquo;these contrasting vocal placements perhaps contribute to the sonic construction of a gender binary, exemplifying one of the ways in which dichotomous conceptions of gender are reinforced through the sound of popular music [4.10].&rdquo; In his response, de Clercq writes that &ldquo;it is unclear <span style="white-space: nowrap;">. . .</span> to what extent the differences in vocal prominence that Duguay identifies can be attributed to gender (as she suggests in [1.7] and [4.10]) as opposed to other factors [5.3].&rdquo; He explains that other elements, such as musical form, may be the cause of variations in vocal placement.</p>

<p>[14] This summary misrepresents my argument: I do not suggest a causal relationship in which the gender of the artists directly impacts the way they are spatialized in a recording. I agree that many factors influence mixing practices in commercial popular music; these include genre, musical micro-trends, artists’ or producers’ preferences, personnel, vocal range and tessitura, formal conventions resulting in texturally dense choruses and thin verses, and many others. It would be misguided, in my opinion, to try and identify the &ldquo;cause&rdquo; of vocal placements without a detailed ethnography of mixing practices.</p>

<fig>
 <p class='fullwidth' style="text-align: center; margin-top:0em"><b>Example 3</b></p><p class='fullwidth' style="text-align: center; margin-bottom:0em"><a class='youtube'  target="blank" href="duguay_examples.php?id=2&nonav=true"><img border="1" alt="Example 3 thumbnail" src="duguay_ex3_small.png"></a></p><p class='fullwidth' style="text-align: center; margin-top:0em"><font size="2">(click to enlarge)</font></p></fig>

<p>[15] Rather than claiming that differences in vocal spatialization are caused by gender, I make somewhat of the opposite argument. I suggest that vocal placement &ldquo;<i>contributes to</i> the sonic formation of gendered difference [1.7],&rdquo; and that &ldquo;contrasting vocal placements<span style="white-space: nowrap;">. . .</span> [exemplify] one of the ways in which dichotomous conceptions of gender are <i>reinforced</i> through the sound of popular music [4.10].&rdquo; Gender, in other words, is not the cause of vocal placement. It is one of its results. The difference between both claims is schematically summarized in <b>Example 3</b>. My approach is in dialogue with recent work that explores the formative role of sound in constructing identity (<a href="#eidsheim_2019" id="citation_eidsheim_2019_67dc99142fb6b">Eidsheim 2019</a>, <a href="#obadike_2005" id="citation_obadike_2005_67dc99142fb6c">Obadike 2005</a>, <a href="#provenzano_2019" id="citation_provenzano_2019_67dc99142fb6e">Provenzano 2019</a>, <a href="#reddington_2018" id="citation_reddington_2018_67dc99142fb6f">Reddington 2018</a>, to only name a few.) As I have proposed elsewhere (2021), we can consider the sound of commercially successful popular music as a gendered soundscape (<a href="#j&auml;rviluoma_moisala_and_vilkko_2003" id="citation_j&auml;rviluoma_moisala_and_vilkko_2003_67dc99142fb72">J&auml;rviluoma, Moisala, and Vilkko 2003</a>, 99) that defines, reifies, and contests received notions of gender. As Christine Ehrick writes, &ldquo;Although many of us have been well-trained to look for gender, I consider what it means to <i>listen</i> for it.&rdquo; Listening for gender in this way, she argues, can shift our attention to &ldquo;the ways that power, inequality and agency might be expressed in the sonic realm&rdquo; (<a href="#ehrick_2015" id="citation_ehrick_2015_67dc99142fb82">2015</a>).<p>

<p>[16] In the context of popular music, I am especially interested in the politics of gendered soundscapes in musical collaborations, where the case of Rihanna and Eminem proves to be an illustrative example. In every instance that the two artists appear on a track together, they assume a vocal placement configuration that generally presents Rihanna’s voice as ornamental and diffuse and Eminem’s voice as direct and relatable. This differentiation could perhaps be attributed, as de Clercq suggests, to formal conventions, since Rihanna sings all the choruses. Mixing practices for different ranges and vocal delivery styles might also be at play.</p>

<p>[17] The sung vocal hook/rapped verse(s) structure encountered in Rihanna’s and Eminem’s collaborations, however, is extremely common in commercial popular music collaborations. To examine this phenomenon, I previously assembled a corpus containing the 113 songs that featured at least one man and at least one woman on the 2008–18 <i>Billboard</i> Year-end &ldquo;Hot 100.&rdquo; Within the corpus, the most common type of collaboration features men rapping and women singing (45 songs). The reverse configuration, in which a woman raps and a man sings, is much rarer (5 songs). Through an analysis of vocal placement, I concluded that, throughout the corpus,


In the soundscape of the corpus—and of chart-topping collaborations between 2008–18—one is more likely hear a collaboration between a man and a woman that is spatialized along the same lines as Rihanna and Eminem. Within this collaboration, the woman is much more likely to perform a repeated chorus, while men advance the narrative through verses. The opposite configuration, heard, for instance, in Justin Bieber’s 2012 hit &ldquo;Beauty and a Beat,&rdquo; ft. Nicki Minaj, is far less common. </p>

<p>[18] Kate Mancey, Johanna Devaney, and I have recently expanded on this work by assembling and analyzing the Collaborative Song Dataset (CoSoD) (<a href="#duguay_devaney_and_mancey_2023" id="citation_duguay_devaney_and_mancey_2023_67dc99142fb85">2023</a>). CoSoD is a 331-song dataset of all multi-artist collaborations on the 2010–2019 <i>Billboard</i> &ldquo;Hot 100&rdquo; year-end charts. The dataset includes <i>Billboard</i> rankings, year, title, collaboration type, gender of artists, and other relevant metadata for each track. We also created analytical files that contain, among other features, vocal placement data on Layering, Environment, and Width. In the same paper, we present an experiment examining the relationship between the gender of a performer and their vocal placement.<sup><a name="FN10REF" href="#FN10" id="footnote10">(10)</a></sup> Our analysis of a subset of the corpus (574 verses and choruses) reveals a statistically significant difference between the ways in which men’s and women’s voices are treated in terms of Width and Environment. Women’s voices are more likely to be set in reverberant environments and occupy a wider space than men’s voices, echoing the trend found in Rihanna’s and Eminem’s collaborations.</p>

<p>[19] The studies described above do not pinpoint the causation of different vocal placements. They instead underscore how multiple factors coalesce to sonically differentiate men’s and women’s voices in the popular music soundscape. My goal is to identify these trends and interrogate their broader cultural implications. How, when, and why did musical collaborations settle into this sonic and formal trope? What kinds of gendered stereotypes are reproduced when voices are spatialized according to certain conventions? How is the gender binary reinforced through these aesthetic practices? Could we imagine an alternate 2010s pop soundscape filled with collaborations in which men sing hooks while women rap verses? What might such a gendered soundscape imply? How have certain artists pushed back, been subsumed into, or reinvented gendered sonic trends?  By presenting detailed data on vocal placement, CoSoD also allows for further inquiry into these questions. It also opens the way for further experiment on the way form, vocal delivery style, year of release correlate with gender, vocal placement, and other musical features. One could for instance set up a counter-study that observes vocal placement in collaborations where two men collaborate.</p>

<p>[20] The current version of CoSoD does not provide data on prominence. We opted to withhold this measurement at this stage, given the imperfect state of source separation technology. Thanks to recent improvements in source separation technology and to de Clercq’s more perceptually-relevant workflow, this data could now potentially be added.  For this reason, I am grateful for de Clercq’s modification to my methodology. I anticipate that it will be instrumental to analysts as they continue to study gendered soundscapes and spatialization conventions in recorded popular music.</p>

<!-------------------------------- END Article Body -------------------------------------------->

     
	<div style="height:24px;width:150px;background-color:#4c7381;float:left;text-align: center;vertical-align: middle;line-height: 24px;">
		&nbsp;&nbsp;&nbsp;
		<a style="color:white;" onmouseover="this.style.color='#0000ff';text-decoration:none" 
		onmouseout="this.style.color='white';" href="#Beginning">Return to beginning</a>
		&nbsp;&nbsp;&nbsp;
	</div><br><br>

	
<!-------------------------------- Author Info -------------------------------------------->

  
<hr>

	<p><a name="AUTHORNOTE1"></a>
	
	Michèle Duguay<br>
	Harvard University<br>3 Oxford Street, 305N<br>Cambridge, MA 02138<br><a href="mailto:mduguay@fas.harvard.edu">mduguay@fas.harvard.edu</a><br>	
</p>

       
	<div style="height:24px;width:150px;background-color:#4c7381;float:left;text-align: center;vertical-align: middle;line-height: 24px;">
		&nbsp;&nbsp;&nbsp;
		<a style="color:white;" onmouseover="this.style.color='#0000ff';text-decoration:none" 
		onmouseout="this.style.color='white';" href="#Beginning">Return to beginning</a>
		&nbsp;&nbsp;&nbsp;
	</div><br><br>

	
<!-------------------------------- Works Cited List -------------------------------------------->

  
	<hr>
	
	<h3><a name="WorksCited">Works Cited</a></h3>
	
	<div id="citediv_anjok07_2023" class="flyoverdiv">Anjok07. 2023. &ldquo;ultimatevocalremovergui.&rdquo; GitHub Repository. <a href='https://github.com/Anjok07/ultimatevocalremovergui'>https://github.com/Anjok07/ultimatevocalremovergui</a>.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="anjok07_2023"></a>Anjok07. 2023. &ldquo;ultimatevocalremovergui.&rdquo; GitHub Repository. <a href='https://github.com/Anjok07/ultimatevocalremovergui'>https://github.com/Anjok07/ultimatevocalremovergui</a>.</p><div id="citediv_duguay_2021" class="flyoverdiv">Duguay, Mich&egrave;le. 2021. &ldquo;Gendering the Virtual Space: Sonic Femininities and Masculinities in Contemporary Top 40 Music.&rdquo; PhD diss., The Graduate Center, CUNY.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="duguay_2021"></a>Duguay, Mich&egrave;le. 2021. &ldquo;Gendering the Virtual Space: Sonic Femininities and Masculinities in Contemporary Top 40 Music.&rdquo; PhD diss., The Graduate Center, CUNY.</p><div id="citediv_duguay_2022" class="flyoverdiv">Duguay, Mich&egrave;le. 2022. &ldquo;Analyzing Vocal Placement in Recorded Virtual Space,&rdquo; <i>Music Theory Online</i> 28 no. 4. <a href='https://mtosmt.org/issues/mto.22.28.4/mto.22.28.4.duguay.html'>https://mtosmt.org/issues/mto.22.28.4/mto.22.28.4.duguay.html</a></div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="duguay_2022"></a><span class='sans'>&mdash;&mdash;&mdash;&mdash;&mdash;</span>. 2022. &ldquo;Analyzing Vocal Placement in Recorded Virtual Space,&rdquo; <i>Music Theory Online</i> 28 no. 4. <a href='https://mtosmt.org/issues/mto.22.28.4/mto.22.28.4.duguay.html'>https://mtosmt.org/issues/mto.22.28.4/mto.22.28.4.duguay.html</a></p><div id="citediv_duguay_devaney_and_mancey_2023" class="flyoverdiv">Duguay, Michèle, Johanna Devaney, and Kate Mancey. 2023. &ldquo;Collaborative Song Dataset (CoSoD): An Annotated Dataset of Multi-Artist Collaborations in Popular Music.&rdquo; <i>In Proceedings of the 24th International Society for Music Information Retrieval Conference (ISMIR)</i>, Milan, Italy, Nov. 5&ndash;9.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="duguay_devaney_and_mancey_2023"></a>Duguay, Michèle, Johanna Devaney, and Kate Mancey. 2023. &ldquo;Collaborative Song Dataset (CoSoD): An Annotated Dataset of Multi-Artist Collaborations in Popular Music.&rdquo; <i>In Proceedings of the 24th International Society for Music Information Retrieval Conference (ISMIR)</i>, Milan, Italy, Nov. 5&ndash;9.</p><div id="citediv_ehrick_2015" class="flyoverdiv">Ehrick, Christine. 2015. &ldquo;Vocal Gender and the Gendered Soundscape: At the Intersection of Gender Studies and Sound Studies.&rdquo; <i>Sounding Out! Blog</i>, February 2. <a href='https://soundstudiesblog.com/2015/02/02/vocal-gender-and-the-gendered-soundscape-at-the-intersection-of-gender-studies-and-sound-studies/'>https://soundstudiesblog.com/2015/02/02/vocal-gender-and-the-gendered-soundscape-at-the-intersection-of-gender-studies-and-sound-studies/</a>.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="ehrick_2015"></a>Ehrick, Christine. 2015. &ldquo;Vocal Gender and the Gendered Soundscape: At the Intersection of Gender Studies and Sound Studies.&rdquo; <i>Sounding Out! Blog</i>, February 2. <a href='https://soundstudiesblog.com/2015/02/02/vocal-gender-and-the-gendered-soundscape-at-the-intersection-of-gender-studies-and-sound-studies/'>https://soundstudiesblog.com/2015/02/02/vocal-gender-and-the-gendered-soundscape-at-the-intersection-of-gender-studies-and-sound-studies/</a>.</p><div id="citediv_eidsheim_2019" class="flyoverdiv">Eidsheim, Nina Sun. 2019. <i>The Race of Sound: Listening, Timbre, and Vocality in African American Music</i>. Durham: Duke University Press.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="eidsheim_2019"></a>Eidsheim, Nina Sun. 2019. <i>The Race of Sound: Listening, Timbre, and Vocality in African American Music</i>. Durham: Duke University Press.</p><div id="citediv_fabbro_et_al_2023" class="flyoverdiv">Fabbro, Giorgio, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Martínez-Ramírez, Weihsiang Liao, Igor Gadelha, et al. 2024.&ldquo;The Sound Demixing Challenge 2023 &ndash; Music Demixing Track.&rdquo; arXiv, 2024. <a href='http://arxiv.org/abs/2308.06979'>http://arxiv.org/abs/2308.06979</a>.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="fabbro_et_al_2023"></a>Fabbro, Giorgio, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Martínez-Ramírez, Weihsiang Liao, Igor Gadelha, et al. 2024.&ldquo;The Sound Demixing Challenge 2023 &ndash; Music Demixing Track.&rdquo; arXiv, 2024. <a href='http://arxiv.org/abs/2308.06979'>http://arxiv.org/abs/2308.06979</a>.</p><div id="citediv_j&auml;rviluoma_moisala_and_vilkko_2003" class="flyoverdiv">J&auml;viluoma, Helmi, Pirkko Moisala, and Anni Vilkko. 2003. <i>Gender and Qualitative Methods</i>. SAGE Publications.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="j&auml;rviluoma_moisala_and_vilkko_2003"></a>J&auml;viluoma, Helmi, Pirkko Moisala, and Anni Vilkko. 2003. <i>Gender and Qualitative Methods</i>. SAGE Publications.</p><div id="citediv_morrison_2019" class="flyoverdiv">Morrison, Matthew D. 2019. &ldquo;Race, Blacksound, and the (Re)Making of Musicological Discourse.&rdquo <i>Journal of the American Musicological Society</i> 72 (3): 781&ndash;823. <a href='https://doi.org/10.1525/jams.2019.72.3.781'>https://doi.org/10.1525/jams.2019.72.3.781</a>.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="morrison_2019"></a>Morrison, Matthew D. 2019. &ldquo;Race, Blacksound, and the (Re)Making of Musicological Discourse.&rdquo <i>Journal of the American Musicological Society</i> 72 (3): 781&ndash;823. <a href='https://doi.org/10.1525/jams.2019.72.3.781'>https://doi.org/10.1525/jams.2019.72.3.781</a>.</p><div id="citediv_obadike_2005" class="flyoverdiv">Obadike, Mendi Dessalines Shirley Lewis Townsend. 2005. &ldquo;Low Fidelity: Stereotyped Blackness in the Field of Sound.&rdquo; Ph.D. diss., Duke University.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="obadike_2005"></a>Obadike, Mendi Dessalines Shirley Lewis Townsend. 2005. &ldquo;Low Fidelity: Stereotyped Blackness in the Field of Sound.&rdquo; Ph.D. diss., Duke University.</p><div id="citediv_provenzano_2019" class="flyoverdiv">Provenzano, Catherine. 2019. &ldquo;Making Voices: The Gendering of Pitch Correction and the Auto Tune Effect in Contemporary Pop Music.&rdquo; <i>Journal of Popular Music Studies</i> 31 (2): 63&ndash;84.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="provenzano_2019"></a>Provenzano, Catherine. 2019. &ldquo;Making Voices: The Gendering of Pitch Correction and the Auto Tune Effect in Contemporary Pop Music.&rdquo; <i>Journal of Popular Music Studies</i> 31 (2): 63&ndash;84.</p><div id="citediv_reddington_2018" class="flyoverdiv">Reddington, Helen. 2018. &ldquo;Gender Ventriloquism in Studio Production.&rdquo; <i>IASPM Journal</i> 8 (1): 59&ndash;73.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="reddington_2018"></a>Reddington, Helen. 2018. &ldquo;Gender Ventriloquism in Studio Production.&rdquo; <i>IASPM Journal</i> 8 (1): 59&ndash;73.</p><div id="citediv_rouard_massa_and_défossez_2023" class="flyoverdiv">Rouard, Simon, Francisco Massa, and Alexandre Défossez. 2023. &ldquo;Hybrid Transformers for Music Source Separation.&rdquo; IC 2023 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2023), Rhodes, Greece, June 4&ndash;10. <a href='https://arxiv.org/abs/2211.08553'>https://arxiv.org/abs/2211.08553</a>.</div><p style="text-indent: -1em; margin-left: 1em; margin-top: 0em"><a name="rouard_massa_and_défossez_2023"></a>Rouard, Simon, Francisco Massa, and Alexandre Défossez. 2023. &ldquo;Hybrid Transformers for Music Source Separation.&rdquo; IC 2023 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2023), Rhodes, Greece, June 4&ndash;10. <a href='https://arxiv.org/abs/2211.08553'>https://arxiv.org/abs/2211.08553</a>.</p>
     
	<div style="height:24px;width:150px;background-color:#4c7381;float:left;text-align: center;vertical-align: middle;line-height: 24px;">
		&nbsp;&nbsp;&nbsp;
		<a style="color:white;" onmouseover="this.style.color='#0000ff';text-decoration:none" 
		onmouseout="this.style.color='white';" href="#Beginning">Return to beginning</a>
		&nbsp;&nbsp;&nbsp;
	</div><br><br>

	
<!-------------------------------- Footnotes List -------------------------------------------->

  	
	<hr>
	
	<h3><a name="Footnotes">Footnotes</a></h3>
	
	<p><a name="FN1">1.</a> The polarity of a track can be reversed in Audacity with Effect > Invert.<br><a href="#FN1REF">Return to text</a></p><p><a name="FN2">2.</a> de Clercq recommends the Youlean Loudness Meter, which—despite being available in a free version—requires the Pro version (priced at 39.99 USD as of this writing) to calculate the integrated LUFS of an audio file. The free audio editing software Audacity, already familiar to many music analysts, can be used to generate the instrumental mix.<br><a href="#FN2REF">Return to text</a></p><p><a name="FN3">3.</a> De Clercq and I used v4 of Jamie Bullock’s LibXtract Library to determine the RMS Amplitude of audio files. We used different versions of the Open-Unmix software to isolate vocal tracks.  De Clercq used Open-Unmix 1.2.1, which was made available on July 23, 2021. I used the initial release of Open-Unmix, which was available in throughout 2020, to isolate the vocal tracks analyzed in my 2022 article.<br><a href="#FN3REF">Return to text</a></p><p><a name="FN4">4.</a> Unless indicated otherwise, all instrumental tracks were generated on Audacity through polarity reversal.<br><a href="#FN4REF">Return to text</a></p><p><a name="FN5">5.</a> This source separation model was trained by the developers of Ultimate Vocal Remover <a href="#anjok07_2023" id="citation_anjok07_2023_67dc991429d91">Anjok07 2023</a>.<br><a href="#FN5REF">Return to text</a></p><p><a name="FN6">6.</a> I used Version 5.6 of Ultimate Vocal Remover GUI.<br><a href="#FN6REF">Return to text</a></p><p><a name="FN7">7.</a> I have not tested more recent versions of iZotope’s Music Rebalance tool.<br><a href="#FN7REF">Return to text</a></p><p><a name="FN8">8.</a> Researchers in academia and industry frequently release updates and new versions of existing models. For an overview of recent developments in source separation models as they apply to music, see Fabbro et al. (2023).<br><a href="#FN8REF">Return to text</a></p><p><a name="FN9">9.</a> Since LU are a continuous data, a sliding scale would offer the best solution.<br><a href="#FN9REF">Return to text</a></p><p><a name="FN10">10.</a> As explained in the article, we opted not to consider the artists’ race in this study to account for the dynamics of Blacksound, as formulated by Matthew D. Morrison (2019). More work is necessary to understand how to best address these issues in datasets, corpus studies, and other large-scale music analytical projects.<br><a href="#FN10REF">Return to text</a></p><div id="fndiv1" class="flyoverdiv">The polarity of a track can be reversed in Audacity with Effect > Invert.</div><div id="fndiv2" class="flyoverdiv">de Clercq recommends the Youlean Loudness Meter, which—despite being available in a free version—requires the Pro version (priced at 39.99 USD as of this writing) to calculate the integrated LUFS of an audio file. The free audio editing software Audacity, already familiar to many music analysts, can be used to generate the instrumental mix.</div><div id="fndiv3" class="flyoverdiv">De Clercq and I used v4 of Jamie Bullock’s LibXtract Library to determine the RMS Amplitude of audio files. We used different versions of the Open-Unmix software to isolate vocal tracks.  De Clercq used Open-Unmix 1.2.1, which was made available on July 23, 2021. I used the initial release of Open-Unmix, which was available in throughout 2020, to isolate the vocal tracks analyzed in my 2022 article.</div><div id="fndiv4" class="flyoverdiv">Unless indicated otherwise, all instrumental tracks were generated on Audacity through polarity reversal.</div><div id="fndiv5" class="flyoverdiv">This source separation model was trained by the developers of Ultimate Vocal Remover <a href="#anjok07_2023" id="citation_anjok07_2023_67dc991429d91">Anjok07 2023</a>.</div><div id="fndiv6" class="flyoverdiv">I used Version 5.6 of Ultimate Vocal Remover GUI.</div><div id="fndiv7" class="flyoverdiv">I have not tested more recent versions of iZotope’s Music Rebalance tool.</div><div id="fndiv8" class="flyoverdiv">Researchers in academia and industry frequently release updates and new versions of existing models. For an overview of recent developments in source separation models as they apply to music, see Fabbro et al. (2023).</div><div id="fndiv9" class="flyoverdiv">Since LU are a continuous data, a sliding scale would offer the best solution.</div><div id="fndiv10" class="flyoverdiv">As explained in the article, we opted not to consider the artists’ race in this study to account for the dynamics of Blacksound, as formulated by Matthew D. Morrison (2019). More work is necessary to understand how to best address these issues in datasets, corpus studies, and other large-scale music analytical projects.</div>
     
	<div style="height:24px;width:150px;background-color:#4c7381;float:left;text-align: center;vertical-align: middle;line-height: 24px;">
		&nbsp;&nbsp;&nbsp;
		<a style="color:white;" onmouseover="this.style.color='#0000ff';text-decoration:none" 
		onmouseout="this.style.color='white';" href="#Beginning">Return to beginning</a>
		&nbsp;&nbsp;&nbsp;
	</div><br><br>

	
<!-------------------------------- FOOTER -------------------------------------------->

  <hr>
<h3>Copyright Statement</h3>
<p><h4>Copyright &copy; 2024 by the Society for Music Theory. All rights reserved.</h4></p>
<p class="small">[1] Copyrights for individual items published in  <i>Music Theory Online</i> (<i>MTO</i>) 
are held by their authors. Items appearing in  <i>MTO</i> may be saved and stored in electronic or paper form, and may be shared among individuals for purposes of 
scholarly research or discussion, but may  <i>not</i>  be republished in any form, electronic or print, without prior, written permission from the author(s), and advance 
notification of the editors of  <i>MTO.</i></p>
<p class="small">[2] Any redistributed form of items published in  <i>MTO</i> must include the following information in a form appropriate to the medium in which the items are 
to appear: </p>
<blockquote>
<p class="small">This item appeared in  <i>Music Theory Online</i> in [VOLUME #, ISSUE #] on [DAY/MONTH/YEAR]. It was authored by [FULL NAME, EMAIL ADDRESS], with whose written 
permission it is reprinted here.</p>
</blockquote>
<p class="small">[3] Libraries may archive issues of  <i>MTO</i> in electronic or paper form for public access so long as each issue is stored in its entirety, and no access fee 
is charged. Exceptions to these requirements must be approved in writing by the editors of  <i>MTO,</i> who will act in accordance with the decisions of the Society 
for Music Theory. </p>
<p class="small">This document and all portions thereof are protected by U.S. and international copyright laws. Material contained herein may be copied and/or distributed for research 
purposes only. </p>
     
	<div style="height:24px;width:150px;background-color:#4c7381;float:left;text-align: center;vertical-align: middle;line-height: 24px;">
		&nbsp;&nbsp;&nbsp;
		<a style="color:white;" onmouseover="this.style.color='#0000ff';text-decoration:none" 
		onmouseout="this.style.color='white';" href="#Beginning">Return to beginning</a>
		&nbsp;&nbsp;&nbsp;
	</div><br><br>

	
  	

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 


<div style="width:55%;float:right"><a href="https://societymusictheory.org">
<img alt="SMT" longdesc="Society for Music Theory" src="https://mtosmt.org/gifs/smtlogo_black.png" width="180"></a></div>
	
<div>
<p style='font-size:1.1rem'>Prepared by Andrew Blake, Editorial Assistant  


<br>
		
			<br>Number of visits:  

		1366		
	</p><br><br>
</i>		

</div>
</div>
</article>
</body>
</html>

